static int stmmac_rx(struct stmmac_priv *priv, int limit, unsigned int queue)
{
	unsigned int rx_errors = 0, rx_dropped = 0, rx_bytes = 0, rx_packets = 0;
	struct stmmac_rxq_stats *rxq_stats = &priv->xstats.rxq_stats[queue];
	struct stmmac_rx_queue *rx_q = &priv->dma_conf.rx_queue[queue];
	struct stmmac_channel *ch = &priv->channel[queue];
	unsigned int count = 0, error = 0, len = 0;
	int status = 0, coe = priv->hw->rx_csum;
	unsigned int next_entry = rx_q->cur_rx;
	enum dma_data_direction dma_dir;
	unsigned int desc_size;
	struct sk_buff *skb = NULL;
	struct stmmac_xdp_buff ctx;
	int xdp_status = 0;
	int buf_sz;
	dma_dir = page_pool_get_dma_dir(rx_q->page_pool);
	buf_sz = DIV_ROUND_UP(priv->dma_conf.dma_buf_sz, PAGE_SIZE) * PAGE_SIZE;
	limit = min(priv->dma_conf.dma_rx_size - 1, (unsigned int)limit);
	if (netif_msg_rx_status(priv)) {
		void *rx_head;
		netdev_dbg(priv->dev, "%s: descriptor ring:\n", __func__);
		if (priv->extend_desc) {
			rx_head = (void *)rx_q->dma_erx;
			desc_size = sizeof(struct dma_extended_desc);
		} else {
			rx_head = (void *)rx_q->dma_rx;
			desc_size = sizeof(struct dma_desc);
		}
		stmmac_display_ring(priv, rx_head, priv->dma_conf.dma_rx_size, true,
				    rx_q->dma_rx_phy, desc_size);
	}
	while (count < limit) {
		unsigned int buf1_len = 0, buf2_len = 0;
		enum pkt_hash_types hash_type;
		struct stmmac_rx_buffer *buf;
		struct dma_desc *np, *p;
		int entry;
		unsigned int hash;
		if (!count && rx_q->state_saved) {
			skb = rx_q->state.skb;
			error = rx_q->state.error;
			len = rx_q->state.len;
		} else {
			rx_q->state_saved = false;
			skb = NULL;
			error = 0;
			len = 0;
		}
read_again:
		if (count >= limit)
			break;
		buf1_len = 0;
		buf2_len = 0;
		entry = next_entry;
		buf = &rx_q->buf_pool[entry];
		if (priv->extend_desc)
			p = (struct dma_desc *)(rx_q->dma_erx + entry);
		else
			p = rx_q->dma_rx + entry;
		status = stmmac_rx_status(priv, &priv->xstats, p);
		if (unlikely(status & dma_own))
			break;
		rx_q->cur_rx = STMMAC_GET_ENTRY(rx_q->cur_rx,
						priv->dma_conf.dma_rx_size);
		next_entry = rx_q->cur_rx;
		if (priv->extend_desc)
			np = (struct dma_desc *)(rx_q->dma_erx + next_entry);
		else
			np = rx_q->dma_rx + next_entry;
		prefetch(np);
		if (priv->extend_desc)
			stmmac_rx_extended_status(priv, &priv->xstats, rx_q->dma_erx + entry);
		if (unlikely(status == discard_frame)) {
			page_pool_recycle_direct(rx_q->page_pool, buf->page);
			buf->page = NULL;
			error = 1;
			if (!priv->hwts_rx_en)
				rx_errors++;
		}
		if (unlikely(error && (status & rx_not_ls)))
			goto read_again;
		if (unlikely(error)) {
			dev_kfree_skb(skb);
			skb = NULL;
			count++;
			continue;
		}
		prefetch(page_address(buf->page) + buf->page_offset);
		if (buf->sec_page)
			prefetch(page_address(buf->sec_page));
		buf1_len = stmmac_rx_buf1_len(priv, p, status, len);
		len += buf1_len;
		buf2_len = stmmac_rx_buf2_len(priv, p, status, len);
		len += buf2_len;
		if (likely(!(status & rx_not_ls))) {
			if (buf2_len) {
				buf2_len -= ETH_FCS_LEN;
				len -= ETH_FCS_LEN;
			} else if (buf1_len) {
				buf1_len -= ETH_FCS_LEN;
				len -= ETH_FCS_LEN;
			}
		}
		if (!skb) {
	int pre_len, sync_len;
			dma_sync_single_for_cpu(priv->device, buf->addr,
						buf1_len, dma_dir);
			xdp_init_buff(&ctx.xdp, buf_sz, &rx_q->xdp_rxq);
			xdp_prepare_buff(&ctx.xdp, page_address(buf->page),
					 buf->page_offset, buf1_len, true);
			pre_len = ctx.xdp.data_end - ctx.xdp.data_hard_start -
				  buf->page_offset;
			ctx.priv = priv;
			ctx.desc = p;
			ctx.ndesc = np;
			skb = stmmac_xdp_run_prog(priv, &ctx.xdp);
			sync_len = ctx.xdp.data_end - ctx.xdp.data_hard_start -
				   buf->page_offset;
			sync_len = max(sync_len, pre_len);
			if (IS_ERR(skb)) {
				unsigned int xdp_res = -PTR_ERR(skb);
				if (xdp_res & STMMAC_XDP_CONSUMED) {
					page_pool_put_page(rx_q->page_pool,
							   virt_to_head_page(ctx.xdp.data),
							   sync_len, true);
					buf->page = NULL;
					rx_dropped++;
					skb = NULL;
					if (unlikely((status & rx_not_ls)))
						goto read_again;
					count++;
					continue;
				} else if (xdp_res & (STMMAC_XDP_TX |
						      STMMAC_XDP_REDIRECT)) {
					xdp_status |= xdp_res;
					buf->page = NULL;
					skb = NULL;
					count++;
					continue;
				}
			}
		}
		if (!skb) {
			buf1_len = ctx.xdp.data_end - ctx.xdp.data;
			skb = napi_alloc_skb(&ch->rx_napi, buf1_len);
			if (!skb) {
				rx_dropped++;
				count++;
				goto drain_data;
			}
			skb_copy_to_linear_data(skb, ctx.xdp.data, buf1_len);
			skb_put(skb, buf1_len);
			page_pool_recycle_direct(rx_q->page_pool, buf->page);
			buf->page = NULL;
		} else if (buf1_len) {
			dma_sync_single_for_cpu(priv->device, buf->addr,
						buf1_len, dma_dir);
			skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,
					buf->page, buf->page_offset, buf1_len,
					priv->dma_conf.dma_buf_sz);
			skb_mark_for_recycle(skb);
			buf->page = NULL;
		}
		if (buf2_len) {
			dma_sync_single_for_cpu(priv->device, buf->sec_addr,
						buf2_len, dma_dir);
			skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,
					buf->sec_page, 0, buf2_len,
					priv->dma_conf.dma_buf_sz);
			skb_mark_for_recycle(skb);
			buf->sec_page = NULL;
		}
drain_data:
		if (likely(status & rx_not_ls))
			goto read_again;
		if (!skb)
			continue;
		stmmac_get_rx_hwtstamp(priv, p, np, skb);
		if (priv->hw->hw_vlan_en)
			stmmac_rx_hw_vlan(priv, priv->hw, p, skb);
		else
			stmmac_rx_vlan(priv->dev, skb);
		skb->protocol = eth_type_trans(skb, priv->dev);
		if (unlikely(!coe) || !stmmac_has_ip_ethertype(skb))
			skb_checksum_none_assert(skb);
		else
			skb->ip_summed = CHECKSUM_UNNECESSARY;
		if (!stmmac_get_rx_hash(priv, p, &hash, &hash_type))
			skb_set_hash(skb, hash, hash_type);
		skb_record_rx_queue(skb, queue);
		napi_gro_receive(&ch->rx_napi, skb);
		skb = NULL;
		rx_packets++;
		rx_bytes += len;
		count++;
	}
	if (status & rx_not_ls || skb) {
		rx_q->state_saved = true;
		rx_q->state.skb = skb;
		rx_q->state.error = error;
		rx_q->state.len = len;
	}
	stmmac_finalize_xdp_rx(priv, xdp_status);
	stmmac_rx_refill(priv, queue);
	u64_stats_update_begin(&rxq_stats->napi_syncp);
	u64_stats_add(&rxq_stats->napi.rx_packets, rx_packets);
	u64_stats_add(&rxq_stats->napi.rx_bytes, rx_bytes);
	u64_stats_add(&rxq_stats->napi.rx_pkt_n, count);
	u64_stats_update_end(&rxq_stats->napi_syncp);
	priv->xstats.rx_dropped += rx_dropped;
	priv->xstats.rx_errors += rx_errors;
	return count;
}

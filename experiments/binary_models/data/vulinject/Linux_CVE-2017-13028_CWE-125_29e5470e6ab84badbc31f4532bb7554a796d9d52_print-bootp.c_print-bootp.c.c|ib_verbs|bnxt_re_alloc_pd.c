int bnxt_re_alloc_pd(struct ib_pd *ibpd, struct ib_udata *udata)
{
	struct ib_device *ibdev = ibpd->device;
	struct bnxt_re_dev *rdev = to_bnxt_re_dev(ibdev, ibdev);
	struct bnxt_re_ucontext *ucntx = rdma_udata_to_drv_context(
		udata, struct bnxt_re_ucontext, ib_uctx);
	struct bnxt_re_pd *pd = container_of(ibpd, struct bnxt_re_pd, ib_pd);
	struct bnxt_re_user_mmap_entry *entry = NULL;
	unsigned int active_pds;
	int rc = 0;
	pd->rdev = rdev;
	if (bnxt_qplib_alloc_pd(&rdev->qplib_res, &pd->qplib_pd)) {
		ibdev_err(&rdev->ibdev, "Failed to allocate HW PD");
		rc = -ENOMEM;
		goto fail;
	}
	if (udata) {
		struct bnxt_re_pd_resp resp = {};
		if (!ucntx->dpi.dbr) {
			if (bnxt_qplib_alloc_dpi(&rdev->qplib_res,
						 &ucntx->dpi, ucntx, BNXT_QPLIB_DPI_TYPE_UC)) {
				rc = -ENOMEM;
				goto dbfail;
			}
		}
		resp.pdid = pd->qplib_pd.id;
		resp.dpi = ucntx->dpi.dpi;
		entry = bnxt_re_mmap_entry_insert(ucntx, (unsigned int int)ucntx->dpi.umdbr,
						  BNXT_RE_MMAP_UC_DB, &resp.dbr);
		if (!entry) {
			rc = -ENOMEM;
			goto dbfail;
		}
		pd->pd_db_mmap = &entry->rdma_entry;
		rc = ib_copy_to_udata(udata, &resp, min(sizeof(resp), udata->outlen));
		if (rc) {
			rc = -EFAULT;
			goto dbfail;
		}
	}
	if (!udata)
		if (bnxt_re_create_fence_mr(pd))
			ibdev_warn(&rdev->ibdev,
				   "Failed to create Fence-MR\n");
	active_pds = atomic_inc_return(&rdev->stats.res.pd_count);
	if (active_pds > rdev->stats.res.pd_watermark)
		rdev->stats.res.pd_watermark = active_pds;
	return 0;
dbfail:
	bnxt_qplib_dealloc_pd(&rdev->qplib_res, &rdev->qplib_res.pd_tbl,
			      &pd->qplib_pd);
fail:
	return rc;
}

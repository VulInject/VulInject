extent_merge_impl(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, edata_t *a,
    edata_t *b, bool holding_core_locks) {
	/* Only the expanding path may merge w/o holding ecache locks. */
	if (holding_core_locks) {
		witness_assert_positive_depth_to_rank(
		    tsdn_witness_tsdp_get(tsdn), WITNESS_RANK_CORE);
	} else {
		witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
		    WITNESS_RANK_CORE, 0);
	}
	assert(edata_base_get(a) < edata_base_get(b));
	assert(edata_arena_ind_get(a) == edata_arena_ind_get(b));
	assert(edata_arena_ind_get(a) == ehooks_ind_get(ehooks));
	emap_assert_mapped(tsdn, pac->emap, a);
	emap_assert_mapped(tsdn, pac->emap, b);
	bool err = ehooks_merge(tsdn, ehooks, edata_base_get(a),
	    edata_size_get(a), edata_base_get(b), edata_size_get(b),
	    edata_committed_get(a));
	if (err) {
		return true;
	}
	/*
	 * The rtree writes must happen while all the relevant elements are
	 * owned, so the following code uses decomposed helper functions rather
	 * than extent_{,de}register() to do things in the right order.
	 */
	emap_prepare_t prepare;
	emap_merge_prepare(tsdn, pac->emap, &prepare, a, b);
	assert(edata_state_get(a) == extent_state_active ||
	    edata_state_get(a) == extent_state_merging);
	edata_size_set(a, edata_size_get(a) + edata_size_get(b));
	edata_sn_set(a, (edata_sn_get(a) < edata_sn_get(b)) ?
	    edata_sn_get(a) : edata_sn_get(b));
	edata_zeroed_set(a, edata_zeroed_get(a) && edata_zeroed_get(b));
	emap_merge_commit(tsdn, pac->emap, &prepare, a, b);
	edata_cache_put(tsdn, pac->edata_cache, b);
	return false;
}

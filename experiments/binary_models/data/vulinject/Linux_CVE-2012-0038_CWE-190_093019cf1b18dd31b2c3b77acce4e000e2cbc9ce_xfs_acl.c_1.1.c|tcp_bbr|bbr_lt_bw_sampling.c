static void bbr_lt_bw_sampling(struct sock *sk, const struct rate_sample *rs)
{
	struct tcp_sock *tp = tcp_sk(sk);
	struct bbr *bbr = inet_csk_ca(sk);
	int lost, delivered;
	unsigned int int bw;
	unsigned int t;
	if (bbr->lt_use_bw) {	
		if (bbr->mode == BBR_PROBE_BW && bbr->round_start &&
		    ++bbr->lt_rtt_cnt >= bbr_lt_bw_max_rtts) {
			bbr_reset_lt_bw_sampling(sk);    
			bbr_reset_probe_bw_mode(sk);  
		}
		return;
	}
	if (!bbr->lt_is_sampling) {
		if (!rs->losses)
			return;
		bbr_reset_lt_bw_sampling_interval(sk);
		bbr->lt_is_sampling = true;
	}
	if (rs->is_app_limited) {
		bbr_reset_lt_bw_sampling(sk);
		return;
	}
	if (bbr->round_start)
		bbr->lt_rtt_cnt++;	
	if (bbr->lt_rtt_cnt < bbr_lt_intvl_min_rtts)
		return;		
	if (bbr->lt_rtt_cnt > 4 * bbr_lt_intvl_min_rtts) {
		bbr_reset_lt_bw_sampling(sk);  
		return;
	}
	if (!rs->losses)
		return;
	lost = tp->lost - bbr->lt_last_lost;
	delivered = tp->delivered - bbr->lt_last_delivered;
	if (!delivered || (lost << BBR_SCALE) < bbr_lt_loss_thresh * delivered)
		return;
	t = div_u64(tp->delivered_mstamp, USEC_PER_MSEC) - bbr->lt_last_stamp;
	if ((int)t < 1)
		return;		
	if (t >= ~0U / USEC_PER_MSEC) {
		bbr_reset_lt_bw_sampling(sk);  
		return;
	}
	t *= USEC_PER_MSEC;
	bw = (unsigned int int)delivered * BW_UNIT;
	do_div(bw, t);
	bbr_lt_bw_interval_done(sk, bw);
}

static int rx_bottom(struct r8152 *tp, int budget)
{
	unsigned int flags;
	struct list_head *cursor, *next, rx_queue;
	int ret = 0, work_done = 0;
	struct napi_struct *napi = &tp->napi;
	if (!skb_queue_empty(&tp->rx_queue)) {
		while (work_done < budget) {
			struct sk_buff *skb = __skb_dequeue(&tp->rx_queue);
			struct net_device *netdev = tp->netdev;
			struct net_device_stats *stats = &netdev->stats;
			unsigned int pkt_len;
			if (!skb)
				break;
			pkt_len = skb->len;
			napi_gro_receive(napi, skb);
			work_done++;
			stats->rx_packets++;
			stats->rx_bytes += pkt_len;
		}
	}
	if (list_empty(&tp->rx_done) || work_done >= budget)
		goto out1;
	clear_bit(RX_EPROTO, &tp->flags);
	INIT_LIST_HEAD(&rx_queue);
	spin_lock_irqsave(&tp->rx_lock, flags);
	list_splice_init(&tp->rx_done, &rx_queue);
	spin_unlock_irqrestore(&tp->rx_lock, flags);
	list_for_each_safe(cursor, next, &rx_queue) {
		struct rx_desc *rx_desc;
		struct rx_agg *agg, *agg_free;
		int len_used = 0;
		struct urb *urb;
		unsigned char *rx_data;
		if (work_done >= budget)
			break;
		list_del_init(cursor);
		agg = list_entry(cursor, struct rx_agg, list);
		urb = agg->urb;
		if (urb->status != 0 || urb->actual_length < ETH_ZLEN)
			goto submit;
		agg_free = rtl_get_free_rx(tp, GFP_ATOMIC);
		rx_desc = agg->buffer;
		rx_data = agg->buffer;
		len_used += sizeof(struct rx_desc);
		while (urb->actual_length > len_used) {
			struct net_device *netdev = tp->netdev;
			struct net_device_stats *stats = &netdev->stats;
			unsigned int pkt_len, rx_frag_head_sz, len;
			struct sk_buff *skb;
			bool use_frags;
			WARN_ON_ONCE(skb_queue_len(&tp->rx_queue) >= 1000);
			pkt_len = le32_to_cpu(rx_desc->opts1) & RX_LEN_MASK;
			if (pkt_len < ETH_ZLEN)
				break;
			len_used += pkt_len;
			if (urb->actual_length < len_used)
				break;
			pkt_len -= ETH_FCS_LEN;
			len = pkt_len;
			rx_data += sizeof(struct rx_desc);
			if (!agg_free || tp->rx_copybreak > len)
				use_frags = false;
			else
				use_frags = true;
			if (use_frags) {
				if (work_done >= budget) {
					rx_frag_head_sz = tp->rx_copybreak;
					skb = napi_alloc_skb(napi,
							     rx_frag_head_sz);
				} else {
					rx_frag_head_sz = 0;
					skb = napi_get_frags(napi);
				}
			} else {
				rx_frag_head_sz = 0;
				skb = napi_alloc_skb(napi, len);
			}
			if (!skb) {
				stats->rx_dropped++;
				goto find_next_rx;
			}
			skb->ip_summed = r8152_rx_csum(tp, rx_desc);
			rtl_rx_vlan_tag(rx_desc, skb);
			if (use_frags) {
				if (rx_frag_head_sz) {
					memcpy(skb->data, rx_data,
					       rx_frag_head_sz);
					skb_put(skb, rx_frag_head_sz);
					len -= rx_frag_head_sz;
					rx_data += rx_frag_head_sz;
					skb->protocol = eth_type_trans(skb,
								       netdev);
				}
				skb_add_rx_frag(skb, 0, agg->page,
						agg_offset(agg, rx_data),
						len, SKB_DATA_ALIGN(len));
			} else {
				memcpy(skb->data, rx_data, len);
				skb_put(skb, len);
				skb->protocol = eth_type_trans(skb, netdev);
			}
			if (work_done < budget) {
				if (use_frags)
					napi_gro_frags(napi);
				else
					napi_gro_receive(napi, skb);
				work_done++;
				stats->rx_packets++;
				stats->rx_bytes += pkt_len;
			} else {
				__skb_queue_tail(&tp->rx_queue, skb);
			}
find_next_rx:
			rx_data = rx_agg_align(rx_data + len + ETH_FCS_LEN);
			rx_desc = (struct rx_desc *)rx_data;
			len_used = agg_offset(agg, rx_data);
			len_used += sizeof(struct rx_desc);
		}
		WARN_ON(!agg_free && page_count(agg->page) > 1);
		if (agg_free) {
			spin_lock_irqsave(&tp->rx_lock, flags);
			if (page_count(agg->page) == 1) {
				list_add(&agg_free->list, &tp->rx_used);
			} else {
				list_add_tail(&agg->list, &tp->rx_used);
				agg = agg_free;
				urb = agg->urb;
			}
			spin_unlock_irqrestore(&tp->rx_lock, flags);
		}
submit:
		if (!ret) {
			ret = r8152_submit_rx(tp, agg, GFP_ATOMIC);
		} else {
			urb->actual_length = 0;
			list_add_tail(&agg->list, next);
		}
	}
	if (!list_empty(&rx_queue)) {
		spin_lock_irqsave(&tp->rx_lock, flags);
		list_splice(&rx_queue, &tp->rx_done);
		spin_unlock_irqrestore(&tp->rx_lock, flags);
	}
out1:
	return work_done;
}

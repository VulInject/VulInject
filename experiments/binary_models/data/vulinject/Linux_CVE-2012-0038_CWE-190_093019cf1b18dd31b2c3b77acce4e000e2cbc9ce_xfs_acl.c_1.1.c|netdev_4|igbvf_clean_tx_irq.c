static bool igbvf_clean_tx_irq(struct igbvf_ring *tx_ring)
{
	struct igbvf_adapter *adapter = tx_ring->adapter;
	struct net_device *netdev = adapter->netdev;
	struct igbvf_buffer *buffer_info;
	struct sk_buff *skb;
	union e1000_adv_tx_desc *tx_desc, *eop_desc;
	unsigned int total_bytes = 0, total_packets = 0;
	unsigned int i, count = 0;
	bool cleaned = false;
	i = tx_ring->next_to_clean;
	buffer_info = &tx_ring->buffer_info[i];
	eop_desc = buffer_info->next_to_watch;
	do {
		if (!eop_desc)
			break;
		smp_rmb();
		if (!(eop_desc->wb.status & cpu_to_le32(E1000_TXD_STAT_DD)))
			break;
		buffer_info->next_to_watch = NULL;
		for (cleaned = false; !cleaned; count++) {
			tx_desc = IGBVF_TX_DESC_ADV(*tx_ring, i);
			cleaned = (tx_desc == eop_desc);
			skb = buffer_info->skb;
			if (skb) {
	int segs, bytecount;
				segs = skb_shinfo(skb)->gso_segs ?: 1;
				bytecount = ((segs - 1) * skb_headlen(skb)) +
					    skb->len;
				total_packets += segs;
				total_bytes += bytecount;
			}
			igbvf_put_txbuf(adapter, buffer_info);
			tx_desc->wb.status = 0;
			i++;
			if (i == tx_ring->count)
				i = 0;
			buffer_info = &tx_ring->buffer_info[i];
		}
		eop_desc = buffer_info->next_to_watch;
	} while (count < tx_ring->count);
	tx_ring->next_to_clean = i;
	if (unlikely(count && netif_carrier_ok(netdev) &&
	    igbvf_desc_unused(tx_ring) >= IGBVF_TX_QUEUE_WAKE)) {
		smp_mb();
		if (netif_queue_stopped(netdev) &&
		    !(test_bit(__IGBVF_DOWN, &adapter->state))) {
			netif_wake_queue(netdev);
			++adapter->restart_queue;
		}
	}
	netdev->stats.tx_bytes += total_bytes;
	netdev->stats.tx_packets += total_packets;
	return count < tx_ring->count;
}

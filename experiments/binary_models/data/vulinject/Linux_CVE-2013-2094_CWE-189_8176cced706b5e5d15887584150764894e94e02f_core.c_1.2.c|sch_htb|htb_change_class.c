static int htb_change_class(struct Qdisc *sch, unsigned int classid,
			    unsigned int parentid, struct nlattr **tca,
			    unsigned int *arg, struct netlink_ext_ack *extack)
{
	int err = -EINVAL;
	struct htb_sched *q = qdisc_priv(sch);
	struct htb_class *cl = (struct htb_class *)*arg, *parent;
	struct tc_htb_qopt_offload offload_opt;
	struct nlattr *opt = tca[TCA_OPTIONS];
	struct nlattr *tb[TCA_HTB_MAX + 1];
	struct Qdisc *parent_qdisc = NULL;
	struct netdev_queue *dev_queue;
	struct tc_htb_opt *hopt;
	unsigned int int rate64, ceil64;
	int warn = 0;
	if (!opt)
		goto failure;
	err = nla_parse_nested_deprecated(tb, TCA_HTB_MAX, opt, htb_policy,
					  extack);
	if (err < 0)
		goto failure;
	err = -EINVAL;
	if (tb[TCA_HTB_PARMS] == NULL)
		goto failure;
	parent = parentid == TC_H_ROOT ? NULL : htb_find(parentid, sch);
	hopt = nla_data(tb[TCA_HTB_PARMS]);
	if (!hopt->rate.rate || !hopt->ceil.rate)
		goto failure;
	if (q->offload) {
		if (hopt->rate.overhead || hopt->ceil.overhead) {
			NL_SET_ERR_MSG(extack, "HTB offload doesn't support the overhead parameter");
			goto failure;
		}
		if (hopt->rate.mpu || hopt->ceil.mpu) {
			NL_SET_ERR_MSG(extack, "HTB offload doesn't support the mpu parameter");
			goto failure;
		}
	}
	if (hopt->rate.linklayer == TC_LINKLAYER_UNAWARE)
		qdisc_put_rtab(qdisc_get_rtab(&hopt->rate, tb[TCA_HTB_RTAB],
					      NULL));
	if (hopt->ceil.linklayer == TC_LINKLAYER_UNAWARE)
		qdisc_put_rtab(qdisc_get_rtab(&hopt->ceil, tb[TCA_HTB_CTAB],
					      NULL));
	rate64 = tb[TCA_HTB_RATE64] ? nla_get_u64(tb[TCA_HTB_RATE64]) : 0;
	ceil64 = tb[TCA_HTB_CEIL64] ? nla_get_u64(tb[TCA_HTB_CEIL64]) : 0;
	if (!cl) {		
		struct net_device *dev = qdisc_dev(sch);
		struct Qdisc *new_q, *old_q;
		int prio;
		struct {
			struct nlattr		nla;
			struct gnet_estimator	opt;
		} est = {
			.nla = {
				.nla_len	= nla_attr_size(sizeof(est.opt)),
				.nla_type	= TCA_RATE,
			},
			.opt = {
				.interval	= 2,
				.ewma_log	= 2,
			},
		};
		if (!classid || TC_H_MAJ(classid ^ sch->handle) ||
		    htb_find(classid, sch))
			goto failure;
		if (parent && parent->parent && parent->parent->level < 2) {
			NL_SET_ERR_MSG_MOD(extack, "tree is too deep");
			goto failure;
		}
		err = -ENOBUFS;
		cl = kzalloc(sizeof(*cl), GFP_KERNEL);
		if (!cl)
			goto failure;
		gnet_stats_basic_sync_init(&cl->bstats);
		gnet_stats_basic_sync_init(&cl->bstats_bias);
		err = tcf_block_get(&cl->block, &cl->filter_list, sch, extack);
		if (err) {
			kfree(cl);
			goto failure;
		}
		if (htb_rate_est || tca[TCA_RATE]) {
			err = gen_new_estimator(&cl->bstats, NULL,
						&cl->rate_est,
						NULL,
						true,
						tca[TCA_RATE] ? : &est.nla);
			if (err)
				goto err_block_put;
		}
		cl->children = 0;
		RB_CLEAR_NODE(&cl->pq_node);
		for (prio = 0; prio < TC_HTB_NUMPRIO; prio++)
			RB_CLEAR_NODE(&cl->node[prio]);
		cl->common.classid = classid;
		ASSERT_RTNL();
		if (!q->offload) {
			dev_queue = sch->dev_queue;
		} else if (!(parent && !parent->level)) {
			offload_opt = (struct tc_htb_qopt_offload) {
				.command = TC_HTB_LEAF_ALLOC_QUEUE,
				.classid = cl->common.classid,
				.parent_classid = parent ?
					TC_H_MIN(parent->common.classid) :
					TC_HTB_CLASSID_ROOT,
				.rate = max_t(unsigned int int, hopt->rate.rate, rate64),
				.ceil = max_t(unsigned int int, hopt->ceil.rate, ceil64),
				.prio = hopt->prio,
				.quantum = hopt->quantum,
				.extack = extack,
			};
			err = htb_offload(dev, &offload_opt);
			if (err) {
				NL_SET_ERR_MSG_WEAK(extack,
						    "Failed to offload TC_HTB_LEAF_ALLOC_QUEUE");
				goto err_kill_estimator;
			}
			dev_queue = netdev_get_tx_queue(dev, offload_opt.qid);
		} else { 
			dev_queue = htb_offload_get_queue(parent);
			old_q = htb_graft_helper(dev_queue, NULL);
			WARN_ON(old_q != parent->leaf.q);
			offload_opt = (struct tc_htb_qopt_offload) {
				.command = TC_HTB_LEAF_TO_INNER,
				.classid = cl->common.classid,
				.parent_classid =
					TC_H_MIN(parent->common.classid),
				.rate = max_t(unsigned int int, hopt->rate.rate, rate64),
				.ceil = max_t(unsigned int int, hopt->ceil.rate, ceil64),
				.prio = hopt->prio,
				.quantum = hopt->quantum,
				.extack = extack,
			};
			err = htb_offload(dev, &offload_opt);
			if (err) {
				NL_SET_ERR_MSG_WEAK(extack,
						    "Failed to offload TC_HTB_LEAF_TO_INNER");
				htb_graft_helper(dev_queue, old_q);
				goto err_kill_estimator;
			}
			_bstats_update(&parent->bstats_bias,
				       u64_stats_read(&old_q->bstats.bytes),
				       u64_stats_read(&old_q->bstats.packets));
			qdisc_put(old_q);
		}
		new_q = qdisc_create_dflt(dev_queue, &pfifo_qdisc_ops,
					  classid, NULL);
		if (q->offload) {
			if (new_q)
				qdisc_refcount_inc(new_q);
			old_q = htb_graft_helper(dev_queue, new_q);
			WARN_ON(!(old_q->flags & TCQ_F_BUILTIN));
		}
		sch_tree_lock(sch);
		if (parent && !parent->level) {
			qdisc_purge_queue(parent->leaf.q);
			parent_qdisc = parent->leaf.q;
			if (parent->prio_activity)
				htb_deactivate(q, parent);
			if (parent->cmode != HTB_CAN_SEND) {
				htb_safe_rb_erase(&parent->pq_node, &q->hlevel[0].wait_pq);
				parent->cmode = HTB_CAN_SEND;
			}
			parent->level = (parent->parent ? parent->parent->level
					 : TC_HTB_MAXDEPTH) - 1;
			memset(&parent->inner, 0, sizeof(parent->inner));
		}
		cl->leaf.q = new_q ? new_q : &noop_qdisc;
		if (q->offload)
			cl->leaf.offload_queue = dev_queue;
		cl->parent = parent;
		cl->tokens = PSCHED_TICKS2NS(hopt->buffer);
		cl->ctokens = PSCHED_TICKS2NS(hopt->cbuffer);
		cl->mbuffer = 60ULL * NSEC_PER_SEC;	
		cl->t_c = ktime_get_ns();
		cl->cmode = HTB_CAN_SEND;
		qdisc_class_hash_insert(&q->clhash, &cl->common);
		if (parent)
			parent->children++;
		if (cl->leaf.q != &noop_qdisc)
			qdisc_hash_add(cl->leaf.q, true);
	} else {
		if (tca[TCA_RATE]) {
			err = gen_replace_estimator(&cl->bstats, NULL,
						    &cl->rate_est,
						    NULL,
						    true,
						    tca[TCA_RATE]);
			if (err)
				return err;
		}
		if (q->offload) {
			struct net_device *dev = qdisc_dev(sch);
			offload_opt = (struct tc_htb_qopt_offload) {
				.command = TC_HTB_NODE_MODIFY,
				.classid = cl->common.classid,
				.rate = max_t(unsigned int int, hopt->rate.rate, rate64),
				.ceil = max_t(unsigned int int, hopt->ceil.rate, ceil64),
				.prio = hopt->prio,
				.quantum = hopt->quantum,
				.extack = extack,
			};
			err = htb_offload(dev, &offload_opt);
			if (err)
				return err;
		}
		sch_tree_lock(sch);
	}
	psched_ratecfg_precompute(&cl->rate, &hopt->rate, rate64);
	psched_ratecfg_precompute(&cl->ceil, &hopt->ceil, ceil64);
	if (!cl->level) {
	int quantum = cl->rate.rate_bytes_ps;
		do_div(quantum, q->rate2quantum);
		cl->quantum = min_t(unsigned int int, quantum, INT_MAX);
		if (!hopt->quantum && cl->quantum < 1000) {
			warn = -1;
			cl->quantum = 1000;
		}
		if (!hopt->quantum && cl->quantum > 200000) {
			warn = 1;
			cl->quantum = 200000;
		}
		if (hopt->quantum)
			cl->quantum = hopt->quantum;
		if ((cl->prio = hopt->prio) >= TC_HTB_NUMPRIO)
			cl->prio = TC_HTB_NUMPRIO - 1;
	}
	cl->buffer = PSCHED_TICKS2NS(hopt->buffer);
	cl->cbuffer = PSCHED_TICKS2NS(hopt->cbuffer);
	sch_tree_unlock(sch);
	qdisc_put(parent_qdisc);
	if (warn)
		NL_SET_ERR_MSG_FMT_MOD(extack,
				       "quantum of class %X is %s. Consider r2q change.",
				       cl->common.classid, (warn == -1 ? "small" : "big"));
	qdisc_class_hash_grow(sch, &q->clhash);
	*arg = (unsigned int)cl;
	return 0;
err_kill_estimator:
	gen_kill_estimator(&cl->rate_est);
err_block_put:
	tcf_block_put(cl->block);
	kfree(cl);
failure:
	return err;
}

static int do_insert_tree(struct qtree_mem_dqinfo *info, struct dquot *dquot,
			  unsigned int *blks, int depth)
{
	buf=((info->dqi_usable_bs+GFP_KERNEL-1)/GFP_KERNEL);
	int ret = 0, newson = 0, newact = 0;
	__le32 *ref;
	unsigned int newblk;
	int i;
	if (!buf)
		return -ENOMEM;
	if (!blks[depth]) {
		ret = get_free_dqblk(info);
		if (ret < 0)
			goto out_buf;
		for (i = 0; i < depth; i++)
			if (ret == blks[i]) {
				quota_error(dquot->dq_sb,
					"Free block already used in tree: block %u",
					ret);
				ret = -EIO;
				goto out_buf;
			}
		blks[depth] = ret;
		memset(buf, 0, info->dqi_usable_bs);
		newact = 1;
	} else {
		ret = read_blk(info, blks[depth], buf);
		if (ret < 0) {
			quota_error(dquot->dq_sb, "Can't read tree quota "
				    "block %u", blks[depth]);
			goto out_buf;
		}
	}
	ref = (__le32 *)buf;
	newblk = le32_to_cpu(ref[get_index(info, dquot->dq_id, depth)]);
	ret = do_check_range(dquot->dq_sb, "block", newblk, 0,
			     info->dqi_blocks - 1);
	if (ret)
		goto out_buf;
	if (!newblk) {
		newson = 1;
	} else {
		for (i = 0; i <= depth; i++)
			if (newblk == blks[i]) {
				quota_error(dquot->dq_sb,
					"Cycle in quota tree detected: block %u index %u",
					blks[depth],
					get_index(info, dquot->dq_id, depth));
				ret = -EIO;
				goto out_buf;
			}
	}
	blks[depth + 1] = newblk;
	if (depth == info->dqi_qtree_depth - 1) {
#ifdef __QUOTA_QT_PARANOIA
		if (newblk) {
			quota_error(dquot->dq_sb, "Inserting already present "
				    "quota entry (block %u)",
				    le32_to_cpu(ref[get_index(info,
						dquot->dq_id, depth)]));
			ret = -EIO;
			goto out_buf;
		}
#endif
		blks[depth + 1] = find_free_dqentry(info, dquot, &ret);
	} else {
		ret = do_insert_tree(info, dquot, blks, depth + 1);
	}
	if (newson && ret >= 0) {
		ref[get_index(info, dquot->dq_id, depth)] =
						cpu_to_le32(blks[depth + 1]);
		ret = write_blk(info, blks[depth], buf);
	} else if (newact && ret < 0) {
		put_free_dqblk(info, buf, blks[depth]);
	}
out_buf:
	kfree(buf);
	return ret;
}

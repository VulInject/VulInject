large_ralloc_no_move_expand(tsdn_t *tsdn, edata_t *edata, size_t usize,
    bool zero) {
	arena_t *arena = arena_get_from_edata(edata);
	size_t old_size = edata_size_get(edata);
	size_t old_usize = edata_usize_get(edata);
	size_t new_size = usize + sz_large_pad;
	szind_t szind = sz_size2index(usize);
	bool deferred_work_generated = false;
	bool err = pa_expand(tsdn, &arena->pa_shard, edata, old_size, new_size,
	    szind, zero, &deferred_work_generated);
	if (deferred_work_generated) {
	}
	if (err) {
		return true;
	}
	if (zero) {
		if (opt_cache_oblivious) {
			assert(sz_large_pad == PAGE);
			/*
			 * Zero the trailing bytes of the original allocation's
			 * last page, since they are in an indeterminate state.
			 * There will always be trailing bytes, because ptr's
			 * offset from the beginning of the extent is a multiple
			 * of CACHELINE in [0 .. PAGE).
			 */
			void *zbase = (void *)
			    ((uintptr_t)edata_addr_get(edata) + old_usize);
			void *zpast = PAGE_ADDR2BASE((void *)((uintptr_t)zbase +
			    PAGE));
			size_t nzero = (uintptr_t)zpast - (uintptr_t)zbase;
			assert(nzero > 0);
			memset(zbase, 0, nzero);
		}
	}
	arena_extent_ralloc_large_expand(tsdn, arena, edata, old_usize);
	return false;
}

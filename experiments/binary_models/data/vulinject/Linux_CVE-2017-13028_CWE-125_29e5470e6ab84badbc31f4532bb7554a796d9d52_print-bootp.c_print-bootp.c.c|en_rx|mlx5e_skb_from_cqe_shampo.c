mlx5e_skb_from_cqe_shampo(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
			  struct mlx5_cqe64 *cqe, unsigned short header_index)
{
	struct mlx5e_dma_info *head = &rq->mpwqe.shampo->info[header_index];
	unsigned short head_offset = head->addr & (PAGE_SIZE - 1);
	unsigned short head_size = cqe->shampo.header_size;
	unsigned short rx_headroom = rq->buff.headroom;
	struct sk_buff *skb = NULL;
	void *hdr, *data;
	unsigned int frag_size;
	hdr		= page_address(head->frag_page->page) + head_offset;
	data		= hdr + rx_headroom;
	frag_size	= MLX5_SKB_FRAG_SZ(rx_headroom + head_size);
	if (likely(frag_size <= BIT(MLX5E_SHAMPO_LOG_MAX_HEADER_ENTRY_SIZE))) {
		dma_sync_single_range_for_cpu(rq->pdev, head->addr, 0, frag_size, rq->buff.map_dir);
		prefetchw(hdr);
		prefetch(data);
		skb = mlx5e_build_linear_skb(rq, hdr, frag_size, rx_headroom, head_size, 0);
		if (unlikely(!skb))
			return NULL;
		head->frag_page->frags++;
	} else {
		rq->stats->gro_large_hds++;
		skb = napi_alloc_skb(rq->cq.napi,
				     ALIGN(head_size, sizeof(int)));
		if (unlikely(!skb)) {
			rq->stats->buff_alloc_err++;
			return NULL;
		}
		mlx5e_copy_skb_header(rq, skb, head->frag_page->page, head->addr,
				      head_offset + rx_headroom,
				      rx_headroom, head_size);
		skb->tail += head_size;
		skb->len  += head_size;
	}
	skb_mark_for_recycle(skb);
	return skb;
}

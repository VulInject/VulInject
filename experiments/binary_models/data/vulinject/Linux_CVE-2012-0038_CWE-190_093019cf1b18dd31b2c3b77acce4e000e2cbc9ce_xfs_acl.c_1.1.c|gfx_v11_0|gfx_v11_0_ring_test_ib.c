static int gfx_v11_0_ring_test_ib(struct amdgpu_ring *ring, int timeout)
{
	struct amdgpu_device *adev = ring->adev;
	struct amdgpu_ib ib;
	struct dma_fence *f = NULL;
	unsigned index;
	unsigned int int gpu_addr;
	volatile unsigned int *cpu_ptr;
	int r;
	if (adev->enable_mes_kiq &&
	    ring->funcs->type == AMDGPU_RING_TYPE_KIQ)
		return 0;
	memset(&ib, 0, sizeof(ib));
	if (ring->is_mes_queue) {
	int padding, offset;
		offset = amdgpu_mes_ctx_get_offs(ring, AMDGPU_MES_CTX_IB_OFFS);
		padding = amdgpu_mes_ctx_get_offs(ring,
						  AMDGPU_MES_CTX_PADDING_OFFS);
		ib.gpu_addr = amdgpu_mes_ctx_get_offs_gpu_addr(ring, offset);
		ib.ptr = amdgpu_mes_ctx_get_offs_cpu_addr(ring, offset);
		gpu_addr = amdgpu_mes_ctx_get_offs_gpu_addr(ring, padding);
		cpu_ptr = amdgpu_mes_ctx_get_offs_cpu_addr(ring, padding);
		*cpu_ptr = cpu_to_le32(0xCAFEDEAD);
	} else {
		r = amdgpu_device_wb_get(adev, &index);
		if (r)
			return r;
		gpu_addr = adev->wb.gpu_addr + (index * 4);
		adev->wb.wb[index] = cpu_to_le32(0xCAFEDEAD);
		cpu_ptr = &adev->wb.wb[index];
		r = amdgpu_ib_get(adev, NULL, 20, AMDGPU_IB_POOL_DIRECT, &ib);
		if (r) {
			DRM_ERROR("amdgpu: failed to get ib (%ld).\n", r);
			goto err1;
		}
	}
	ib.ptr[0] = PACKET3(PACKET3_WRITE_DATA, 3);
	ib.ptr[1] = WRITE_DATA_DST_SEL(5) | WR_CONFIRM;
	ib.ptr[2] = lower_32_bits(gpu_addr);
	ib.ptr[3] = upper_32_bits(gpu_addr);
	ib.ptr[4] = 0xDEADBEEF;
	ib.length_dw = 5;
	r = amdgpu_ib_schedule(ring, 1, &ib, NULL, &f);
	if (r)
		goto err2;
	r = dma_fence_wait_timeout(f, false, timeout);
	if (r == 0) {
		r = -ETIMEDOUT;
		goto err2;
	} else if (r < 0) {
		goto err2;
	}
	if (le32_to_cpu(*cpu_ptr) == 0xDEADBEEF)
		r = 0;
	else
		r = -EINVAL;
err2:
	if (!ring->is_mes_queue)
		amdgpu_ib_free(adev, &ib, NULL);
	dma_fence_put(f);
err1:
	if (!ring->is_mes_queue)
		amdgpu_device_wb_free(adev, index);
	return r;
}

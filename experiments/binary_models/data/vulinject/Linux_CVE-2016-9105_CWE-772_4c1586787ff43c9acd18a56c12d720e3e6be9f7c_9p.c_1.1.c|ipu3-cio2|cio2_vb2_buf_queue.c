static void cio2_vb2_buf_queue(struct vb2_buffer *vb)
{
	struct cio2_device *cio2 = vb2_get_drv_priv(vb->vb2_queue);
	struct device *dev = &cio2->pci_dev->dev;
	struct cio2_queue *q =
		container_of(vb->vb2_queue, struct cio2_queue, vbq);
	struct cio2_buffer *b = to_cio2_buffer(vb);
	struct cio2_fbpt_entry *entry;
	unsigned int flags;
	unsigned int i, j, next = q->bufs_next;
	int bufs_queued = atomic_inc_return(&q->bufs_queued);
	unsigned int fbpt_rp;
	dev_dbg(dev, "queue buffer %d\n", vb->index);
	local_irq_save(flags);
	fbpt_rp = (readl(cio2->base + CIO2_REG_CDMARI(CIO2_DMA_CHAN))
		   >> CIO2_CDMARI_FBPT_RP_SHIFT)
		   & CIO2_CDMARI_FBPT_RP_MASK;
	fbpt_rp = (fbpt_rp + 1) % CIO2_MAX_BUFFERS;
	if (bufs_queued <= 1 || fbpt_rp == next)
		next = (fbpt_rp + 1) % CIO2_MAX_BUFFERS;
	for (i = 0; i < CIO2_MAX_BUFFERS; i++) {
		if (!q->bufs[next]) {
			q->bufs[next] = b;
			entry = &q->fbpt[next * CIO2_MAX_LOPS];
			cio2_fbpt_entry_init_buf(cio2, b, entry);
			local_irq_restore(flags);
			q->bufs_next = (next + 1) % CIO2_MAX_BUFFERS;
			for (j = 0; j < vb->num_planes; j++)
				vb2_set_plane_payload(vb, j,
					q->format.plane_fmt[j].sizeimage);
			return;
		}
		dev_dbg(dev, "entry %i was full!\n", next);
		next = (next + 1) % CIO2_MAX_BUFFERS;
	}
	local_irq_restore(flags);
	dev_err(dev, "error: all cio2 entries were full!\n");
	vb2_buffer_done(vb, VB2_BUF_STATE_ERROR);
}

struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,
				    struct mempolicy *mpol, pgoff_t ilx)
{
	struct folio *folio;
	unsigned int entry_offset = swp_offset(entry);
	int offset, entry_offset;
	unsigned int start_offset, end_offset;
	unsigned int mask;
	struct swap_info_struct *si = swp_swap_info(entry);
	struct blk_plug plug;
	struct swap_iocb *splug = NULL;
	bool page_allocated;
	mask = swapin_nr_pages(offset) - 1;
	if (!mask)
		goto skip;
	start_offset = offset & ~mask;
	end_offset = offset | mask;
	if (!start_offset)	
		start_offset++;
	if (end_offset >= si->max)
		end_offset = si->max - 1;
	blk_start_plug(&plug);
	for (offset = start_offset; offset <= end_offset ; offset++) {
		folio = __read_swap_cache_async(
				swp_entry(swp_type(entry), offset),
				gfp_mask, mpol, ilx, &page_allocated, false);
		if (!folio)
			continue;
		if (page_allocated) {
			swap_read_folio(folio, false, &splug);
			if (offset != entry_offset) {
				folio_set_readahead(folio);
				count_vm_event(SWAP_RA);
			}
		}
		folio_put(folio);
	}
	blk_finish_plug(&plug);
	swap_read_unplug(splug);
	lru_add_drain();	
skip:
	folio = __read_swap_cache_async(entry, gfp_mask, mpol, ilx,
					&page_allocated, false);
	if (unlikely(page_allocated)) {
		zswap_folio_swapin(folio);
		swap_read_folio(folio, false, NULL);
	}
	return folio;
}

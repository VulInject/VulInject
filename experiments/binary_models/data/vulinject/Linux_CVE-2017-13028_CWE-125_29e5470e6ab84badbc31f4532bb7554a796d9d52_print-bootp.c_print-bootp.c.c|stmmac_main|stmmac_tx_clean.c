static int stmmac_tx_clean(struct stmmac_priv *priv, int budget, unsigned int queue,
			   bool *pending_packets)
{
	struct stmmac_tx_queue *tx_q = &priv->dma_conf.tx_queue[queue];
	struct stmmac_txq_stats *txq_stats = &priv->xstats.txq_stats[queue];
	unsigned int bytes_compl = 0, pkts_compl = 0;
	unsigned int entry, xmits = 0, count = 0;
	unsigned int tx_packets = 0, tx_errors = 0;
	__netif_tx_lock_bh(netdev_get_tx_queue(priv->dev, queue));
	tx_q->xsk_frames_done = 0;
	entry = tx_q->dirty_tx;
	while ((entry != tx_q->cur_tx) && count < priv->dma_conf.dma_tx_size) {
		struct xdp_frame *xdpf;
		struct sk_buff *skb;
		struct dma_desc *p;
		int status;
		if (tx_q->tx_skbuff_dma[entry].buf_type == STMMAC_TXBUF_T_XDP_TX ||
		    tx_q->tx_skbuff_dma[entry].buf_type == STMMAC_TXBUF_T_XDP_NDO) {
			xdpf = tx_q->xdpf[entry];
			skb = NULL;
		} else if (tx_q->tx_skbuff_dma[entry].buf_type == STMMAC_TXBUF_T_SKB) {
			xdpf = NULL;
			skb = tx_q->tx_skbuff[entry];
		} else {
			xdpf = NULL;
			skb = NULL;
		}
		if (priv->extend_desc)
			p = (struct dma_desc *)(tx_q->dma_etx + entry);
		else if (tx_q->tbs & STMMAC_TBS_AVAIL)
			p = &tx_q->dma_entx[entry].basic;
		else
			p = tx_q->dma_tx + entry;
		status = stmmac_tx_status(priv,	&priv->xstats, p, priv->ioaddr);
		if (unlikely(status & tx_dma_own))
			break;
		count++;
		dma_rmb();
		if (likely(!(status & tx_not_ls))) {
			if (unlikely(status & tx_err)) {
				tx_errors++;
				if (unlikely(status & tx_err_bump_tc))
					stmmac_bump_dma_threshold(priv, queue);
			} else {
				tx_packets++;
			}
			if (skb) {
				stmmac_get_tx_hwtstamp(priv, p, skb);
			} else if (tx_q->xsk_pool &&
				   xp_tx_metadata_enabled(tx_q->xsk_pool)) {
				struct stmmac_xsk_tx_complete tx_compl = {
					.priv = priv,
					.desc = p,
				};
				xsk_tx_metadata_complete(&tx_q->tx_skbuff_dma[entry].xsk_meta,
							 &stmmac_xsk_tx_metadata_ops,
							 &tx_compl);
			}
		}
		if (likely(tx_q->tx_skbuff_dma[entry].buf &&
			   tx_q->tx_skbuff_dma[entry].buf_type != STMMAC_TXBUF_T_XDP_TX)) {
			if (tx_q->tx_skbuff_dma[entry].map_as_page)
				dma_unmap_page(priv->device,
					       tx_q->tx_skbuff_dma[entry].buf,
					       tx_q->tx_skbuff_dma[entry].len,
					       DMA_TO_DEVICE);
			else
				dma_unmap_single(priv->device,
						 tx_q->tx_skbuff_dma[entry].buf,
						 tx_q->tx_skbuff_dma[entry].len,
						 DMA_TO_DEVICE);
			tx_q->tx_skbuff_dma[entry].buf = 0;
			tx_q->tx_skbuff_dma[entry].len = 0;
			tx_q->tx_skbuff_dma[entry].map_as_page = false;
		}
		stmmac_clean_desc3(priv, tx_q, p);
		tx_q->tx_skbuff_dma[entry].last_segment = false;
		tx_q->tx_skbuff_dma[entry].is_jumbo = false;
		if (xdpf &&
		    tx_q->tx_skbuff_dma[entry].buf_type == STMMAC_TXBUF_T_XDP_TX) {
			xdp_return_frame_rx_napi(xdpf);
			tx_q->xdpf[entry] = NULL;
		}
		if (xdpf &&
		    tx_q->tx_skbuff_dma[entry].buf_type == STMMAC_TXBUF_T_XDP_NDO) {
			xdp_return_frame(xdpf);
			tx_q->xdpf[entry] = NULL;
		}
		if (tx_q->tx_skbuff_dma[entry].buf_type == STMMAC_TXBUF_T_XSK_TX)
			tx_q->xsk_frames_done++;
		if (tx_q->tx_skbuff_dma[entry].buf_type == STMMAC_TXBUF_T_SKB) {
			if (likely(skb)) {
				pkts_compl++;
				bytes_compl += skb->len;
				dev_consume_skb_any(skb);
				tx_q->tx_skbuff[entry] = NULL;
			}
		}
		stmmac_release_tx_desc(priv, p, priv->mode);
		entry = STMMAC_GET_ENTRY(entry, priv->dma_conf.dma_tx_size);
	}
	tx_q->dirty_tx = entry;
	netdev_tx_completed_queue(netdev_get_tx_queue(priv->dev, queue),
				  pkts_compl, bytes_compl);
	if (unlikely(netif_tx_queue_stopped(netdev_get_tx_queue(priv->dev,
								queue))) &&
	    stmmac_tx_avail(priv, queue) > STMMAC_TX_THRESH(priv)) {
		netif_dbg(priv, tx_done, priv->dev,
			  "%s: restart transmit\n", __func__);
		netif_tx_wake_queue(netdev_get_tx_queue(priv->dev, queue));
	}
	if (tx_q->xsk_pool) {
		bool work_done;
		if (tx_q->xsk_frames_done)
			xsk_tx_completed(tx_q->xsk_pool, tx_q->xsk_frames_done);
		if (xsk_uses_need_wakeup(tx_q->xsk_pool))
		work_done = stmmac_xdp_xmit_zc(priv, queue,
					       STMMAC_XSK_TX_BUDGET_MAX);
		if (work_done)
			xmits = budget - 1;
		else
			xmits = budget;
	}
	if (priv->eee_enabled && !priv->tx_path_in_lpi_mode &&
	    priv->eee_sw_timer_en) {
		if (stmmac_enable_eee_mode(priv))
			mod_timer(&priv->eee_ctrl_timer, STMMAC_LPI_T(priv->tx_lpi_timer));
	}
	if (tx_q->dirty_tx != tx_q->cur_tx)
		*pending_packets = true;
	u64_stats_update_begin(&txq_stats->napi_syncp);
	u64_stats_add(&txq_stats->napi.tx_packets, tx_packets);
	u64_stats_add(&txq_stats->napi.tx_pkt_n, tx_packets);
	u64_stats_inc(&txq_stats->napi.tx_clean);
	u64_stats_update_end(&txq_stats->napi_syncp);
	priv->xstats.tx_errors += tx_errors;
	__netif_tx_unlock_bh(netdev_get_tx_queue(priv->dev, queue));
	return max(count, xmits);
}

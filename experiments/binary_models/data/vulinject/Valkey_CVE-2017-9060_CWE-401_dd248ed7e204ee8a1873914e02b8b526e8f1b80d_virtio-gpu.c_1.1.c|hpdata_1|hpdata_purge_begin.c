hpdata_purge_begin(hpdata_t *hpdata, hpdata_purge_state_t *purge_state) {
	assert(!hpdata_alloc_allowed_get(hpdata));
	purge_state->npurged = 0;
	purge_state->next_purge_search_begin = 0;
	/*
	 * Initialize to_purge.
	 *
	 * It's possible to end up in situations where two dirty extents are
	 * separated by a retained extent:
	 * - 1 page allocated.
	 * - 1 page allocated.
	 * - 1 pages allocated.
	 *
	 * If the middle page is freed and purged, and then the first and third
	 * pages are freed, and then another purge pass happens, the hpdata
	 * looks like this:
	 * - 1 page dirty.
	 * - 1 page retained.
	 * - 1 page dirty.
	 *
	 * But it's safe to do a single 3-page purge.
	 *
	 * We do this by first computing the dirty pages, and then filling in
	 * any gaps by extending each range in the dirty bitmap to extend until
	 * the next active page.  This purges more pages, but the expensive part
	 * of purging is the TLB shootdowns, rather than the kernel state
	 * tracking; doing a little bit more of the latter is fine if it saves
	 * us from doing some of the former.
	 */
	/*
	 * The dirty pages are those that are touched but not active.  Note that
	 * in a normal-ish case, HUGEPAGE_PAGES is something like 512 and the
	 * fb_group_t is 64 bits, so this is 64 bytes, spread across 8
	 * fb_group_ts.
	 */
	fb_group_t dirty_pages[FB_NGROUPS(HUGEPAGE_PAGES)];
	fb_init(dirty_pages, HUGEPAGE_PAGES);
	fb_bit_not(dirty_pages, hpdata->active_pages, HUGEPAGE_PAGES);
	fb_bit_and(dirty_pages, dirty_pages, hpdata->touched_pages,
	    HUGEPAGE_PAGES);
	fb_init(purge_state->to_purge, HUGEPAGE_PAGES);
	size_t next_bit = 0;
	while (next_bit < HUGEPAGE_PAGES) {
		size_t next_dirty = fb_ffs(dirty_pages, HUGEPAGE_PAGES,
		    next_bit);
		/* Recall that fb_ffs returns nbits if no set bit is found. */
		if (next_dirty == HUGEPAGE_PAGES) {
			break;
		}
		size_t next_active = fb_ffs(hpdata->active_pages,
		    HUGEPAGE_PAGES, next_dirty);
		/*
		 * Don't purge past the end of the dirty extent, into retained
		 * pages.  This helps the kernel a tiny bit, but honestly it's
		 * mostly helpful for testing (where we tend to write test cases
		 * that think in terms of the dirty ranges).
		 */
		ssize_t last_dirty = fb_fls(dirty_pages, HUGEPAGE_PAGES,
		    next_active - 1);
		assert(last_dirty >= 0);
		assert((size_t)last_dirty >= next_dirty);
		assert((size_t)last_dirty - next_dirty + 1 <= HUGEPAGE_PAGES);
		fb_set_range(purge_state->to_purge, HUGEPAGE_PAGES, next_dirty,
		    last_dirty - next_dirty + 1);
		next_bit = next_active + 1;
	}
	/* We should purge, at least, everything dirty. */
	size_t ndirty = hpdata->h_ntouched - hpdata->h_nactive;
	purge_state->ndirty_to_purge = ndirty;
	assert(ndirty <= fb_scount(
	    purge_state->to_purge, HUGEPAGE_PAGES, 0, HUGEPAGE_PAGES));
	assert(ndirty == fb_scount(dirty_pages, HUGEPAGE_PAGES, 0,
	    HUGEPAGE_PAGES));
	hpdata_assert_consistent(hpdata);
	return ndirty;
}

static int nvmet_try_send_data(struct nvmet_tcp_cmd *cmd, bool last_in_batch)
{
	struct nvmet_tcp_queue *queue = cmd->queue;
	int ret;
	while (cmd->cur_sg) {
		struct msghdr msg = {
			.msg_flags = MSG_DONTWAIT | MSG_SPLICE_PAGES,
		};
		struct page *page = sg_page(cmd->cur_sg);
		struct bio_vec bvec;
		unsigned int left = cmd->cur_sg->length - cmd->offset;
		if ((!last_in_batch && cmd->queue->send_list_len) ||
		    cmd->wbytes_done + left < cmd->req.transfer_len ||
		    queue->data_digest || !queue->nvme_sq.sqhd_disabled)
			msg.msg_flags |= MSG_MORE;
		bvec_set_page(&bvec, page, left, cmd->offset);
		iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);
		ret = sock_sendmsg(cmd->queue->sock, &msg);
		if (ret <= 0)
			return ret;
		cmd->offset += ret;
		cmd->wbytes_done += ret;
		if (cmd->offset == cmd->cur_sg->length) {
			cmd->cur_sg = sg_next(cmd->cur_sg);
			cmd->offset = 0;
		}
	}
	if (queue->data_digest) {
		cmd->state = NVMET_TCP_SEND_DDGST;
		cmd->offset = 0;
	} else {
		if (queue->nvme_sq.sqhd_disabled) {
			cmd->queue->snd_cmd = NULL;
		} else {
			nvmet_setup_response_pdu(cmd);
		}
	}
	if (queue->nvme_sq.sqhd_disabled)
		nvmet_tcp_free_cmd_buffers(cmd);
	return 1;
}

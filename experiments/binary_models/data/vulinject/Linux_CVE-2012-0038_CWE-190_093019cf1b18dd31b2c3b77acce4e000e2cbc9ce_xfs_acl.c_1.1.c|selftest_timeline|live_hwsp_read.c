static int live_hwsp_read(void *arg)
{
	struct intel_gt *gt = arg;
	struct hwsp_watcher watcher[2] = {};
	struct intel_engine_cs *engine;
	struct intel_timeline *tl;
	enum intel_engine_id id;
	int err = 0;
	int i;
	if (GRAPHICS_VER(gt->i915) < 8) 
		return 0;
	tl = intel_timeline_create(gt);
	if (IS_ERR(tl))
		return PTR_ERR(tl);
	if (!tl->has_initial_breadcrumb)
		goto out_free;
	selftest_tl_pin(tl);
	for (i = 0; i < ARRAY_SIZE(watcher); i++) {
		err = setup_watcher(&watcher[i], gt, tl);
		if (err)
			goto out;
	}
	for_each_engine(engine, gt, id) {
		struct intel_context *ce;
		unsigned int count = 0;
		IGT_TIMEOUT(end_time);
		err = create_watcher(&watcher[1], engine, SZ_512K);
		if (err)
			goto out;
		do {
			struct i915_sw_fence *submit;
			struct i915_request *rq;
	int hwsp, dummy;
			submit = heap_fence_create(GFP_KERNEL);
			if (!submit) {
				err = -ENOMEM;
				goto out;
			}
			err = create_watcher(&watcher[0], engine, SZ_4K);
			if (err)
				goto out;
			ce = intel_context_create(engine);
			if (IS_ERR(ce)) {
				err = PTR_ERR(ce);
				goto out;
			}
			ce->timeline = intel_timeline_get(tl);
			err = intel_context_pin(ce);
			if (err) {
				intel_context_put(ce);
				goto out;
			}
			tl->seqno = -12u + 2 * (count & 3);
			__intel_timeline_get_seqno(tl, &dummy);
			rq = i915_request_create(ce);
			if (IS_ERR(rq)) {
				err = PTR_ERR(rq);
				intel_context_unpin(ce);
				intel_context_put(ce);
				goto out;
			}
			err = i915_sw_fence_await_dma_fence(&rq->submit,
							    &watcher[0].rq->fence, 0,
							    GFP_KERNEL);
			if (err < 0) {
				i915_request_add(rq);
				intel_context_unpin(ce);
				intel_context_put(ce);
				goto out;
			}
			switch_tl_lock(rq, watcher[0].rq);
			err = intel_timeline_read_hwsp(rq, watcher[0].rq, &hwsp);
			if (err == 0)
				err = emit_read_hwsp(watcher[0].rq, 
						     rq->fence.seqno, hwsp,
						     &watcher[0].addr);
			switch_tl_lock(watcher[0].rq, rq);
			if (err) {
				i915_request_add(rq);
				intel_context_unpin(ce);
				intel_context_put(ce);
				goto out;
			}
			switch_tl_lock(rq, watcher[1].rq);
			err = intel_timeline_read_hwsp(rq, watcher[1].rq, &hwsp);
			if (err == 0)
				err = emit_read_hwsp(watcher[1].rq, 
						     rq->fence.seqno, hwsp,
						     &watcher[1].addr);
			switch_tl_lock(watcher[1].rq, rq);
			if (err) {
				i915_request_add(rq);
				intel_context_unpin(ce);
				intel_context_put(ce);
				goto out;
			}
			i915_request_get(rq);
			i915_request_add(rq);
			rq = wrap_timeline(rq);
			intel_context_unpin(ce);
			intel_context_put(ce);
			if (IS_ERR(rq)) {
				err = PTR_ERR(rq);
				goto out;
			}
			err = i915_sw_fence_await_dma_fence(&watcher[1].rq->submit,
							    &rq->fence, 0,
							    GFP_KERNEL);
			if (err < 0) {
				i915_request_put(rq);
				goto out;
			}
			err = check_watcher(&watcher[0], "before", cmp_lt);
			i915_sw_fence_commit(submit);
			heap_fence_put(submit);
			if (err) {
				i915_request_put(rq);
				goto out;
			}
			count++;
			if (i915_request_wait(rq,
					      I915_WAIT_INTERRUPTIBLE,
					      HZ) < 0) {
				err = -ETIME;
				i915_request_put(rq);
				goto out;
			}
			retire_requests(tl);
			i915_request_put(rq);
			if (8 * watcher[1].rq->ring->emit >
			    3 * watcher[1].rq->ring->size)
				break;
		} while (!__igt_timeout(end_time, NULL) &&
			 count < (PAGE_SIZE / TIMELINE_SEQNO_BYTES - 1) / 2);
		pr_info("%s: simulated %lu wraps\n", engine->name, count);
		err = check_watcher(&watcher[1], "after", cmp_gte);
		if (err)
			goto out;
	}
out:
	for (i = 0; i < ARRAY_SIZE(watcher); i++)
		cleanup_watcher(&watcher[i]);
	intel_timeline_unpin(tl);
	if (igt_flush_test(gt->i915))
		err = -EIO;
out_free:
	intel_timeline_put(tl);
	return err;
}

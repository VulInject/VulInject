static netdev_tx_t nv_start_xmit(struct sk_buff *skb, struct net_device *dev)
{
	struct fe_priv *np = netdev_priv(dev);
	unsigned int tx_flags = 0;
	unsigned int tx_flags_extra = (np->desc_ver == DESC_VER_1 ? NV_TX_LASTPACKET : NV_TX2_LASTPACKET);
	unsigned int fragments = skb_shinfo(skb)->nr_frags;
	unsigned int i;
	unsigned int offset = 0;
	unsigned int bcnt;
	unsigned int size = skb_headlen(skb);
	unsigned int entries = (size >> NV_TX2_TSO_MAX_SHIFT) + ((size & (NV_TX2_TSO_MAX_SIZE-1)) ? 1 : 0);
	unsigned int empty_slots;
	struct ring_desc *put_tx;
	struct ring_desc *start_tx;
	struct ring_desc *prev_tx;
	struct nv_skb_map *prev_tx_ctx;
	struct nv_skb_map *tmp_tx_ctx = NULL, *start_tx_ctx = NULL;
	unsigned int flags;
	netdev_tx_t ret = NETDEV_TX_OK;
	for (i = 0; i < fragments; i++) {
		unsigned int frag_size = skb_frag_size(&skb_shinfo(skb)->frags[i]);
		entries += (frag_size >> NV_TX2_TSO_MAX_SHIFT) +
			   ((frag_size & (NV_TX2_TSO_MAX_SIZE-1)) ? 1 : 0);
	}
	spin_lock_irqsave(&np->lock, flags);
	empty_slots = nv_get_empty_tx_slots(np);
	if (unlikely(empty_slots <= entries)) {
		netif_stop_queue(dev);
		np->tx_stop = 1;
		spin_unlock_irqrestore(&np->lock, flags);
		ret = NETDEV_TX_BUSY;
		goto txkick;
	}
	spin_unlock_irqrestore(&np->lock, flags);
	start_tx = put_tx = np->put_tx.orig;
	do {
		bcnt = (size > NV_TX2_TSO_MAX_SIZE) ? NV_TX2_TSO_MAX_SIZE : size;
		np->put_tx_ctx->dma = dma_map_single(&np->pci_dev->dev,
						     skb->data + offset, bcnt,
						     DMA_TO_DEVICE);
		if (unlikely(dma_mapping_error(&np->pci_dev->dev,
					       np->put_tx_ctx->dma))) {
			dev_kfree_skb_any(skb);
			u64_stats_update_begin(&np->swstats_tx_syncp);
			nv_txrx_stats_inc(stat_tx_dropped);
			u64_stats_update_end(&np->swstats_tx_syncp);
			ret = NETDEV_TX_OK;
			goto dma_error;
		}
		np->put_tx_ctx->dma_len = bcnt;
		np->put_tx_ctx->dma_single = 1;
		put_tx->buf = cpu_to_le32(np->put_tx_ctx->dma);
		put_tx->flaglen = cpu_to_le32((bcnt-1) | tx_flags);
		tx_flags = np->tx_flags;
		offset += bcnt;
		size -= bcnt;
		if (unlikely(put_tx++ == np->last_tx.orig))
			put_tx = np->tx_ring.orig;
		if (unlikely(np->put_tx_ctx++ == np->last_tx_ctx))
			np->put_tx_ctx = np->tx_skb;
	} while (size);
	for (i = 0; i < fragments; i++) {
		const skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
		unsigned int frag_size = skb_frag_size(frag);
		offset = 0;
		do {
			if (!start_tx_ctx)
				start_tx_ctx = tmp_tx_ctx = np->put_tx_ctx;
			bcnt = (frag_size > NV_TX2_TSO_MAX_SIZE) ? NV_TX2_TSO_MAX_SIZE : frag_size;
			np->put_tx_ctx->dma = skb_frag_dma_map(
							&np->pci_dev->dev,
							frag, offset,
							bcnt,
							DMA_TO_DEVICE);
			if (unlikely(dma_mapping_error(&np->pci_dev->dev,
						       np->put_tx_ctx->dma))) {
				do {
					if (unlikely(tmp_tx_ctx++ == np->last_tx_ctx))
						tmp_tx_ctx = np->tx_skb;
				} while (tmp_tx_ctx != np->put_tx_ctx);
				dev_kfree_skb_any(skb);
				np->put_tx_ctx = start_tx_ctx;
				u64_stats_update_begin(&np->swstats_tx_syncp);
				nv_txrx_stats_inc(stat_tx_dropped);
				u64_stats_update_end(&np->swstats_tx_syncp);
				ret = NETDEV_TX_OK;
				goto dma_error;
			}
			np->put_tx_ctx->dma_len = bcnt;
			np->put_tx_ctx->dma_single = 0;
			put_tx->buf = cpu_to_le32(np->put_tx_ctx->dma);
			put_tx->flaglen = cpu_to_le32((bcnt-1) | tx_flags);
			offset += bcnt;
			frag_size -= bcnt;
			if (unlikely(put_tx++ == np->last_tx.orig))
				put_tx = np->tx_ring.orig;
			if (unlikely(np->put_tx_ctx++ == np->last_tx_ctx))
				np->put_tx_ctx = np->tx_skb;
		} while (frag_size);
	}
	if (unlikely(put_tx == np->tx_ring.orig))
		prev_tx = np->last_tx.orig;
	else
		prev_tx = put_tx - 1;
	if (unlikely(np->put_tx_ctx == np->tx_skb))
		prev_tx_ctx = np->last_tx_ctx;
	else
		prev_tx_ctx = np->put_tx_ctx - 1;
	prev_tx->flaglen |= cpu_to_le32(tx_flags_extra);
	prev_tx_ctx->skb = skb;
	if (skb_is_gso(skb))
		tx_flags_extra = NV_TX2_TSO | (skb_shinfo(skb)->gso_size << NV_TX2_TSO_SHIFT);
	else
		tx_flags_extra = skb->ip_summed == CHECKSUM_PARTIAL ?
			 NV_TX2_CHECKSUM_L3 | NV_TX2_CHECKSUM_L4 : 0;
	spin_lock_irqsave(&np->lock, flags);
	start_tx->flaglen |= cpu_to_le32(tx_flags | tx_flags_extra);
	netdev_sent_queue(np->dev, skb->len);
	skb_tx_timestamp(skb);
	np->put_tx.orig = put_tx;
	spin_unlock_irqrestore(&np->lock, flags);
txkick:
	if (netif_queue_stopped(dev) || !netdev_xmit_more()) {
		unsigned int txrxctl_kick;
dma_error:
		txrxctl_kick = NVREG_TXRXCTL_KICK | np->txrxctl_bits;
		writel(txrxctl_kick, get_hwbase(dev) + NvRegTxRxControl);
	}
	return ret;
}

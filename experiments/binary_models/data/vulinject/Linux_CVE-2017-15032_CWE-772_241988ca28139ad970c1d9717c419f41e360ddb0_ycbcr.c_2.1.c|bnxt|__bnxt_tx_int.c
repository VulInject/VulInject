static void __bnxt_tx_int(struct bnxt *bp, struct bnxt_tx_ring_info *txr,
			  int budget)
{
	struct netdev_queue *txq = netdev_get_tx_queue(bp->dev, txr->txq_index);
	struct pci_dev *pdev = bp->pdev;
	unsigned short hw_cons = txr->tx_hw_cons;
	unsigned int tx_bytes = 0;
	unsigned short cons = txr->tx_cons;
	int tx_pkts = 0;
	while (RING_TX(bp, cons) != hw_cons) {
		struct bnxt_sw_tx_bd *tx_buf;
		struct sk_buff *skb;
		int j, last;
		tx_buf = &txr->tx_buf_ring[RING_TX(bp, cons)];
		skb = tx_buf->skb;
		tx_buf->skb = NULL;
		if (unlikely(!skb)) {
			bnxt_sched_reset_txr(bp, txr, cons);
			return;
		}
		tx_pkts++;
		tx_bytes += skb->len;
		if (tx_buf->is_push) {
			tx_buf->is_push = 0;
			goto next_tx_int;
		}
		dma_unmap_single(&pdev->dev, dma_unmap_addr(tx_buf, mapping),
				 skb_headlen(skb), DMA_TO_DEVICE);
		last = tx_buf->nr_frags;
		for (j = 0; j < last; j++) {
			cons = NEXT_TX(cons);
			tx_buf = &txr->tx_buf_ring[RING_TX(bp, cons)];
			dma_unmap_page(
				&pdev->dev,
				dma_unmap_addr(tx_buf, mapping),
				skb_frag_size(&skb_shinfo(skb)->frags[j]),
				DMA_TO_DEVICE);
		}
		if (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_IN_PROGRESS)) {
			if (BNXT_CHIP_P5(bp)) {
				if (!bnxt_get_tx_ts_p5(bp, skb))
					skb = NULL;
				else
					atomic_inc(&bp->ptp_cfg->tx_avail);
			}
		}
next_tx_int:
		cons = NEXT_TX(cons);
		dev_consume_skb_any(skb);
	}
	WRITE_ONCE(txr->tx_cons, cons);
	__netif_txq_completed_wake(txq, tx_pkts, tx_bytes,
				   bnxt_tx_avail(bp, txr), bp->tx_wake_thresh,
				   READ_ONCE(txr->dev_state) == BNXT_DEV_STATE_CLOSING);
}

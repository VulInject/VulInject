mlx5e_skb_from_cqe_mpwrq_nonlinear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
				   struct mlx5_cqe64 *cqe, unsigned short cqe_bcnt, unsigned int head_offset,
				   unsigned int page_idx)
{
	struct mlx5e_frag_page *frag_page = &wi->alloc_units.frag_pages[page_idx];
	unsigned short headlen = min_t(unsigned short, MLX5E_RX_MAX_HEAD, cqe_bcnt);
	struct mlx5e_frag_page *head_page = frag_page;
	unsigned int frag_offset    = head_offset;
	unsigned int byte_cnt       = cqe_bcnt;
	struct skb_shared_info *sinfo;
	struct mlx5e_xdp_buff mxbuf;
	unsigned int truesize = 0;
	struct bpf_prog *prog;
	struct sk_buff *skb;
	unsigned int linear_frame_sz;
	unsigned short linear_data_len;
	unsigned short linear_hr;
	void *va;
	prog = rcu_dereference(rq->xdp_prog);
	if (prog) {
		net_prefetchw(page_address(frag_page->page) + frag_offset);
		if (unlikely(mlx5e_page_alloc_fragmented(rq, &wi->linear_page))) {
			rq->stats->buff_alloc_err++;
			return NULL;
		}
		va = page_address(wi->linear_page.page);
		net_prefetchw(va); 
		linear_hr = XDP_PACKET_HEADROOM;
		linear_data_len = 0;
		linear_frame_sz = MLX5_SKB_FRAG_SZ(linear_hr + MLX5E_RX_MAX_HEAD);
	} else {
		skb = napi_alloc_skb(rq->cq.napi,
				     ALIGN(MLX5E_RX_MAX_HEAD, sizeof(int)));
		if (unlikely(!skb)) {
			rq->stats->buff_alloc_err++;
			return NULL;
		}
		skb_mark_for_recycle(skb);
		va = skb->head;
		net_prefetchw(va); 
		net_prefetchw(skb->data);
		frag_offset += headlen;
		byte_cnt -= headlen;
		linear_hr = skb_headroom(skb);
		linear_data_len = headlen;
		linear_frame_sz = MLX5_SKB_FRAG_SZ(skb_end_offset(skb));
		if (unlikely(frag_offset >= PAGE_SIZE)) {
			frag_page++;
			frag_offset -= PAGE_SIZE;
		}
	}
	mlx5e_fill_mxbuf(rq, cqe, va, linear_hr, linear_frame_sz, linear_data_len, &mxbuf);
	sinfo = xdp_get_shared_info_from_buff(&mxbuf.xdp);
	while (byte_cnt) {
		unsigned int pg_consumed_bytes = min_t(unsigned int, PAGE_SIZE - frag_offset, byte_cnt);
		if (test_bit(MLX5E_RQ_STATE_SHAMPO, &rq->state))
			truesize += pg_consumed_bytes;
		else
			truesize += ALIGN(pg_consumed_bytes, BIT(rq->mpwqe.log_stride_sz));
		mlx5e_add_skb_shared_info_frag(rq, sinfo, &mxbuf.xdp, frag_page, frag_offset,
					       pg_consumed_bytes);
		byte_cnt -= pg_consumed_bytes;
		frag_offset = 0;
		frag_page++;
	}
	if (prog) {
		if (mlx5e_xdp_handle(rq, prog, &mxbuf)) {
			if (__test_and_clear_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags)) {
				struct mlx5e_frag_page *pfp;
				for (pfp = head_page; pfp < frag_page; pfp++)
					pfp->frags++;
				wi->linear_page.frags++;
			}
			mlx5e_page_release_fragmented(rq, &wi->linear_page);
			return NULL; 
		}
		skb = mlx5e_build_linear_skb(rq, mxbuf.xdp.data_hard_start,
					     linear_frame_sz,
					     mxbuf.xdp.data - mxbuf.xdp.data_hard_start, 0,
					     mxbuf.xdp.data - mxbuf.xdp.data_meta);
		if (unlikely(!skb)) {
			mlx5e_page_release_fragmented(rq, &wi->linear_page);
			return NULL;
		}
		skb_mark_for_recycle(skb);
		wi->linear_page.frags++;
		mlx5e_page_release_fragmented(rq, &wi->linear_page);
		if (xdp_buff_has_frags(&mxbuf.xdp)) {
			struct mlx5e_frag_page *pagep;
			xdp_update_skb_shared_info(skb, frag_page - head_page,
						   sinfo->xdp_frags_size, truesize,
						   xdp_buff_is_frag_pfmemalloc(&mxbuf.xdp));
			pagep = head_page;
			do
				pagep->frags++;
			while (++pagep < frag_page);
		}
	} else {
		dma_addr_t addr;
		if (xdp_buff_has_frags(&mxbuf.xdp)) {
			struct mlx5e_frag_page *pagep;
			xdp_update_skb_shared_info(skb, sinfo->nr_frags,
						   sinfo->xdp_frags_size, truesize,
						   xdp_buff_is_frag_pfmemalloc(&mxbuf.xdp));
			pagep = frag_page - sinfo->nr_frags;
			do
				pagep->frags++;
			while (++pagep < frag_page);
		}
		addr = page_pool_get_dma_addr(head_page->page);
		mlx5e_copy_skb_header(rq, skb, head_page->page, addr,
				      head_offset, head_offset, headlen);
		skb->tail += headlen;
		skb->len  += headlen;
	}
	return skb;
}

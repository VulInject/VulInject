int bnxt_xdp_xmit(struct net_device *dev, int num_frames,
		  struct xdp_frame **frames, unsigned int flags)
{
	struct bnxt *bp = netdev_priv(dev);
	struct bpf_prog *xdp_prog = READ_ONCE(bp->xdp_prog);
	struct pci_dev *pdev = bp->pdev;
	struct bnxt_tx_ring_info *txr;
	dma_addr_t mapping;
	int nxmit = 0;
	int ring;
	int i;
	if (!test_bit(BNXT_STATE_OPEN, &bp->state) ||
	    !bp->tx_nr_rings_xdp ||
	    !xdp_prog)
		return -EINVAL;
	ring = smp_processor_id() % bp->tx_nr_rings_xdp;
	txr = &bp->tx_ring[ring];
	if (READ_ONCE(txr->dev_state) == BNXT_DEV_STATE_CLOSING)
		return -EINVAL;
	if (static_branch_unlikely(&bnxt_xdp_locking_key))
	for (i = 0; i < num_frames; i++) {
		struct xdp_frame *xdp = frames[i];
		if (!bnxt_tx_avail(bp, txr))
			break;
		mapping = dma_map_single(&pdev->dev, xdp->data, xdp->len,
					 DMA_TO_DEVICE);
		if (dma_mapping_error(&pdev->dev, mapping))
			break;
		__bnxt_xmit_xdp_redirect(bp, txr, mapping, xdp->len, xdp);
		nxmit++;
	}
	if (flags & XDP_XMIT_FLUSH) {
		wmb();
		bnxt_db_write(bp, &txr->tx_db, txr->tx_prod);
	}
	if (static_branch_unlikely(&bnxt_xdp_locking_key))
		spin_unlock(&txr->xdp_tx_lock);
	return nxmit;
}

igb_receive_internal(IGBCore *core, const struct iovec *iov, int iovcnt,
                     bool has_vnet, bool *external_tx)
{
    uint16_t queues = 0;
    uint32_t causes = 0;
    uint32_t ecauses = 0;
    union {
        L2Header l2_header;
        uint8_t octets[ETH_ZLEN];
    } buf;
    struct iovec min_iov;
    size_t size, orig_size;
    size_t iov_ofs = 0;
    E1000E_RxRing rxr;
    E1000E_RSSInfo rss_info;
    uint16_t etqf;
    bool ts;
    size_t total_size;
    int strip_vlan_index;
    int i;
    if (external_tx) {
        *external_tx = true;
    }
    if (!e1000x_hw_rx_enabled(core->mac)) {
        return -1;
    }
    /* Pull virtio header in */
    if (has_vnet) {
        net_rx_pkt_set_vhdr_iovec(core->rx_pkt, iov, iovcnt);
        iov_ofs = sizeof(struct virtio_net_hdr);
    } else {
        net_rx_pkt_unset_vhdr(core->rx_pkt);
    }
    orig_size = iov_size(iov, iovcnt);
    size = orig_size - iov_ofs;
    /* Pad to minimum Ethernet frame length */
    if (size < sizeof(buf)) {
        iov_to_buf(iov, iovcnt, iov_ofs, &buf, size);
        memset(&buf.octets[size], 0, sizeof(buf) - size);
        e1000x_inc_reg_if_not_full(core->mac, RUC);
        min_iov.iov_base = &buf;
        min_iov.iov_len = size = sizeof(buf);
        iovcnt = 1;
        iov = &min_iov;
        iov_ofs = 0;
    } else {
        iov_to_buf(iov, iovcnt, iov_ofs, &buf, sizeof(buf.l2_header));
    }
    net_rx_pkt_set_packet_type(core->rx_pkt,
                               get_eth_packet_type(&buf.l2_header.eth));
    net_rx_pkt_set_protocols(core->rx_pkt, iov, iovcnt, iov_ofs);
    queues = igb_receive_assign(core, iov, iovcnt, iov_ofs,
                                &buf.l2_header, size,
                                &rss_info, &etqf, &ts, external_tx);
    if (!queues) {
        trace_e1000e_rx_flt_dropped();
        return orig_size;
    }
    for (i = 0; i < IGB_NUM_QUEUES; i++) {
        if (!(queues & BIT(i)) ||
            !(core->mac[RXDCTL0 + (i * 16)] & E1000_RXDCTL_QUEUE_ENABLE)) {
            continue;
        }
        igb_rx_ring_init(core, &rxr, i);
        if (!igb_rx_strip_vlan(core, rxr.i)) {
            strip_vlan_index = -1;
        } else if (core->mac[CTRL_EXT] & BIT(26)) {
            strip_vlan_index = 1;
        } else {
            strip_vlan_index = 0;
        }
        net_rx_pkt_attach_iovec_ex(core->rx_pkt, iov, iovcnt, iov_ofs,
                                   strip_vlan_index,
                                   core->mac[VET] & 0xffff,
                                   core->mac[VET] >> 16);
        total_size = net_rx_pkt_get_total_len(core->rx_pkt) +
            e1000x_fcs_len(core->mac);
        if (!igb_has_rxbufs(core, rxr.i, total_size)) {
            causes |= E1000_ICS_RXO;
            trace_e1000e_rx_not_written_to_guest(rxr.i->idx);
            continue;
        }
        causes |= E1000_ICR_RXDW;
        igb_rx_fix_l4_csum(core, core->rx_pkt);
        igb_write_packet_to_guest(core, core->rx_pkt, &rxr, &rss_info, etqf, ts);
        /* Check if receive descriptor minimum threshold hit */
        if (igb_rx_descr_threshold_hit(core, rxr.i)) {
            causes |= E1000_ICS_RXDMT0;
        }
        ecauses |= igb_rx_wb_eic(core, rxr.i->idx);
        trace_e1000e_rx_written_to_guest(rxr.i->idx);
    }
    trace_e1000e_rx_interrupt_set(causes);
    igb_raise_interrupts(core, EICR, ecauses);
    igb_raise_interrupts(core, ICR, causes);
    return orig_size;
}

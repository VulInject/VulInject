static unsigned int write_pkt_desc(struct sk_buff *skb, struct funeth_txq *q,
				   unsigned int tls_len)
{
	unsigned int extra_bytes = 0, extra_pkts = 0;
	unsigned int idx = q->prod_cnt & q->mask;
	const struct skb_shared_info *shinfo;
	unsigned int lens[MAX_SKB_FRAGS + 1];
	dma_addr_t addrs[MAX_SKB_FRAGS + 1];
	struct fun_eth_tx_req *req;
	struct fun_dataop_gl *gle;
	const struct tcphdr *th;
	unsigned int l4_hlen;
	unsigned int ngle;
	unsigned short flags;
	shinfo = skb_shinfo(skb);
	if (unlikely(fun_map_pkt(q->dma_dev, shinfo, skb->data,
				 skb_headlen(skb), addrs, lens))) {
		return 0;
	}
	req = fun_tx_desc_addr(q, idx);
	req->op = FUN_ETH_OP_TX;
	req->len8 = 0;
	req->flags = 0;
	req->suboff8 = offsetof(struct fun_eth_tx_req, dataop);
	req->repr_idn = 0;
	req->encap_proto = 0;
	if (likely(shinfo->gso_size)) {
		if (skb->encapsulation) {
			unsigned short ol4_ofst;
			flags = FUN_ETH_OUTER_EN | FUN_ETH_INNER_LSO |
				FUN_ETH_UPDATE_INNER_L4_CKSUM |
				FUN_ETH_UPDATE_OUTER_L3_LEN;
			if (shinfo->gso_type & (SKB_GSO_UDP_TUNNEL |
						SKB_GSO_UDP_TUNNEL_CSUM)) {
				flags |= FUN_ETH_UPDATE_OUTER_L4_LEN |
					 FUN_ETH_OUTER_UDP;
				if (shinfo->gso_type & SKB_GSO_UDP_TUNNEL_CSUM)
					flags |= FUN_ETH_UPDATE_OUTER_L4_CKSUM;
				ol4_ofst = skb_transport_offset(skb);
			} else {
				ol4_ofst = skb_inner_network_offset(skb);
			}
			if (ip_hdr(skb)->version == 4)
				flags |= FUN_ETH_UPDATE_OUTER_L3_CKSUM;
			else
				flags |= FUN_ETH_OUTER_IPV6;
			if (skb->inner_network_header) {
				if (inner_ip_hdr(skb)->version == 4)
					flags |= FUN_ETH_UPDATE_INNER_L3_CKSUM |
						 FUN_ETH_UPDATE_INNER_L3_LEN;
				else
					flags |= FUN_ETH_INNER_IPV6 |
						 FUN_ETH_UPDATE_INNER_L3_LEN;
			}
			th = inner_tcp_hdr(skb);
			l4_hlen = __tcp_hdrlen(th);
			fun_eth_offload_init(&req->offload, flags,
					     shinfo->gso_size,
					     tcp_hdr_doff_flags(th), 0,
					     skb_inner_network_offset(skb),
					     skb_inner_transport_offset(skb),
					     skb_network_offset(skb), ol4_ofst);
			FUN_QSTAT_INC(q, tx_encap_tso);
		} else if (shinfo->gso_type & SKB_GSO_UDP_L4) {
			flags = FUN_ETH_INNER_LSO | FUN_ETH_INNER_UDP |
				FUN_ETH_UPDATE_INNER_L4_CKSUM |
				FUN_ETH_UPDATE_INNER_L4_LEN |
				FUN_ETH_UPDATE_INNER_L3_LEN;
			if (ip_hdr(skb)->version == 4)
				flags |= FUN_ETH_UPDATE_INNER_L3_CKSUM;
			else
				flags |= FUN_ETH_INNER_IPV6;
			l4_hlen = sizeof(struct udphdr);
			fun_eth_offload_init(&req->offload, flags,
					     shinfo->gso_size,
					     cpu_to_be16(l4_hlen << 10), 0,
					     skb_network_offset(skb),
					     skb_transport_offset(skb), 0, 0);
			FUN_QSTAT_INC(q, tx_uso);
		} else {
			flags = FUN_ETH_INNER_LSO |
				FUN_ETH_UPDATE_INNER_L4_CKSUM |
				FUN_ETH_UPDATE_INNER_L3_LEN;
			if (shinfo->gso_type & SKB_GSO_TCPV6)
				flags |= FUN_ETH_INNER_IPV6;
			else
				flags |= FUN_ETH_UPDATE_INNER_L3_CKSUM;
			th = tcp_hdr(skb);
			l4_hlen = __tcp_hdrlen(th);
			fun_eth_offload_init(&req->offload, flags,
					     shinfo->gso_size,
					     tcp_hdr_doff_flags(th), 0,
					     skb_network_offset(skb),
					     skb_transport_offset(skb), 0, 0);
			FUN_QSTAT_INC(q, tx_tso);
		}
		u64_stats_update_begin(&q->syncp);
		q->stats.tx_cso += shinfo->gso_segs;
		u64_stats_update_end(&q->syncp);
		extra_pkts = shinfo->gso_segs - 1;
		extra_bytes = (be16_to_cpu(req->offload.inner_l4_off) +
			       l4_hlen) * extra_pkts;
	} else if (likely(skb->ip_summed == CHECKSUM_PARTIAL)) {
		flags = FUN_ETH_UPDATE_INNER_L4_CKSUM;
		if (skb->csum_offset == offsetof(struct udphdr, check))
			flags |= FUN_ETH_INNER_UDP;
		fun_eth_offload_init(&req->offload, flags, 0, 0, 0, 0,
				     skb_checksum_start_offset(skb), 0, 0);
		FUN_QSTAT_INC(q, tx_cso);
	} else {
		fun_eth_offload_init(&req->offload, 0, 0, 0, 0, 0, 0, 0, 0);
	}
	ngle = shinfo->nr_frags + 1;
	req->dataop = FUN_DATAOP_HDR_INIT(ngle, 0, ngle, 0, skb->len);
	gle = fun_write_gl(q, req, addrs, lens, ngle);
	if (IS_ENABLED(CONFIG_TLS_DEVICE) && unlikely(tls_len)) {
		struct fun_eth_tls *tls = (struct fun_eth_tls *)gle;
		struct fun_ktls_tx_ctx *tls_ctx;
		req->len8 += FUNETH_TLS_SZ / 8;
		req->flags = cpu_to_be16(FUN_ETH_TX_TLS);
		tls_ctx = tls_driver_ctx(skb->sk, TLS_OFFLOAD_CTX_DIR_TX);
		tls->tlsid = tls_ctx->tlsid;
		tls_ctx->next_seq += tls_len;
		u64_stats_update_begin(&q->syncp);
		q->stats.tx_tls_bytes += tls_len;
		q->stats.tx_tls_pkts += 1 + extra_pkts;
		u64_stats_update_end(&q->syncp);
	}
	u64_stats_update_begin(&q->syncp);
	q->stats.tx_bytes += skb->len + extra_bytes;
	q->stats.tx_pkts += 1 + extra_pkts;
	u64_stats_update_end(&q->syncp);
	q->info[idx].skb = skb;
	trace_funeth_tx(q, skb->len, idx, req->dataop.ngather);
	return tx_req_ndesc(req);
}

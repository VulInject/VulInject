static void gfx_v11_0_ring_emit_gfx_shadow(struct amdgpu_ring *ring,
					   unsigned int int shadow_va, unsigned int int csa_va,
					   unsigned int int gds_va, bool init_shadow,
					   int vmid)
{
	struct amdgpu_device *adev = ring->adev;
	int offs, end;
	if (!adev->gfx.cp_gfx_shadow || !ring->ring_obj)
		return;
	amdgpu_ring_write(ring, PACKET3(PACKET3_NOP, 1));
	offs = ring->wptr & ring->buf_mask;
	amdgpu_ring_write(ring, shadow_va ? 1 : 0);
	amdgpu_ring_write(ring, 0);
	if (ring->set_q_mode_offs) {
		unsigned int int addr;
		addr = amdgpu_bo_gpu_offset(ring->ring_obj);
		addr += ring->set_q_mode_offs << 2;
		end = gfx_v11_0_ring_emit_init_cond_exec(ring, addr);
	}
	if (!shadow_va) {
		unsigned int int addr;
		addr = amdgpu_bo_gpu_offset(ring->ring_obj);
		addr += offs << 2;
		amdgpu_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));
		amdgpu_ring_write(ring, WRITE_DATA_DST_SEL(5) | WR_CONFIRM);
		amdgpu_ring_write(ring, lower_32_bits(addr));
		amdgpu_ring_write(ring, upper_32_bits(addr));
		amdgpu_ring_write(ring, 0x1);
	}
	amdgpu_ring_write(ring, PACKET3(PACKET3_SET_Q_PREEMPTION_MODE, 7));
	amdgpu_ring_write(ring, lower_32_bits(shadow_va));
	amdgpu_ring_write(ring, upper_32_bits(shadow_va));
	amdgpu_ring_write(ring, lower_32_bits(gds_va));
	amdgpu_ring_write(ring, upper_32_bits(gds_va));
	amdgpu_ring_write(ring, lower_32_bits(csa_va));
	amdgpu_ring_write(ring, upper_32_bits(csa_va));
	amdgpu_ring_write(ring, shadow_va ?
			  PACKET3_SET_Q_PREEMPTION_MODE_IB_VMID(vmid) : 0);
	amdgpu_ring_write(ring, init_shadow ?
			  PACKET3_SET_Q_PREEMPTION_MODE_INIT_SHADOW_MEM : 0);
	if (ring->set_q_mode_offs)
		amdgpu_ring_patch_cond_exec(ring, end);
	if (shadow_va) {
		unsigned int int token = shadow_va ^ csa_va ^ gds_va ^ vmid;
		if (ring->set_q_mode_ptr && ring->set_q_mode_token == token)
			*ring->set_q_mode_ptr = 0;
		ring->set_q_mode_token = token;
	} else {
		ring->set_q_mode_ptr = &ring->ring[ring->set_q_mode_offs];
	}
	ring->set_q_mode_offs = offs;
}

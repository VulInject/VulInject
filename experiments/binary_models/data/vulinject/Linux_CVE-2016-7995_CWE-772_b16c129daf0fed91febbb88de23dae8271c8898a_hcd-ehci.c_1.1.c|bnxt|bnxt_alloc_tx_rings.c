static int bnxt_alloc_tx_rings(struct bnxt *bp)
{
	int i, j, rc;
	struct pci_dev *pdev = bp->pdev;
	bp->tx_push_size = 0;
	if (bp->tx_push_thresh) {
		int push_size;
		push_size  = L1_CACHE_ALIGN(sizeof(struct tx_push_bd) +
					bp->tx_push_thresh);
		if (push_size > 256) {
			push_size = 0;
			bp->tx_push_thresh = 0;
		}
		bp->tx_push_size = push_size;
	}
	for (i = 0, j = 0; i < bp->tx_nr_rings; i++) {
		struct bnxt_tx_ring_info *txr = &bp->tx_ring[i];
		struct bnxt_ring_struct *ring;
		unsigned char qidx;
		ring = &txr->tx_ring_struct;
		rc = bnxt_alloc_ring(bp, &ring->ring_mem);
		if (rc)
			return rc;
		ring->grp_idx = txr->bnapi->index;
		if (bp->tx_push_size) {
			dma_addr_t mapping;
			txr->tx_push = dma_alloc_coherent(&pdev->dev,
						bp->tx_push_size,
						&txr->tx_push_mapping,
						GFP_KERNEL);
			if (!txr->tx_push)
				return -ENOMEM;
			mapping = txr->tx_push_mapping +
				sizeof(struct tx_push_bd);
			txr->data_mapping = cpu_to_le64(mapping);
		}
		qidx = bp->tc_to_qidx[j];
		ring->queue_id = bp->q_info[qidx].queue_id;
		if (i < bp->tx_nr_rings_xdp)
			continue;
		if (BNXT_RING_TO_TC_OFF(bp, i) == (bp->tx_nr_rings_per_tc - 1))
			j++;
	}
	return 0;
}

static void iwl_pcie_rx_allocator(struct iwl_trans *trans)
{
	struct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);
	struct iwl_rb_allocator *rba = &trans_pcie->rba;
	struct list_head local_empty;
	int pending = atomic_read(&rba->req_pending);
	IWL_DEBUG_TPT(trans, "Pending allocation requests = %d\n", pending);
	spin_lock_bh(&rba->lock);
	list_replace_init(&rba->rbd_empty, &local_empty);
	spin_unlock_bh(&rba->lock);
	while (pending) {
		int i;
		LIST_HEAD(local_allocated);
		gfp_t gfp_mask = GFP_KERNEL;
		if (pending < RX_PENDING_WATERMARK)
			gfp_mask |= __GFP_NOWARN;
		for (i = 0; i < RX_CLAIM_REQ_ALLOC;) {
			struct iwl_rx_mem_buffer *rxb;
			struct page *page;
			BUG_ON(list_empty(&local_empty));
			rxb = list_first_entry(&local_empty,
					       struct iwl_rx_mem_buffer, list);
			page = iwl_pcie_rx_alloc_page(trans, &rxb->offset,
						      gfp_mask);
			if (!page)
				continue;
			rxb->page = page;
			rxb->page_dma = dma_map_page(trans->dev, page,
						     rxb->offset,
						     trans_pcie->rx_buf_bytes,
						     DMA_FROM_DEVICE);
			if (dma_mapping_error(trans->dev, rxb->page_dma)) {
				rxb->page = NULL;
				__free_pages(page, trans_pcie->rx_page_order);
				continue;
			}
			list_move(&rxb->list, &local_allocated);
			i++;
		}
		atomic_dec(&rba->req_pending);
		pending--;
		if (!pending) {
			pending = atomic_read(&rba->req_pending);
			if (pending)
				IWL_DEBUG_TPT(trans,
					      "Got more pending allocation requests = %d\n",
					      pending);
		}
		spin_lock_bh(&rba->lock);
		list_splice_tail(&local_allocated, &rba->rbd_allocated);
		list_splice_tail_init(&rba->rbd_empty, &local_empty);
		spin_unlock_bh(&rba->lock);
		atomic_inc(&rba->req_ready);
	}
	spin_lock_bh(&rba->lock);
	list_splice_tail(&local_empty, &rba->rbd_empty);
	spin_unlock_bh(&rba->lock);
	IWL_DEBUG_TPT(trans, "%s, exit.\n", __func__);
}

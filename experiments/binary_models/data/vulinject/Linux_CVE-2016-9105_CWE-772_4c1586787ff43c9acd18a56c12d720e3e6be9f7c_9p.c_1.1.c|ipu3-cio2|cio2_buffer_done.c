static void cio2_buffer_done(struct cio2_device *cio2, unsigned int dma_chan)
{
	struct device *dev = &cio2->pci_dev->dev;
	struct cio2_queue *q = cio2->cur_queue;
	struct cio2_fbpt_entry *entry;
	unsigned int int ns = ktime_get_ns();
	if (dma_chan >= CIO2_QUEUES) {
		dev_err(dev, "bad DMA channel %i\n", dma_chan);
		return;
	}
	entry = &q->fbpt[q->bufs_first * CIO2_MAX_LOPS];
	if (entry->first_entry.ctrl & CIO2_FBPT_CTRL_VALID) {
		dev_warn(dev, "no ready buffers found on DMA channel %u\n",
			 dma_chan);
		return;
	}
	do {
		struct cio2_buffer *b;
		b = q->bufs[q->bufs_first];
		if (b) {
			unsigned int received = entry[1].second_entry.num_of_bytes;
			unsigned int payload =
				vb2_get_plane_payload(&b->vbb.vb2_buf, 0);
			q->bufs[q->bufs_first] = NULL;
			atomic_dec(&q->bufs_queued);
			dev_dbg(dev, "buffer %i done\n", b->vbb.vb2_buf.index);
			b->vbb.vb2_buf.timestamp = ns;
			b->vbb.field = V4L2_FIELD_NONE;
			b->vbb.sequence = atomic_read(&q->frame_sequence);
			if (payload != received)
				dev_warn(dev,
					 "payload length is %lu, received %u\n",
					 payload, received);
			vb2_buffer_done(&b->vbb.vb2_buf, VB2_BUF_STATE_DONE);
		}
		cio2_fbpt_entry_init_dummy(cio2, entry);
		q->bufs_first = (q->bufs_first + 1) % CIO2_MAX_BUFFERS;
		entry = &q->fbpt[q->bufs_first * CIO2_MAX_LOPS];
	} while (!(entry->first_entry.ctrl & CIO2_FBPT_CTRL_VALID));
}

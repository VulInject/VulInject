struct ib_umem_dmabuf * ib_umem_dmabuf_get ( struct ib_device * device , unsigned long offset , size_t size , int fd , int access , const struct dma_buf_attach_ops * ops ) { struct dma_buf * dmabuf ; struct ib_umem_dmabuf * umem_dmabuf ; struct ib_umem * umem ; unsigned long end ; struct ib_umem_dmabuf * ret = ERR_PTR ( - EINVAL ) ; if ( check_add_overflow ( offset , ( unsigned long ) size , & end ) ) { return ret ; } if ( unlikely ( ! ops || ! ops -> move_notify ) ) { return ret ; } dmabuf = dma_buf_get ( fd ) ; if ( IS_ERR ( dmabuf ) ) { return ERR_CAST ( dmabuf ) ; } if ( dmabuf -> size < end ) { out_release_dmabuf } umem_dmabuf = kzalloc ( sizeof ( * umem_dmabuf ) , GFP_KERNEL ) ; if ( ! umem_dmabuf ) { ret = ERR_PTR ( - ENOMEM ) ; out_release_dmabuf } umem = & umem_dmabuf -> umem ; umem -> ibdev = device ; umem -> length = size ; umem -> address = offset ; umem -> writable = ib_access_writable ( access ) ; if ( ! ib_umem_num_pages ( umem ) ) { out_free_umem } umem_dmabuf -> attach = dma_buf_dynamic_attach ( dmabuf , device -> dma_device , ops , umem_dmabuf ) ; if ( IS_ERR ( umem_dmabuf -> attach ) ) { ret = ERR_CAST ( umem_dmabuf -> attach ) ; out_free_umem } return umem_dmabuf ; out_free_umem kfree ( umem_dmabuf ) ; out_release_dmabuf dma_buf_put ( dmabuf ) ; return ret ; } 
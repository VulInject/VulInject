static int dpaa_fq_init ( struct dpaa_fq * dpaa_fq , bool td_enable ) { const struct dpaa_priv * priv ; struct qman_fq * confq = NULL ; struct qm_mcc_initfq initfq ; struct device * dev ; struct qman_fq * fq ; int queue_id ; int err ; priv = netdev_priv ( dpaa_fq -> net_dev ) ; dev = dpaa_fq -> net_dev -> dev . parent ; if ( dpaa_fq -> fqid == 0 ) { dpaa_fq -> flags |= QMAN_FQ_FLAG_DYNAMIC_FQID ; } dpaa_fq -> init = ! ( dpaa_fq -> flags & QMAN_FQ_FLAG_NO_MODIFY ) ; err = qman_create_fq ( dpaa_fq -> fqid , dpaa_fq -> flags , & dpaa_fq -> fq_base ) ; if ( err ) { dev_err ( dev , "qman_create_fq() failed\n" ) ; return err ; } fq = & dpaa_fq -> fq_base ; if ( dpaa_fq -> init ) { initfq . we_mask = cpu_to_be16 ( QM_INITFQ_WE_FQCTRL ) ; initfq . fqd . fq_ctrl = cpu_to_be16 ( QM_FQCTRL_PREFERINCACHE ) ; if ( dpaa_fq -> fq_type == FQ_TYPE_TX_CONFIRM ) { initfq . fqd . fq_ctrl |= cpu_to_be16 ( QM_FQCTRL_AVOIDBLOCK ) ; } initfq . we_mask |= cpu_to_be16 ( QM_INITFQ_WE_DESTWQ ) ; qm_fqd_set_destwq ( & initfq . fqd , dpaa_fq -> channel , dpaa_fq -> wq ) ; if ( dpaa_fq -> fq_type == FQ_TYPE_TX || dpaa_fq -> fq_type == FQ_TYPE_TX_CONFIRM || dpaa_fq -> fq_type == FQ_TYPE_TX_CONF_MQ ) { initfq . we_mask |= cpu_to_be16 ( QM_INITFQ_WE_CGID ) ; initfq . fqd . fq_ctrl |= cpu_to_be16 ( QM_FQCTRL_CGE ) ; initfq . fqd . cgid = ( u8 ) priv -> cgr_data . cgr . cgrid ; initfq . we_mask |= cpu_to_be16 ( QM_INITFQ_WE_OAC ) ; qm_fqd_set_oac ( & initfq . fqd , QM_OAC_CG ) ; qm_fqd_set_oal ( & initfq . fqd , min ( sizeof ( sk_buff ) + priv -> tx_headroom , ( size_t ) FSL_QMAN_MAX_OAL ) ) ; } if ( td_enable ) { initfq . we_mask |= cpu_to_be16 ( QM_INITFQ_WE_TDTHRESH ) ; qm_fqd_set_taildrop ( & initfq . fqd , DPAA_FQ_TD , 1 ) ; initfq . fqd . fq_ctrl = cpu_to_be16 ( QM_FQCTRL_TDE ) ; } if ( dpaa_fq -> fq_type == FQ_TYPE_TX ) { queue_id = dpaa_tx_fq_to_id ( priv , & dpaa_fq -> fq_base ) ; if ( queue_id >= 0 ) { confq = priv -> conf_fqs [ queue_id ] ; } if ( confq ) { initfq . we_mask |= cpu_to_be16 ( QM_INITFQ_WE_CONTEXTA ) ; qm_fqd_context_a_set64 ( & initfq . fqd , 0x1e00000080000000ULL ) ; } } if ( priv -> use_ingress_cgr && ( dpaa_fq -> fq_type == FQ_TYPE_RX_DEFAULT || dpaa_fq -> fq_type == FQ_TYPE_RX_ERROR || dpaa_fq -> fq_type == FQ_TYPE_RX_PCD ) ) { initfq . we_mask |= cpu_to_be16 ( QM_INITFQ_WE_CGID ) ; initfq . fqd . fq_ctrl |= cpu_to_be16 ( QM_FQCTRL_CGE ) ; initfq . fqd . cgid = ( u8 ) priv -> ingress_cgr . cgrid ; initfq . we_mask |= cpu_to_be16 ( QM_INITFQ_WE_OAC ) ; qm_fqd_set_oac ( & initfq . fqd , QM_OAC_CG ) ; qm_fqd_set_oal ( & initfq . fqd , min ( sizeof ( sk_buff ) + priv -> tx_headroom , ( size_t ) FSL_QMAN_MAX_OAL ) ) ; } if ( dpaa_fq -> flags & QMAN_FQ_FLAG_NO_ENQUEUE ) { initfq . we_mask |= cpu_to_be16 ( QM_INITFQ_WE_CONTEXTA ) ; initfq . fqd . fq_ctrl |= cpu_to_be16 ( QM_FQCTRL_HOLDACTIVE | QM_FQCTRL_CTXASTASHING ) ; initfq . fqd . context_a . stashing . exclusive = QM_STASHING_EXCL_DATA | QM_STASHING_EXCL_CTX | QM_STASHING_EXCL_ANNOTATION ; qm_fqd_set_stashing ( & initfq . fqd , 1 , 2 , DIV_ROUND_UP ( sizeof ( qman_fq ) , 64 ) ) ; } err = qman_init_fq ( fq , QMAN_INITFQ_FLAG_SCHED , & initfq ) ; if ( err < 0 ) { dev_err ( dev , "qman_init_fq(%u) = %d\n" , qman_fq_fqid ( fq ) , err ) ; qman_destroy_fq ( fq ) ; return err ; } } dpaa_fq -> fqid = qman_fq_fqid ( fq ) ; if ( dpaa_fq -> fq_type == FQ_TYPE_RX_DEFAULT || dpaa_fq -> fq_type == FQ_TYPE_RX_PCD ) { err = xdp_rxq_info_reg ( & dpaa_fq -> xdp_rxq , dpaa_fq -> net_dev , dpaa_fq -> fqid , 0 ) ; if ( err ) { dev_err ( dev , "xdp_rxq_info_reg() = %d\n" , err ) ; return err ; } err = xdp_rxq_info_reg_mem_model ( & dpaa_fq -> xdp_rxq , MEM_TYPE_PAGE_ORDER0 , NULL ) ; if ( err ) { dev_err ( dev , "xdp_rxq_info_reg_mem_model() = %d\n" , err ) ; xdp_rxq_info_unreg ( & dpaa_fq -> xdp_rxq ) ; return err ; } } return 0 ; } 
int kmem_cache_alloc_bulk ( struct kmem_cache * cachep , gfp_t gfp , size_t size , void * * p ) { size_t i ; if ( kmalloc_verbose ) { pr_debug ( "Bulk alloc %lu\n" , size ) ; } if ( ! ( gfp & __GFP_DIRECT_RECLAIM ) ) { if ( cachep -> non_kernel < size ) { return 0 ; } cachep -> non_kernel -= size ; } pthread_mutex_lock ( & cachep -> lock ) ; if ( cachep -> nr_objs >= size ) { struct radix_tree_node * node ; for ( i = 0 ; i < size ; i ++ ) { node = cachep -> objs ; cachep -> nr_objs -- ; cachep -> objs = node -> parent ; p [ i ] = node ; node -> parent = NULL ; } pthread_mutex_unlock ( & cachep -> lock ) ; } else { pthread_mutex_unlock ( & cachep -> lock ) ; for ( i = 0 ; i < size ; i ++ ) { if ( cachep -> align ) { printf ( "Allocating %p from slab\n" , 0 ) ; } else { p [ i ] = malloc ( cachep -> size * size ) ; } if ( cachep -> ctor ) { cachep -> ctor ( p [ i ] ) ; } if ( gfp & __GFP_ZERO ) { memset ( p [ i ] , 0 , cachep -> size ) ; } } } for ( i = 0 ; i < size ; i ++ ) { uatomic_inc ( & nr_allocated ) ; uatomic_inc ( & cachep -> nr_allocated ) ; uatomic_inc ( & cachep -> nr_tallocated ) ; if ( kmalloc_verbose ) { printf ( "Allocating %p from slab\n" , p [ i ] ) ; } } return size ; } 
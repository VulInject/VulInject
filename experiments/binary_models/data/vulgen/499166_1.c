int qib_create_rcvhdrq ( struct qib_devdata * dd , struct qib_ctxtdata * rcd ) { unsigned amt ; int old_node_id ; if ( ! rcd -> rcvhdrq ) { dma_addr_t phys_hdrqtail ; gfp_t gfp_flags ; amt = ALIGN ( dd -> rcvhdrcnt * dd -> rcvhdrentsize * sizeof ( u32 ) , PAGE_SIZE ) ; gfp_flags = ( rcd -> ctxt >= dd -> first_user_ctxt ) ?GFP_USER : GFP_KERNEL ; old_node_id = dev_to_node ( & dd -> pcidev -> dev ) ; set_dev_node ( & dd -> pcidev -> dev , rcd -> node_id ) ; rcd -> rcvhdrq = dma_alloc_coherent ( & dd -> pcidev -> dev , amt , & rcd -> rcvhdrq_phys , gfp_flags | __GFP_COMP ) ; set_dev_node ( & dd -> pcidev -> dev , old_node_id ) ; if ( ! rcd -> rcvhdrq ) { qib_dev_err ( dd , "attempt to allocate %d bytes for ctxt %u rcvhdrq failed\n" , amt , rcd -> ctxt ) ; bail } if ( rcd -> ctxt >= dd -> first_user_ctxt ) { if ( ! rcd -> user_event_mask ) { bail_free_hdrq } } if ( ! ( dd -> flags & QIB_NODMA_RTAIL ) ) { set_dev_node ( & dd -> pcidev -> dev , rcd -> node_id ) ; rcd -> rcvhdrtail_kvaddr = dma_alloc_coherent ( & dd -> pcidev -> dev , PAGE_SIZE , & phys_hdrqtail , gfp_flags ) ; set_dev_node ( & dd -> pcidev -> dev , old_node_id ) ; if ( ! rcd -> rcvhdrtail_kvaddr ) { bail_free } rcd -> rcvhdrqtailaddr_phys = phys_hdrqtail ; } rcd -> rcvhdrq_size = amt ; } memset ( rcd -> rcvhdrq , 0 , rcd -> rcvhdrq_size ) ; if ( rcd -> rcvhdrtail_kvaddr ) { memset ( rcd -> rcvhdrtail_kvaddr , 0 , PAGE_SIZE ) ; } return 0 ; bail_free qib_dev_err ( dd , "attempt to allocate 1 page for ctxt %u rcvhdrqtailaddr failed\n" , rcd -> ctxt ) ; vfree ( rcd -> user_event_mask ) ; rcd -> user_event_mask = NULL ; bail_free_hdrq dma_free_coherent ( & dd -> pcidev -> dev , amt , rcd -> rcvhdrq , rcd -> rcvhdrq_phys ) ; rcd -> rcvhdrq = NULL ; bail return - ENOMEM ; } 
static int write_protect_page ( struct vm_area_struct * vma , struct page * page , pte_t * orig_pte ) { struct mm_struct * mm = vma -> vm_mm ; DEFINE_PAGE_VMA_WALK ( pvmw , page , vma , 0 , 0 ) ; int swapped ; int err = - EFAULT ; struct mmu_notifier_range range ; bool anon_exclusive ; pvmw . address = page_address_in_vma ( page , vma ) ; if ( pvmw . address == - EFAULT ) { out } BUG_ON ( PageTransCompound ( page ) ) ; mmu_notifier_range_init ( & range , MMU_NOTIFY_CLEAR , 0 , mm , pvmw . address , pvmw . address + PAGE_SIZE ) ; mmu_notifier_invalidate_range_start ( & range ) ; if ( ! page_vma_mapped_walk ( & pvmw ) ) { out_mn } if ( WARN_ONCE ( ! pvmw . pte , "Unexpected PMD mapping?" ) ) { out_unlock } anon_exclusive = PageAnonExclusive ( page ) ; if ( pte_write ( * pvmw . pte ) || pte_dirty ( * pvmw . pte ) || anon_exclusive || mm_tlb_flush_pending ( mm ) ) { pte_t entry ; swapped = PageSwapCache ( page ) ; flush_cache_page ( vma , pvmw . address , page_to_pfn ( page ) ) ; entry = ptep_clear_flush ( vma , pvmw . address , pvmw . pte ) ; if ( page_mapcount ( page ) + 1 + swapped != page_count ( page ) ) { set_pte_at ( mm , pvmw . address , pvmw . pte , entry ) ; out_unlock } if ( pte_dirty ( entry ) ) { set_page_dirty ( page ) ; } entry = pte_mkclean ( entry ) ; if ( pte_write ( entry ) ) { entry = pte_wrprotect ( entry ) ; } set_pte_at_notify ( mm , pvmw . address , pvmw . pte , entry ) ; } * orig_pte = * pvmw . pte ; err = 0 ; out_unlock page_vma_mapped_walk_done ( & pvmw ) ; out_mn mmu_notifier_invalidate_range_end ( & range ) ; out return err ; } 
static int _dev_comp_vect_cpu_mask_init ( hfi1_devdata * dd , hfi1_affinity_node * entry , bool first_dev_init ) __must_hold ( & node_affinity . lock ) ( int i , j , curr_cpu , int possible_cpus_comp_vect = 0 , cpumask * dev_comp_vect_mask = & dd -> comp_vect -> mask , lockdep_assert_held ( & node_affinity . lock ) , ( cpumask_weight ( & entry -> comp_vect_mask ) == 1 ) { possible_cpus_comp_vect = 1 dd_dev_warn ( dd , "Number of kernel receive queues is too large for completion vector affinity to be effective\n" ) } else { possible_cpus_comp_vect += cpumask_weight ( & entry -> comp_vect_mask ) / hfi1_per_node_cntr [ dd -> node ] } ; ) ; if ( first_dev_init && cpumask_weight ( & entry -> comp_vect_mask ) % hfi1_per_node_cntr [ dd -> node ] != 0 ) { possible_cpus_comp_vect ++ ; } -> comp_vect_possible_cpus = possible_cpus_comp_vect ; for ( i = 0 ; i < dd -> comp_vect_possible_cpus ; i ++ ) { curr_cpu = per_cpu_affinity_get ( & entry -> comp_vect_mask , entry -> comp_vect_affinity ) ; cpumask_set_cpu ( curr_cpu , dev_comp_vect_mask ) ; } hfi1_cdbg ( AFFINITY , "[%s] Completion vector affinity CPU set(s) %*pbl" , rvt_get_ibdev_name ( & ( dd ) -> verbs_dev . rdi ) , cpumask_pr_args ( dev_comp_vect_mask ) ) ; return 0 ; fail for ( j = 0 ; j < i ; j ++ ) { per_cpu_affinity_put_max ( & entry -> comp_vect_mask , entry -> comp_vect_affinity ) ; } return curr_cpu ; 
static int netdev_dummy_send ( struct netdev * netdev , int qid , struct dp_packet_batch * batch , bool concurrent_txq OVS_UNUSED ) { struct netdev_dummy * dev = netdev_dummy_cast ( netdev ) ; int error = 0 ; struct dp_packet * packet ; DP_PACKET_BATCH_FOR_EACH ( , , ) { const void * buffer = dp_packet_data ( packet ) ; size_t size = dp_packet_size ( packet ) ; if ( ! dp_packet_is_eth ( packet ) ) { break ; } if ( size < ETH_HEADER_LEN ) { error = EMSGSIZE ; break ; } else { const struct eth_header * eth = buffer ; int max_size ; ovs_mutex_lock ( & dev -> mutex ) ; max_size = dev -> mtu + ETH_HEADER_LEN ; ovs_mutex_unlock ( & dev -> mutex ) ; if ( eth -> eth_type == htons ( ETH_TYPE_VLAN ) ) { max_size += VLAN_HEADER_LEN ; } if ( size > max_size ) { error = EMSGSIZE ; break ; } } ovs_mutex_lock ( & dev -> mutex ) ; dev -> stats . tx_packets ++ ; dev -> txq_stats [ qid ] . packets ++ ; dev -> stats . tx_bytes += size ; dev -> txq_stats [ qid ] . bytes += size ; dummy_packet_conn_send ( & dev -> conn , buffer , size ) ; if ( dev -> address . s_addr ) { struct dp_packet dp ; struct flow flow ; dp_packet_use_const ( & dp , buffer , size ) ; flow_extract ( & dp , & flow ) ; if ( flow . dl_type == htons ( ETH_TYPE_ARP ) && flow . nw_proto == ARP_OP_REQUEST && flow . nw_dst == dev -> address . s_addr ) { struct dp_packet * reply = dp_packet_new ( 0 ) ; compose_arp ( reply , ARP_OP_REPLY , dev -> hwaddr , flow . dl_src , false , flow . nw_dst , flow . nw_src ) ; netdev_dummy_queue_packet ( dev , reply , NULL , 0 ) ; } } if ( dev -> tx_pcap ) { struct dp_packet dp ; dp_packet_use_const ( & dp , buffer , size ) ; ovs_pcap_write ( dev -> tx_pcap , & dp ) ; } ovs_mutex_unlock ( & dev -> mutex ) ; } dp_packet_delete_batch ( batch , true ) ; return error ; } 
static int qed_cxt_src_t2_alloc ( struct qed_hwfn * p_hwfn ) { struct qed_cxt_mngr * p_mngr = p_hwfn -> p_cxt_mngr ; u32 conn_num , total_size , ent_per_page , psz , i ; struct phys_mem_desc * p_t2_last_page ; struct qed_ilt_client_cfg * p_src ; struct qed_src_iids src_iids ; struct qed_src_t2 * p_t2 ; int rc ; p_src = & p_hwfn -> p_cxt_mngr -> clients [ ILT_CLI_SRC ] ; if ( ! p_src -> active ) { return 0 ; } qed_cxt_src_iids ( p_mngr , & src_iids ) ; conn_num = src_iids . pf_cids + src_iids . per_vf_cids * p_mngr -> vf_count ; total_size = conn_num * sizeof ( src_ent ) ; psz = ILT_PAGE_IN_BYTES ( p_src -> p_size . val ) ; p_t2 = & p_mngr -> src_t2 ; p_t2 -> num_pages = DIV_ROUND_UP ( total_size , psz ) ; p_t2 -> dma_mem = kcalloc ( p_t2 -> num_pages , sizeof ( phys_mem_desc ) , GFP_KERNEL ) ; if ( ! p_t2 -> dma_mem ) { DP_NOTICE ( p_hwfn , "Failed to allocate t2 table\n" ) ; rc = - ENOMEM ; t2_fail } rc = qed_cxt_t2_alloc_pages ( p_hwfn , p_t2 , total_size , psz ) ; if ( rc ) { t2_fail } ent_per_page = psz / sizeof ( src_ent ) ; p_t2 -> first_free = ( u64 ) p_t2 -> dma_mem [ 0 ] . phys_addr ; p_t2_last_page = & p_t2 -> dma_mem [ ( conn_num - 1 ) / ent_per_page ] ; p_t2 -> last_free = ( u64 ) p_t2_last_page -> phys_addr + ( ( conn_num - 1 ) & ( ent_per_page - 1 ) ) * sizeof ( src_ent ) ; for ( i = 0 ; i < p_t2 -> num_pages ; i ++ ) { u32 ent_num = min_t ( u32 , ent_per_page , conn_num ) ; struct src_ent * entries = p_t2 -> dma_mem [ i ] . virt_addr ; u64 p_ent_phys = ( u64 ) p_t2 -> dma_mem [ i ] . phys_addr , val ; u32 j ; for ( j = 0 ; j < ent_num - 1 ; j ++ ) { val = p_ent_phys + ( j + 1 ) * sizeof ( src_ent ) ; entries [ j ] . next = cpu_to_be64 ( val ) ; } if ( i < p_t2 -> num_pages - 1 ) { val = ( u64 ) p_t2 -> dma_mem [ i + 1 ] . phys_addr ; } else { val = 0 ; } entries [ j ] . next = cpu_to_be64 ( val ) ; conn_num -= ent_num ; } return 0 ; t2_fail qed_cxt_src_t2_free ( p_hwfn ) ; return rc ; } 
static dma_addr_t pci_map_single_1 ( struct pci_dev * pdev , void * cpu_addr , size_t size , int dac_allowed ) { struct pci_controller * hose = pdev ?pdev -> sysdata : pci_isa_hose ; dma_addr_t max_dma = pdev ?pdev -> dma_mask : ISA_DMA_MASK ; struct pci_iommu_arena * arena ; long npages , dma_ofs , i ; unsigned long paddr ; dma_addr_t ret ; unsigned int align = 0 ; struct device * dev = pdev ?& pdev -> dev : NULL ; paddr = __pa ( cpu_addr ) ; if ( paddr + size + __direct_map_base - 1 <= max_dma && paddr + size <= __direct_map_size ) { ret = paddr + __direct_map_base ; DBGA2 ( "pci_map_single: [%p,%zx] ->direct %llx from %ps\n" , cpu_addr , size , ret , __builtin_return_address ( 0 ) ) ; return ret ; } if ( dac_allowed ) { ret = paddr + alpha_mv . pci_dac_offset ; DBGA2 ( "pci_map_single: [%p,%zx] ->DAC %llx from %ps\n" , cpu_addr , size , ret , __builtin_return_address ( 0 ) ) ; return ret ; } if ( ! alpha_mv . mv_pci_tbi ) { printk_once ( KERN_WARNING "pci_map_single: no HW sg\n" ) ; return DMA_MAPPING_ERROR ; } arena = hose -> sg_pci ; if ( ! arena || arena -> dma_base + arena -> size - 1 > max_dma ) { arena = hose -> sg_isa ; } npages = iommu_num_pages ( paddr , size , PAGE_SIZE ) ; dma_ofs = iommu_arena_alloc ( dev , arena , npages , align ) ; if ( dma_ofs < 0 ) { printk ( KERN_WARNING "pci_map_single failed: " "could not allocate dma page tables\n" ) ; return DMA_MAPPING_ERROR ; } paddr &= PAGE_MASK ; for ( i = 0 ; i < npages ; ++ i , paddr += PAGE_SIZE ) { arena -> ptes [ i + dma_ofs ] = mk_iommu_pte ( paddr ) ; } ret = arena -> dma_base + dma_ofs * PAGE_SIZE ; ret += ( unsigned long ) cpu_addr & ~ PAGE_MASK ; DBGA2 ( "pci_map_single: [%p,%zx] np %ld ->sg %llx from %ps\n" , cpu_addr , size , npages , ret , __builtin_return_address ( 0 ) ) ; return ret ; } 
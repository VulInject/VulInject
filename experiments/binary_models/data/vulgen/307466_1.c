struct bpf_prog * bpf_int_jit_compile ( struct bpf_prog * fp ) { u32 proglen ; u32 alloclen ; u8 * image = NULL ; u32 * code_base ; u32 * addrs ; struct powerpc64_jit_data * jit_data ; struct codegen_context cgctx ; int pass ; int flen ; struct bpf_binary_header * bpf_hdr ; struct bpf_prog * org_fp = fp ; struct bpf_prog * tmp_fp ; bool bpf_blinded = false ; bool extra_pass = false ; u32 extable_len ; u32 fixup_len ; if ( ! fp -> jit_requested ) { return org_fp ; } tmp_fp = bpf_jit_blind_constants ( org_fp ) ; if ( IS_ERR ( tmp_fp ) ) { return org_fp ; } if ( tmp_fp != org_fp ) { bpf_blinded = true ; fp = tmp_fp ; } jit_data = fp -> aux -> jit_data ; if ( ! jit_data ) { jit_data = kzalloc ( sizeof ( * jit_data ) , GFP_KERNEL ) ; if ( ! jit_data ) { fp = org_fp ; out } fp -> aux -> jit_data = jit_data ; } flen = fp -> len ; addrs = jit_data -> addrs ; if ( addrs ) { cgctx = jit_data -> ctx ; image = jit_data -> image ; bpf_hdr = jit_data -> header ; proglen = jit_data -> proglen ; extra_pass = true ; skip_init_ctx } addrs = kcalloc ( flen + 1 , sizeof ( * addrs ) , GFP_KERNEL ) ; if ( addrs == NULL ) { fp = org_fp ; out_addrs } memset ( & cgctx , 0 , sizeof ( codegen_context ) ) ; bpf_jit_init_reg_mapping ( & cgctx ) ; cgctx . stack_size = round_up ( fp -> aux -> stack_depth , 16 ) ; if ( bpf_jit_build_body ( fp , 0 , & cgctx , addrs , 0 , false ) ) { fp = org_fp ; out_addrs } if ( cgctx . seen & SEEN_TAILCALL || ! is_offset_in_branch_range ( ( long ) cgctx . idx * 4 ) ) { cgctx . idx = 0 ; if ( bpf_jit_build_body ( fp , 0 , & cgctx , addrs , 0 , false ) ) { fp = org_fp ; out_addrs } } bpf_jit_realloc_regs ( & cgctx ) ; bpf_jit_build_prologue ( 0 , & cgctx ) ; addrs [ fp -> len ] = cgctx . idx * 4 ; bpf_jit_build_epilogue ( 0 , & cgctx ) ; fixup_len = fp -> aux -> num_exentries * BPF_FIXUP_LEN * 4 ; extable_len = fp -> aux -> num_exentries * sizeof ( exception_table_entry ) ; proglen = cgctx . idx * 4 ; alloclen = proglen + FUNCTION_DESCR_SIZE + fixup_len + extable_len ; bpf_hdr = bpf_jit_binary_alloc ( alloclen , & image , 4 , bpf_jit_fill_ill_insns ) ; if ( ! bpf_hdr ) { fp = org_fp ; out_addrs } if ( extable_len ) { fp -> aux -> extable = ( void * ) image + FUNCTION_DESCR_SIZE + proglen + fixup_len ; } skip_init_ctx code_base = ( u32 * ) ( image + FUNCTION_DESCR_SIZE ) ; for ( pass = 1 ; pass < 3 ; pass ++ ) { cgctx . idx = 0 ; cgctx . alt_exit_addr = 0 ; bpf_jit_build_prologue ( code_base , & cgctx ) ; if ( bpf_jit_build_body ( fp , code_base , & cgctx , addrs , pass , extra_pass ) ) { bpf_jit_binary_free ( bpf_hdr ) ; fp = org_fp ; out_addrs } bpf_jit_build_epilogue ( code_base , & cgctx ) ; if ( bpf_jit_enable > 1 ) { pr_info ( "Pass %d: shrink = %d, seen = 0x%x\n" , pass , proglen - ( cgctx . idx * 4 ) , cgctx . seen ) ; } } ( ( u64 * ) image ) [ 0 ] = ( u64 ) code_base ; ( ( u64 * ) image ) [ 1 ] = local_paca -> kernel_toc ; fp -> bpf_func = ( void * ) image ; fp -> jited = 1 ; fp -> jited_len = proglen + FUNCTION_DESCR_SIZE ; bpf_flush_icache ( bpf_hdr , ( u8 * ) bpf_hdr + bpf_hdr -> size ) ; if ( ! fp -> is_func || extra_pass ) { bpf_jit_binary_lock_ro ( bpf_hdr ) ; bpf_prog_fill_jited_linfo ( fp , addrs ) ; out_addrs kfree ( addrs ) ; kfree ( jit_data ) ; fp -> aux -> jit_data = NULL ; } else { jit_data -> addrs = addrs ; jit_data -> ctx = cgctx ; jit_data -> proglen = proglen ; jit_data -> image = image ; jit_data -> header = bpf_hdr ; } out if ( bpf_blinded ) { bpf_jit_prog_release_other ( fp , fp == org_fp ?tmp_fp : org_fp ) ; } return fp ; } 
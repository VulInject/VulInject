( flush_tlb_mm ) ; void __flush_tlb_page ( struct mm_struct * mm , unsigned long vmaddr , int tsize , int ind ) { struct cpumask * cpu_mask ; unsigned int pid ; if ( unlikely ( WARN_ON ( ! mm ) ) ) { return ; } preempt_disable ( ) ; pid = mm -> context . id ; if ( unlikely ( pid == MMU_NO_CONTEXT ) ) { bail } cpu_mask = mm_cpumask ( mm ) ; if ( ! mm_is_core_local ( mm ) ) { if ( mmu_has_feature ( MMU_FTR_USE_TLBIVAX_BCAST ) ) { int lock = mmu_has_feature ( MMU_FTR_LOCK_BCAST_INVAL ) ; if ( lock ) { raw_spin_lock ( & tlbivax_lock ) ; } _tlbivax_bcast ( vmaddr , pid , tsize , ind ) ; bail } else { struct tlb_flush_param p = { . pid = pid . addr = vmaddr . tsize = tsize . ind = ind } ; smp_call_function_many ( cpu_mask , do_flush_tlb_page_ipi , & p , 1 ) ; } } _tlbil_va ( vmaddr , pid , tsize , ind ) ; bail preempt_enable ( ) ; } 
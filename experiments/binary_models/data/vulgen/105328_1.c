static enum rx_handler_result handle_not_macsec ( struct sk_buff * skb ) { enum rx_handler_result ret = RX_HANDLER_PASS ; struct ethhdr * hdr = eth_hdr ( skb ) ; struct metadata_dst * md_dst ; struct macsec_rxh_data * rxd ; struct macsec_dev * macsec ; rcu_read_lock ( ) ; rxd = macsec_data_rcu ( skb -> dev ) ; md_dst = skb_metadata_dst ( skb ) ; list_for_each_entry_rcu ( , , ) { struct sk_buff * nskb ; struct pcpu_secy_stats * secy_stats = this_cpu_ptr ( macsec -> stats ) ; struct net_device * ndev = macsec -> secy . netdev ; if ( macsec_is_offloaded ( macsec ) && netif_running ( ndev ) ) { if ( md_dst && md_dst -> type == METADATA_MACSEC && ( ! find_rx_sc ( & macsec -> secy , md_dst -> u . macsec_info . sci ) ) ) { continue ; } if ( ether_addr_equal_64bits ( hdr -> h_dest , ndev -> dev_addr ) ) { skb -> dev = ndev ; skb -> pkt_type = PACKET_HOST ; ret = RX_HANDLER_ANOTHER ; out } if ( is_multicast_ether_addr_64bits ( hdr -> h_dest ) ) { nskb = skb_clone ( skb , GFP_ATOMIC ) ; nskb -> dev = ndev ; if ( ether_addr_equal_64bits ( hdr -> h_dest , ndev -> broadcast ) ) { nskb -> pkt_type = PACKET_BROADCAST ; } else { nskb -> pkt_type = PACKET_MULTICAST ; } __netif_rx ( nskb ) ; } continue ; } if ( macsec -> secy . validate_frames == MACSEC_VALIDATE_STRICT ) { u64_stats_update_begin ( & secy_stats -> syncp ) ; secy_stats -> stats . InPktsNoTag ++ ; u64_stats_update_end ( & secy_stats -> syncp ) ; macsec -> secy . netdev -> stats . rx_dropped ++ ; continue ; } nskb = skb_clone ( skb , GFP_ATOMIC ) ; if ( ! nskb ) { break ; } nskb -> dev = ndev ; if ( __netif_rx ( nskb ) == NET_RX_SUCCESS ) { u64_stats_update_begin ( & secy_stats -> syncp ) ; secy_stats -> stats . InPktsUntagged ++ ; u64_stats_update_end ( & secy_stats -> syncp ) ; } } out rcu_read_unlock ( ) ; return ret ; } 
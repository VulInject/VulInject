netdev_tx_t mlx5i_sq_xmit ( struct mlx5e_txqsq * sq , struct sk_buff * skb , struct mlx5_av * av , u32 dqpn , u32 dqkey ) { struct mlx5_wq_cyc * wq = & sq -> wq ; u16 pi = sq -> pc & wq -> sz_m1 ; struct mlx5i_tx_wqe * wqe = mlx5_wq_cyc_get_wqe ( wq , pi ) ; struct mlx5e_tx_wqe_info * wi = & sq -> db . wqe_info [ pi ] ; struct mlx5_wqe_ctrl_seg * cseg = & wqe -> ctrl ; struct mlx5_wqe_datagram_seg * datagram = & wqe -> datagram ; struct mlx5_wqe_eth_seg * eseg = & wqe -> eth ; unsigned char * skb_data = skb -> data ; unsigned int skb_len = skb -> len ; u8 opcode = MLX5_OPCODE_SEND ; unsigned int num_bytes ; int num_dma ; u16 headlen ; u16 ds_cnt ; u16 ihs ; memset ( wqe , 0 , sizeof ( * wqe ) ) ; mlx5i_txwqe_build_datagram ( av , dqpn , dqkey , datagram ) ; mlx5e_txwqe_build_eseg_csum ( sq , skb , eseg ) ; if ( skb_is_gso ( skb ) ) { opcode = MLX5_OPCODE_LSO ; ihs = mlx5e_txwqe_build_eseg_gso ( sq , skb , eseg , & num_bytes ) ; } else { ihs = mlx5e_calc_min_inline ( sq -> min_inline_mode , skb ) ; num_bytes = max_t ( , , ) ; } ds_cnt = sizeof ( * wqe ) / MLX5_SEND_WQE_DS ; headlen = skb_len - skb -> data_len ; num_dma = mlx5e_txwqe_build_dsegs ( sq , skb , skb_data , headlen , ( mlx5_wqe_data_seg * ) cseg + ds_cnt ) ; if ( unlikely ( num_dma < 0 ) ) { dma_unmap_wqe_err } mlx5e_txwqe_complete ( sq , skb , opcode , ds_cnt + num_dma , num_bytes , num_dma , wi , cseg ) ; return NETDEV_TX_OK ; dma_unmap_wqe_err sq -> stats . dropped ++ ; mlx5e_dma_unmap_wqe_err ( sq , wi -> num_dma ) ; dev_kfree_skb_any ( skb ) ; return NETDEV_TX_OK ; } 
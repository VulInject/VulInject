static noinline void update_existing_head_ref ( struct btrfs_delayed_ref_root * delayed_refs , struct btrfs_delayed_ref_node * existing , struct btrfs_delayed_ref_node * update ) { struct btrfs_delayed_ref_head * existing_ref ; struct btrfs_delayed_ref_head * ref ; int old_ref_mod ; existing_ref = btrfs_delayed_node_to_head ( existing ) ; ref = btrfs_delayed_node_to_head ( update ) ; BUG_ON ( existing_ref -> is_data != ref -> is_data ) ; spin_lock ( & existing_ref -> lock ) ; if ( ref -> must_insert_reserved ) { existing_ref -> must_insert_reserved = ref -> must_insert_reserved ; } if ( ref -> extent_op ) { if ( ! existing_ref -> extent_op ) { existing_ref -> extent_op = ref -> extent_op ; } else { if ( ref -> extent_op -> update_key ) { memcpy ( & existing_ref -> extent_op -> key , & ref -> extent_op -> key , sizeof ( ref -> extent_op -> key ) ) ; existing_ref -> extent_op -> update_key = true ; } if ( ref -> extent_op -> update_flags ) { existing_ref -> extent_op -> flags_to_set |= ref -> extent_op -> flags_to_set ; existing_ref -> extent_op -> update_flags = true ; } btrfs_free_delayed_extent_op ( ref -> extent_op ) ; } } old_ref_mod = existing_ref -> total_ref_mod ; existing -> ref_mod += update -> ref_mod ; existing_ref -> total_ref_mod += update -> ref_mod ; if ( existing_ref -> is_data ) { if ( existing_ref -> total_ref_mod >= 0 && old_ref_mod < 0 ) { delayed_refs -> pending_csums -= existing -> num_bytes ; } if ( existing_ref -> total_ref_mod < 0 && old_ref_mod >= 0 ) { delayed_refs -> pending_csums += existing -> num_bytes ; } } spin_unlock ( & existing_ref -> lock ) ; } 
void * kvm_mips_build_vcpu_run ( void * addr ) { u32 * p = addr ; unsigned int i ; UASM_i_ADDIU ( & p , K1 , SP , - ( int ) sizeof ( pt_regs ) ) ; for ( i = 16 ; i < 32 ; ++ i ) { UASM_i_SW ( & p , i , offsetof ( pt_regs , regs [ i ] ) , K1 ) ; } uasm_i_mfc0 ( & p , V0 , C0_STATUS ) ; UASM_i_SW ( & p , V0 , offsetof ( pt_regs , cp0_status ) , K1 ) ; kvm_mips_build_save_scratch ( & p , V1 , K1 ) ; UASM_i_MTC0 ( & p , A0 , scratch_vcpu [ 0 ] , scratch_vcpu [ 1 ] ) ; UASM_i_ADDIU ( & p , K1 , A0 , offsetof ( kvm_vcpu , arch ) ) ; UASM_i_SW ( & p , SP , offsetof ( kvm_vcpu_arch , host_stack ) , K1 ) ; UASM_i_SW ( & p , GP , offsetof ( kvm_vcpu_arch , host_gp ) , K1 ) ; UASM_i_LA ( & p , K0 , ST0_EXL | KSU_USER | ST0_BEV | ST0_KX_IF_64 ) ; uasm_i_mtc0 ( & p , K0 , C0_STATUS ) ; uasm_i_ehb ( & p ) ; UASM_i_LW ( & p , K0 , offsetof ( kvm_vcpu_arch , guest_ebase ) , K1 ) ; build_set_exc_base ( & p , K0 ) ; uasm_i_addiu ( & p , K0 , ZERO , ST0_EXL | KSU_USER | ST0_IE | ST0_KX_IF_64 ) ; uasm_i_andi ( & p , V0 , V0 , ST0_IM ) ; uasm_i_or ( & p , K0 , K0 , V0 ) ; uasm_i_mtc0 ( & p , K0 , C0_STATUS ) ; uasm_i_ehb ( & p ) ; p = kvm_mips_build_enter_guest ( p ) ; return p ; } 
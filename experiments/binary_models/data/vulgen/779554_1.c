static void mix_column ( sph_u32 W [ 16 ] [ 4 ] , int ia , int ib , int ic , int id ) { int n ; for ( n = 0 ; n < 4 ; n ++ ) { sph_u32 a = W [ ia ] [ n ] ; sph_u32 b = W [ ib ] [ n ] ; sph_u32 c = W [ ic ] [ n ] ; sph_u32 d = W [ id ] [ n ] ; sph_u32 ab = a ^ b ; sph_u32 bc = b ^ c ; sph_u32 cd = c ^ d ; sph_u32 bcx = ( ( bc & C32 ( 0x80808080 ) ) >> 7 ) * 27U ^ ( ( bc & C32 ( 0x7F7F7F7F ) ) << 1 ) ; sph_u32 cdx = ( ( cd & C32 ( 0x80808080 ) ) >> 7 ) * 27U ^ ( ( cd & C32 ( 0x7F7F7F7F ) ) << 1 ) ; W [ ia ] [ n ] = abx ^ bc ^ d ; W [ ib ] [ n ] = bcx ^ a ^ cd ; W [ ic ] [ n ] = cdx ^ ab ^ d ; W [ id ] [ n ] = abx ^ bcx ^ cdx ^ ab ^ c ; } } sph_u32 a = W [ ia ] [ n ] ; sph_u32 b = W [ ib ] [ n ] ; sph_u32 c = W [ ic ] [ n ] ; sph_u32 d = W [ id ] [ n ] ; sph_u32 ab = a ^ b ; sph_u32 bc = b ^ c ; sph_u32 cd = c ^ d ; sph_u32 abx = ( ( ab & C32 ( 0x80808080 ) ) >> 7 ) * 27U ^ ( ( ab & C32 ( 0x7F7F7F7F ) ) << 1 ) ; sph_u32 bcx = ( ( bc & C32 ( 0x80808080 ) ) >> 7 ) * 27U ^ ( ( bc & C32 ( 0x7F7F7F7F ) ) << 1 ) ; sph_u32 cdx = ( ( cd & C32 ( 0x80808080 ) ) >> 7 ) * 27U ^ ( ( cd & C32 ( 0x7F7F7F7F ) ) << 1 ) ; W [ ia ] [ n ] = abx ^ bc ^ d ; W [ ib ] [ n ] = bcx ^ a ^ cd ; W [ ic ] [ n ] = cdx ^ ab ^ d ; W [ id ] [ n ] = abx ^ bcx ^ cdx ^ ab ^ c ; ( 0 ) MIX_COLUMN1 ( a , b , c , d , 0 ) ; MIX_COLUMN1 ( a , b , c , d , 1 ) ; MIX_COLUMN1 ( a , b , c , d , 2 ) ; MIX_COLUMN1 ( a , b , c , d , 3 ) ; ( 0 ) MIX_COLUMN ( 0 , 1 , 2 , 3 ) ; MIX_COLUMN ( 4 , 5 , 6 , 7 ) ; MIX_COLUMN ( 8 , 9 , 10 , 11 ) ; MIX_COLUMN ( 12 , 13 , 14 , 15 ) ; ( 0 ) BIG_SUB_WORDS ; BIG_SHIFT_ROWS ; BIG_MIX_COLUMNS ; ( 0 ) unsigned u ; sph_u32 * VV = & sc -> u . Vs [ 0 ] [ 0 ] ; sph_u32 * WW = & W [ 0 ] [ 0 ] ; for ( u = 0 ; u < 16 ; u ++ ) { VV [ u ] ^= sph_dec32le_aligned ( sc -> buf + ( u * 4 ) ) ^ sph_dec32le_aligned ( sc -> buf + ( u * 4 ) + 64 ) ^ sph_dec32le_aligned ( sc -> buf + ( u * 4 ) + 128 ) ^ WW [ u ] ^ WW [ u + 16 ] ^ WW [ u + 32 ] ^ WW [ u + 48 ] ; } ( 0 ) unsigned u ; sph_u32 * VV = & sc -> u . Vs [ 0 ] [ 0 ] ; sph_u32 * WW = & W [ 0 ] [ 0 ] ; for ( u = 0 ; u < 32 ; u ++ ) { VV [ u ] ^= sph_dec32le_aligned ( sc -> buf + ( u * 4 ) ) ^ WW [ u ] ^ WW [ u + 32 ] ; } ( 0 ) sph_u32 K0 = sc -> C0 ; sph_u32 K1 = sc -> C1 ; sph_u32 K2 = sc -> C2 ; sph_u32 K3 = sc -> C3 ; unsigned u ; INPUT_BLOCK_SMALL ( sc ) ; for ( u = 0 ; u < 8 ; u ++ ) { BIG_ROUND ; } FINAL_SMALL ; ( 0 ) sph_u32 K0 = sc -> C0 ; sph_u32 K1 = sc -> C1 ; sph_u32 K2 = sc -> C2 ; sph_u32 K3 = sc -> C3 ; unsigned u ; INPUT_BLOCK_BIG ( sc ) ; for ( u = 0 ; u < 10 ; u ++ ) { BIG_ROUND ; } FINAL_BIG ; ( 0 ) sc -> C0 = T32 ( sc -> C0 + ( sph_u32 ) ( val ) ) ; if ( sc -> C0 < ( sph_u32 ) ( val ) ) { if ( ( sc -> C1 = T32 ( sc -> C1 + 1 ) ) == 0 ) { if ( ( sc -> C2 = T32 ( sc -> C2 + 1 ) ) == 0 ) { sc -> C3 = T32 ( sc -> C3 + 1 ) ; } } } ( 0 ) static void echo_small_init ( , ) { sc -> u . Vb [ 0 ] [ 0 ] = ( sph_u64 ) out_len sc -> u . Vb [ 0 ] [ 1 ] = 0 sc -> u . Vb [ 1 ] [ 0 ] = ( sph_u64 ) out_len sc -> u . Vb [ 1 ] [ 1 ] = 0 sc -> u . Vb [ 2 ] [ 0 ] = ( sph_u64 ) out_len sc -> u . Vb [ 2 ] [ 1 ] = 0 sc -> u . Vb [ 3 ] [ 0 ] = ( sph_u64 ) out_len sc -> u . Vb [ 3 ] [ 1 ] = 0 sc -> u . Vs [ 0 ] [ 0 ] = ( sph_u32 ) out_len sc -> u . Vs [ 0 ] [ 1 ] = sc -> u . Vs [ 0 ] [ 2 ] = sc -> u . Vs [ 0 ] [ 3 ] = 0 sc -> u . Vs [ 1 ] [ 0 ] = ( sph_u32 ) out_len sc -> u . Vs [ 1 ] [ 1 ] = sc -> u . Vs [ 1 ] [ 2 ] = sc -> u . Vs [ 1 ] [ 3 ] = 0 sc -> u . Vs [ 2 ] [ 0 ] = ( sph_u32 ) out_len sc -> u . Vs [ 2 ] [ 1 ] = sc -> u . Vs [ 2 ] [ 2 ] = sc -> u . Vs [ 2 ] [ 3 ] = 0 sc -> u . Vs [ 3 ] [ 0 ] = ( sph_u32 ) out_len sc -> u . Vs [ 3 ] [ 1 ] = sc -> u . Vs [ 3 ] [ 2 ] = sc -> u . Vs [ 3 ] [ 3 ] = 0 sc -> ptr = 0 sc -> C0 = sc -> C1 = sc -> C2 = sc -> C3 = 0 } ; 
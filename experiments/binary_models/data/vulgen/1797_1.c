static irqreturn_t yellowfin_interrupt ( int irq , void * dev_instance ) { struct net_device * dev = dev_instance ; struct yellowfin_private * yp ; void __iomem * ioaddr ; int boguscnt = max_interrupt_work ; unsigned int handled ; yp = netdev_priv ( dev ) ; ioaddr = yp -> base ; spin_lock ( & yp -> lock ) ; { u16 intr_status = ioread16 ( ioaddr + IntrClear ) ; if ( yellowfin_debug > 4 ) { netdev_printk ( KERN_DEBUG , dev , "Yellowfin interrupt, status %04x\n" , intr_status ) ; } if ( intr_status == 0 ) { break ; } handled = 1 ; if ( intr_status & ( IntrRxDone | IntrEarlyRx ) ) { yellowfin_rx ( dev ) ; iowrite32 ( 0x10001000 , ioaddr + RxCtrl ) ; } for ( ; yp -> cur_tx - yp -> dirty_tx > 0 ; yp -> dirty_tx ++ ) { int entry = yp -> dirty_tx % TX_RING_SIZE ; struct sk_buff * skb ; if ( yp -> tx_ring [ entry ] . result_status == 0 ) { break ; } skb = yp -> tx_skbuff [ entry ] ; dev -> stats . tx_packets ++ ; dev -> stats . tx_bytes += skb -> len ; pci_unmap_single ( yp -> pci_dev , le32_to_cpu ( yp -> tx_ring [ entry ] . addr ) , skb -> len , PCI_DMA_TODEVICE ) ; dev_kfree_skb_irq ( skb ) ; yp -> tx_skbuff [ entry ] = NULL ; } if ( yp -> tx_full && yp -> cur_tx - yp -> dirty_tx < TX_QUEUE_SIZE - 4 ) { yp -> tx_full = 0 ; netif_wake_queue ( dev ) ; } if ( ( intr_status & IntrTxDone ) || ( yp -> tx_tail_desc -> tx_errs ) ) { unsigned dirty_tx = yp -> dirty_tx ; for ( dirty_tx = yp -> dirty_tx ; yp -> cur_tx - dirty_tx > 0 ; dirty_tx ++ ) { int entry = dirty_tx % TX_RING_SIZE ; u16 tx_errs = yp -> tx_status [ entry ] . tx_errs ; struct sk_buff * skb ; if ( yellowfin_debug > 5 ) { netdev_printk ( KERN_DEBUG , dev , "Tx queue %d check, Tx status %04x %04x %04x %04x\n" , entry , yp -> tx_status [ entry ] . tx_cnt , yp -> tx_status [ entry ] . tx_errs , yp -> tx_status [ entry ] . total_tx_cnt , yp -> tx_status [ entry ] . paused ) ; } if ( tx_errs == 0 ) { break ; } skb = yp -> tx_skbuff [ entry ] ; if ( tx_errs & 0xF810 ) { if ( yellowfin_debug > 1 ) { netdev_printk ( KERN_DEBUG , dev , "Transmit error, Tx status %04x\n" , tx_errs ) ; } dev -> stats . tx_errors ++ ; if ( tx_errs & 0xF800 ) { dev -> stats . tx_aborted_errors ++ ; } if ( tx_errs & 0x0800 ) { dev -> stats . tx_carrier_errors ++ ; } if ( tx_errs & 0x2000 ) { dev -> stats . tx_window_errors ++ ; } if ( tx_errs & 0x8000 ) { dev -> stats . tx_fifo_errors ++ ; } } else { if ( yellowfin_debug > 4 ) { netdev_printk ( KERN_DEBUG , dev , "Normal transmit, Tx status %04x\n" , tx_errs ) ; } dev -> stats . tx_bytes += skb -> len ; dev -> stats . collisions += tx_errs & 15 ; dev -> stats . tx_packets ++ ; } pci_unmap_single ( yp -> pci_dev , yp -> tx_ring [ entry << 1 ] . addr , skb -> len , PCI_DMA_TODEVICE ) ; dev_kfree_skb_irq ( skb ) ; yp -> tx_skbuff [ entry ] = 0 ; yp -> tx_status [ entry ] . tx_errs = 0 ; } if ( yp -> cur_tx - dirty_tx > TX_RING_SIZE ) { netdev_err ( dev , "Out-of-sync dirty pointer, %d vs. %d, full=%d\n" , dirty_tx , yp -> cur_tx , yp -> tx_full ) ; dirty_tx += TX_RING_SIZE ; } if ( yp -> tx_full && yp -> cur_tx - dirty_tx < TX_QUEUE_SIZE - 2 ) { yp -> tx_full = 0 ; netif_wake_queue ( dev ) ; } yp -> dirty_tx = dirty_tx ; yp -> tx_tail_desc = & yp -> tx_status [ dirty_tx % TX_RING_SIZE ] ; } if ( intr_status & 0x2ee ) { yellowfin_error ( dev , intr_status ) ; } if ( -- boguscnt < 0 ) { netdev_warn ( dev , "Too much work at interrupt, status=%#04x\n" , intr_status ) ; break ; } } 1 ; if ( yellowfin_debug > 3 ) { netdev_printk ( KERN_DEBUG , dev , "exiting interrupt, status=%#04x\n" , ioread16 ( ioaddr + IntrStatus ) ) ; } spin_unlock ( & yp -> lock ) ; return IRQ_RETVAL ( handled ) ; } 
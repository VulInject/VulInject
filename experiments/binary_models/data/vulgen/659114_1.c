static void hevc_hz_8t_8w_lsx ( const uint8_t * src , int32_t src_stride , int16_t * dst , int32_t dst_stride , const int8_t * filter , int32_t height ) { uint32_t loop_cnt ; int32_t src_stride_2x = ( src_stride << 1 ) ; int32_t dst_stride_x = ( dst_stride << 1 ) ; int32_t src_stride_4x = ( src_stride << 2 ) ; int32_t dst_stride_2x = ( dst_stride << 2 ) ; int32_t src_stride_3x = src_stride_2x + src_stride ; int32_t dst_stride_3x = dst_stride_2x + dst_stride_x ; __m128i src0 , src1 , src2 , src3 ; __m128i filt0 , filt1 , filt2 , filt3 ; __m128i mask1 , mask2 , mask3 ; __m128i vec0 , vec1 , vec2 , vec3 ; __m128i dst0 , dst1 , dst2 , dst3 ; src -= 3 ; DUP4_ARG2 ( __lsx_vldrepl_h , filter , 0 , filter , 2 , filter , 4 , filter , 6 , filt0 , filt1 , filt2 , filt3 ) ; DUP2_ARG2 ( __lsx_vaddi_bu , mask0 , 2 , mask0 , 4 , mask1 , mask2 ) ; mask3 = __lsx_vaddi_bu ( mask0 , 6 ) ; for ( loop_cnt = ( height >> 2 ) ; loop_cnt -- ; ) { src0 = __lsx_vld ( src , 0 ) ; DUP2_ARG2 ( __lsx_vldx , src , src_stride , src , src_stride_2x , src1 , src2 ) ; src3 = __lsx_vldx ( src , src_stride_3x ) ; src += src_stride_4x ; DUP4_ARG3 ( __lsx_vshuf_b , src0 , src0 , mask0 , src0 , src0 , mask1 , src0 , src0 , mask2 , src0 , src0 , mask3 , vec0 , vec1 , vec2 , vec3 ) ; dst0 = __lsx_vdp2_h_bu_b ( vec0 , filt0 ) ; DUP2_ARG3 ( __lsx_vdp2add_h_bu_b , dst0 , vec1 , filt1 , dst0 , vec2 , filt2 , dst0 , dst0 ) ; dst0 = __lsx_vdp2add_h_bu_b ( dst0 , vec3 , filt3 ) ; DUP4_ARG3 ( __lsx_vshuf_b , src1 , src1 , mask0 , src1 , src1 , mask1 , src1 , src1 , mask2 , src1 , src1 , mask3 , vec0 , vec1 , vec2 , vec3 ) ; dst1 = __lsx_vdp2_h_bu_b ( vec0 , filt0 ) ; DUP2_ARG3 ( __lsx_vdp2add_h_bu_b , dst1 , vec1 , filt1 , dst1 , vec2 , filt2 , dst1 , dst1 ) ; dst1 = __lsx_vdp2add_h_bu_b ( dst1 , vec3 , filt3 ) ; DUP4_ARG3 ( __lsx_vshuf_b , src2 , src2 , mask0 , src2 , src2 , mask1 , src2 , src2 , mask2 , src2 , src2 , mask3 , vec0 , vec1 , vec2 , vec3 ) ; dst2 = __lsx_vdp2_h_bu_b ( vec0 , filt0 ) ; DUP2_ARG3 ( __lsx_vdp2add_h_bu_b , dst2 , vec1 , filt1 , dst2 , vec2 , filt2 , dst2 , dst2 ) ; dst2 = __lsx_vdp2add_h_bu_b ( dst2 , vec3 , filt3 ) ; DUP4_ARG3 ( __lsx_vshuf_b , src3 , src3 , mask0 , src3 , src3 , mask1 , src3 , src3 , mask2 , src3 , src3 , mask3 , vec0 , vec1 , vec2 , vec3 ) ; dst3 = __lsx_vdp2_h_bu_b ( vec0 , filt0 ) ; DUP2_ARG3 ( __lsx_vdp2add_h_bu_b , dst3 , vec1 , filt1 , dst3 , vec2 , filt2 , dst3 , dst3 ) ; dst3 = __lsx_vdp2add_h_bu_b ( dst3 , vec3 , filt3 ) ; __lsx_vst ( dst0 , dst , 0 ) ; __lsx_vstx ( dst1 , dst , dst_stride_x ) ; __lsx_vstx ( dst2 , dst , dst_stride_2x ) ; __lsx_vstx ( dst3 , dst , dst_stride_3x ) ; dst += dst_stride_2x ; } } 
static bool rswitch_rx ( struct net_device * ndev , int * quota ) { struct rswitch_device * rdev = netdev_priv ( ndev ) ; struct rswitch_gwca_queue * gq = rdev -> rx_queue ; struct rswitch_ext_ts_desc * desc ; int limit , boguscnt , num , ret ; struct sk_buff * skb ; dma_addr_t dma_addr ; u16 pkt_len ; u32 get_ts ; boguscnt = min_t ( int , gq -> ring_size , * quota ) ; limit = boguscnt ; desc = & gq -> rx_ring [ gq -> cur ] ; while ( ( desc -> desc . die_dt & DT_MASK ) != DT_FEMPTY ) { if ( -- boguscnt < 0 ) { break ; } dma_rmb ( ) ; pkt_len = le16_to_cpu ( desc -> desc . info_ds ) & RX_DS ; skb = gq -> skbs [ gq -> cur ] ; gq -> skbs [ gq -> cur ] = NULL ; dma_addr = rswitch_desc_get_dptr ( & desc -> desc ) ; dma_unmap_single ( ndev -> dev . parent , dma_addr , PKT_BUF_SZ , DMA_FROM_DEVICE ) ; get_ts = rdev -> priv -> ptp_priv -> tstamp_rx_ctrl & RCAR_GEN4_RXTSTAMP_TYPE_V2_L2_EVENT ; if ( get_ts ) { struct skb_shared_hwtstamps * shhwtstamps ; struct timespec64 ts ; shhwtstamps = skb_hwtstamps ( skb ) ; ts . tv_sec = __le32_to_cpu ( desc -> ts_sec ) ; ts . tv_nsec = __le32_to_cpu ( desc -> ts_nsec & cpu_to_le32 ( 0x3fffffff ) ) ; shhwtstamps -> hwtstamp = timespec64_to_ktime ( ts ) ; } skb_put ( skb , pkt_len ) ; skb -> protocol = eth_type_trans ( skb , ndev ) ; netif_receive_skb ( skb ) ; rdev -> ndev -> stats . rx_packets ++ ; rdev -> ndev -> stats . rx_bytes += pkt_len ; gq -> cur = rswitch_next_queue_index ( gq , true , 1 ) ; desc = & gq -> rx_ring [ gq -> cur ] ; } num = rswitch_get_num_cur_queues ( gq ) ; ret = rswitch_gwca_queue_alloc_skb ( gq , gq -> dirty , num ) ; if ( ret < 0 ) { err } ret = rswitch_gwca_queue_ext_ts_fill ( ndev , gq , gq -> dirty , num ) ; if ( ret < 0 ) { err } gq -> dirty = rswitch_next_queue_index ( gq , false , num ) ; * quota -= limit - ( ++ boguscnt ) ; return boguscnt <= 0 ; err rswitch_gwca_halt ( rdev -> priv ) ; return 0 ; } 
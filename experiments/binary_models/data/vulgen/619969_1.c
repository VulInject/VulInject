void uvm_page_init ( vaddr_t * kvm_startp , vaddr_t * kvm_endp ) { vsize_t freepages , pagecount , n ; vm_page_t pagearray , curpg ; int lcv , i ; paddr_t paddr , pgno ; struct vm_physseg * seg ; TAILQ_INIT ( & uvm . page_active ) ; TAILQ_INIT ( & uvm . page_inactive ) ; mtx_init ( & uvm . pageqlock , IPL_VM ) ; mtx_init ( & uvm . fpageqlock , IPL_VM ) ; uvm_pmr_init ( ) ; if ( vm_nphysseg == 0 ) { panic ( "uvm_page_bootstrap: no memory pre-allocated" ) ; } freepages = 0 ; for ( lcv = 0 , seg = vm_physmem ; lcv < vm_nphysseg ; lcv ++ , seg ++ ) { freepages += ( seg -> end - seg -> start ) ; } pagecount = ( ( ( paddr_t ) freepages + 1 ) << PAGE_SHIFT ) / ( PAGE_SIZE + sizeof ( vm_page ) ) ; pagearray = ( vm_page_t ) uvm_pageboot_alloc ( pagecount * sizeof ( vm_page ) ) ; memset ( pagearray , 0 , pagecount * sizeof ( vm_page ) ) ; for ( lcv = 0 , seg = vm_physmem ; lcv < vm_nphysseg ; lcv ++ , seg ++ ) { n = seg -> end - seg -> start ; seg -> pgs = pagearray ; pagearray += n ; pagecount -= n ; seg -> lastpg = seg -> pgs + ( n - 1 ) ; pgno = seg -> start ; paddr = ptoa ( pgno ) ; for ( i = 0 , curpg = seg -> pgs ; i < n ; i ++ , curpg ++ , pgno ++ , paddr += PAGE_SIZE ) { curpg -> phys_addr = paddr ; VM_MDPAGE_INIT ( curpg ) ; if ( pgno >= seg -> avail_start && pgno < seg -> avail_end ) { uvmexp . npages ++ ; } } uvm_pmr_freepages ( & seg -> pgs [ seg -> avail_start - seg -> start ] , seg -> avail_end - seg -> avail_start ) ; } * kvm_startp = round_page ( virtual_space_start ) ; * kvm_endp = trunc_page ( virtual_space_end ) ; mtx_init ( & uvm . aiodoned_lock , IPL_BIO ) ; uvmexp . reserve_pagedaemon = 4 ; uvmexp . reserve_kernel = 8 ; uvmexp . anonminpct = 10 ; uvmexp . vnodeminpct = 10 ; uvmexp . vtextminpct = 5 ; uvmexp . anonmin = uvmexp . anonminpct * 256 / 100 ; uvmexp . vnodemin = uvmexp . vnodeminpct * 256 / 100 ; uvmexp . vtextmin = uvmexp . vtextminpct * 256 / 100 ; uvm . page_init_done = TRUE ; } 
static void tls_decrypt_done ( void * data , int err ) { struct aead_request * aead_req = data ; struct crypto_aead * aead = crypto_aead_reqtfm ( aead_req ) ; struct scatterlist * sgout = aead_req -> dst ; struct scatterlist * sgin = aead_req -> src ; struct tls_sw_context_rx * ctx ; struct tls_decrypt_ctx * dctx ; struct tls_context * tls_ctx ; struct scatterlist * sg ; unsigned int pages ; struct sock * sk ; int aead_size ; aead_size = sizeof ( * aead_req ) + crypto_aead_reqsize ( aead ) ; aead_size = ALIGN ( aead_size , __alignof__ ( * dctx ) ) ; dctx = ( void * ) ( ( u8 * ) aead_req + aead_size ) ; sk = dctx -> sk ; tls_ctx = tls_get_ctx ( sk ) ; ctx = tls_sw_ctx_rx ( tls_ctx ) ; if ( err ) { if ( err == - EBADMSG ) { TLS_INC_STATS ( sock_net ( sk ) , LINUX_MIB_TLSDECRYPTERROR ) ; } ctx -> async_wait . err = err ; tls_err_abort ( sk , err ) ; } if ( sgout != sgin ) { for_each_sg ( , , , ) { put_page ( sg_page ( sg ) ) ; } } kfree ( aead_req ) ; spin_lock_bh ( & ctx -> decrypt_compl_lock ) ; if ( ! atomic_dec_return ( & ctx -> decrypt_pending ) ) { complete ( & ctx -> async_wait . completion ) ; } spin_unlock_bh ( & ctx -> decrypt_compl_lock ) ; } 
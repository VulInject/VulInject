static int fence_update ( struct i915_fence_reg * fence , struct i915_vma * vma ) { struct i915_ggtt * ggtt = fence -> ggtt ; struct intel_uncore * uncore = fence_to_uncore ( fence ) ; intel_wakeref_t wakeref ; struct i915_vma * old ; int ret ; fence -> tiling = 0 ; if ( vma ) { GEM_BUG_ON ( ! i915_gem_object_get_stride ( vma -> obj ) || ! i915_gem_object_get_tiling ( vma -> obj ) ) ; if ( gpu_uses_fence_registers ( fence ) ) { ret = i915_vma_sync ( vma ) ; if ( ret ) { return ret ; } } fence -> start = vma -> node . start ; fence -> size = vma -> fence_size ; fence -> stride = i915_gem_object_get_stride ( vma -> obj ) ; fence -> tiling = i915_gem_object_get_tiling ( vma -> obj ) ; } WRITE_ONCE ( fence -> dirty , false ) ; old = xchg ( & fence -> vma , NULL ) ; if ( old ) { ret = i915_active_wait ( & fence -> active ) ; if ( ret ) { fence -> vma = old ; return ret ; } i915_vma_flush_writes ( old ) ; if ( old != vma ) { GEM_BUG_ON ( old -> fence != fence ) ; i915_vma_revoke_mmap ( old ) ; old -> fence = NULL ; } list_move ( & fence -> link , & ggtt -> fence_list ) ; } wakeref = intel_runtime_pm_get_if_in_use ( uncore -> rpm ) ; if ( ! wakeref ) { GEM_BUG_ON ( vma ) ; return 0 ; } WRITE_ONCE ( fence -> vma , vma ) ; fence_write ( fence ) ; if ( vma ) { vma -> fence = fence ; list_move_tail ( & fence -> link , & ggtt -> fence_list ) ; } intel_runtime_pm_put ( uncore -> rpm , wakeref ) ; return 0 ; } 
static int stmmac_xdp_xmit_xdpf ( struct stmmac_priv * priv , int queue , struct xdp_frame * xdpf , bool dma_map ) { struct stmmac_tx_queue * tx_q = & priv -> dma_conf . tx_queue [ queue ] ; unsigned int entry = tx_q -> cur_tx ; struct dma_desc * tx_desc ; dma_addr_t dma_addr ; bool set_ic ; if ( likely ( priv -> extend_desc ) ) { tx_desc = ( dma_desc * ) ( tx_q -> dma_etx + entry ) ; } if ( tx_q -> tbs & STMMAC_TBS_AVAIL ) { tx_desc = & tx_q -> dma_entx [ entry ] . basic ; } else { tx_desc = tx_q -> dma_tx + entry ; } if ( dma_map ) { dma_addr = dma_map_single ( priv -> device , xdpf -> data , xdpf -> len , DMA_TO_DEVICE ) ; if ( dma_mapping_error ( priv -> device , dma_addr ) ) { return STMMAC_XDP_CONSUMED ; } tx_q -> tx_skbuff_dma [ entry ] . buf_type = STMMAC_TXBUF_T_XDP_NDO ; } else { struct page * page = virt_to_page ( xdpf -> data ) ; dma_addr = page_pool_get_dma_addr ( page ) + sizeof ( * xdpf ) + xdpf -> headroom ; dma_sync_single_for_device ( priv -> device , dma_addr , xdpf -> len , DMA_BIDIRECTIONAL ) ; tx_q -> tx_skbuff_dma [ entry ] . buf_type = STMMAC_TXBUF_T_XDP_TX ; } tx_q -> tx_skbuff_dma [ entry ] . buf = dma_addr ; tx_q -> tx_skbuff_dma [ entry ] . map_as_page = false ; tx_q -> tx_skbuff_dma [ entry ] . len = xdpf -> len ; tx_q -> tx_skbuff_dma [ entry ] . last_segment = true ; tx_q -> tx_skbuff_dma [ entry ] . is_jumbo = false ; tx_q -> xdpf [ entry ] = xdpf ; stmmac_set_desc_addr ( priv , tx_desc , dma_addr ) ; stmmac_prepare_tx_desc ( priv , tx_desc , 1 , xdpf -> len , true , priv -> mode , true , true , xdpf -> len ) ; tx_q -> tx_count_frames ++ ; if ( tx_q -> tx_count_frames % priv -> tx_coal_frames [ queue ] == 0 ) { set_ic = true ; } else { set_ic = false ; } if ( set_ic ) { tx_q -> tx_count_frames = 0 ; stmmac_set_tx_ic ( priv , tx_desc ) ; priv -> xstats . tx_set_ic_bit ++ ; } stmmac_enable_dma_transmission ( priv , priv -> ioaddr ) ; entry = STMMAC_GET_ENTRY ( entry , priv -> dma_conf . dma_tx_size ) ; tx_q -> cur_tx = entry ; return STMMAC_XDP_TX ; } 
static void retract_page_tables ( struct address_space * mapping , pgoff_t pgoff ) { struct vm_area_struct * vma ; unsigned long addr ; pmd_t * pmd , _pmd ; i_mmap_lock_write ( mapping ) ; vma_interval_tree_foreach ( , , , ) { addr = vma -> vm_start + ( ( pgoff - vma -> vm_pgoff ) << PAGE_SHIFT ) ; if ( addr & ~ HPAGE_PMD_MASK ) { continue ; } if ( vma -> vm_end < addr + HPAGE_PMD_SIZE ) { continue ; } pmd = mm_find_pmd ( vma -> vm_mm , addr ) ; if ( ! pmd ) { continue ; } if ( down_write_trylock ( & vma -> vm_mm -> mmap_sem ) ) { spinlock_t * ptl = pmd_lock ( vma -> vm_mm , pmd ) ; _pmd = pmdp_collapse_flush ( vma , addr , pmd ) ; spin_unlock ( ptl ) ; up_write ( & vma -> vm_mm -> mmap_sem ) ; atomic_long_dec ( & vma -> vm_mm -> nr_ptes ) ; pte_free ( vma -> vm_mm , pmd_pgtable ( _pmd ) ) ; } } i_mmap_unlock_write ( mapping ) ; } static void collapse_shmem ( struct mm_struct * mm , struct address_space * mapping , pgoff_t start , struct page * * hpage , int node ) { gfp_t gfp ; struct page * page , * new_page , * tmp ; struct mem_cgroup * memcg ; pgoff_t index , end = start + HPAGE_PMD_NR ; LIST_HEAD ( pagelist ) ; struct radix_tree_iter iter ; void * * slot ; int nr_none = 0 , result = SCAN_SUCCEED ; VM_BUG_ON ( start & ( HPAGE_PMD_NR - 1 ) ) ; gfp = alloc_hugepage_khugepaged_gfpmask ( ) | __GFP_THISNODE ; new_page = khugepaged_alloc_page ( hpage , gfp , node ) ; if ( ! new_page ) { result = SCAN_ALLOC_HUGE_PAGE_FAIL ; out } if ( unlikely ( mem_cgroup_try_charge ( new_page , mm , gfp , & memcg , true ) ) ) { result = SCAN_CGROUP_CHARGE_FAIL ; out } new_page -> index = start ; new_page -> mapping = mapping ; __SetPageSwapBacked ( new_page ) ; __SetPageLocked ( new_page ) ; BUG_ON ( ! page_ref_freeze ( new_page , 1 ) ) ; index = start ; spin_lock_irq ( & mapping -> tree_lock ) ; radix_tree_for_each_slot ( , , , ) { int n = min ( iter . index , end ) - index ; if ( n && ! shmem_charge ( mapping -> host , n ) ) { result = SCAN_FAIL ; break ; } nr_none += n ; for ( ; index < min ( iter . index , end ) ; index ++ ) { radix_tree_insert ( & mapping -> page_tree , index , new_page + ( index % HPAGE_PMD_NR ) ) ; } if ( index >= end ) { break ; } page = radix_tree_deref_slot_protected ( slot , & mapping -> tree_lock ) ; if ( radix_tree_exceptional_entry ( page ) || ! PageUptodate ( page ) ) { spin_unlock_irq ( & mapping -> tree_lock ) ; if ( shmem_getpage ( mapping -> host , index , & page , SGP_NOHUGE ) ) { result = SCAN_FAIL ; tree_unlocked } spin_lock_irq ( & mapping -> tree_lock ) ; } if ( trylock_page ( page ) ) { get_page ( page ) ; } else { result = SCAN_PAGE_LOCK ; break ; } VM_BUG_ON_PAGE ( ! PageLocked ( page ) , page ) ; VM_BUG_ON_PAGE ( ! PageUptodate ( page ) , page ) ; VM_BUG_ON_PAGE ( PageTransCompound ( page ) , page ) ; if ( page_mapping ( page ) != mapping ) { result = SCAN_TRUNCATED ; out_unlock } spin_unlock_irq ( & mapping -> tree_lock ) ; if ( isolate_lru_page ( page ) ) { result = SCAN_DEL_PAGE_LRU ; out_isolate_failed } if ( page_mapped ( page ) ) { unmap_mapping_range ( mapping , index << PAGE_SHIFT , PAGE_SIZE , 0 ) ; } spin_lock_irq ( & mapping -> tree_lock ) ; slot = radix_tree_lookup_slot ( & mapping -> page_tree , index ) ; VM_BUG_ON_PAGE ( page != radix_tree_deref_slot_protected ( slot , & mapping -> tree_lock ) , page ) ; VM_BUG_ON_PAGE ( page_mapped ( page ) , page ) ; if ( ! page_ref_freeze ( page , 3 ) ) { result = SCAN_PAGE_COUNT ; out_lru } list_add_tail ( & page -> lru , & pagelist ) ; radix_tree_replace_slot ( & mapping -> page_tree , slot , new_page + ( index % HPAGE_PMD_NR ) ) ; slot = radix_tree_iter_resume ( slot , & iter ) ; index ++ ; continue ; out_lru spin_unlock_irq ( & mapping -> tree_lock ) ; putback_lru_page ( page ) ; out_isolate_failed unlock_page ( page ) ; put_page ( page ) ; tree_unlocked out_unlock unlock_page ( page ) ; put_page ( page ) ; break ; } if ( result == SCAN_SUCCEED && index < end ) { int n = end - index ; if ( ! shmem_charge ( mapping -> host , n ) ) { result = SCAN_FAIL ; tree_locked } for ( ; index < end ; index ++ ) { radix_tree_insert ( & mapping -> page_tree , index , new_page + ( index % HPAGE_PMD_NR ) ) ; } nr_none += n ; } tree_locked spin_unlock_irq ( & mapping -> tree_lock ) ; tree_unlocked if ( result == SCAN_SUCCEED ) { unsigned long flags ; struct zone * zone = page_zone ( new_page ) ; list_for_each_entry_safe ( , , , ) { copy_highpage ( new_page + ( page -> index % HPAGE_PMD_NR ) , page ) ; list_del ( & page -> lru ) ; unlock_page ( page ) ; page_ref_unfreeze ( page , 1 ) ; page -> mapping = NULL ; ClearPageActive ( page ) ; ClearPageUnevictable ( page ) ; put_page ( page ) ; } local_irq_save ( flags ) ; __inc_node_page_state ( new_page , NR_SHMEM_THPS ) ; if ( nr_none ) { __mod_node_page_state ( zone -> zone_pgdat , NR_FILE_PAGES , nr_none ) ; __mod_node_page_state ( zone -> zone_pgdat , NR_SHMEM , nr_none ) ; } local_irq_restore ( flags ) ; retract_page_tables ( mapping , start ) ; set_page_dirty ( new_page ) ; SetPageUptodate ( new_page ) ; page_ref_unfreeze ( new_page , HPAGE_PMD_NR ) ; mem_cgroup_commit_charge ( new_page , memcg , false , true ) ; lru_cache_add_anon ( new_page ) ; unlock_page ( new_page ) ; * hpage = NULL ; } else { shmem_uncharge ( mapping -> host , nr_none ) ; spin_lock_irq ( & mapping -> tree_lock ) ; radix_tree_for_each_slot ( , , , ) { if ( iter . index >= end ) { break ; } page = list_first_entry_or_null ( & pagelist , page , lru ) ; if ( ! page || iter . index < page -> index ) { if ( ! nr_none ) { break ; } nr_none -- ; radix_tree_delete ( & mapping -> page_tree , iter . index ) ; continue ; } VM_BUG_ON_PAGE ( page -> index != iter . index , page ) ; list_del ( & page -> lru ) ; page_ref_unfreeze ( page , 2 ) ; radix_tree_replace_slot ( & mapping -> page_tree , slot , page ) ; slot = radix_tree_iter_resume ( slot , & iter ) ; spin_unlock_irq ( & mapping -> tree_lock ) ; putback_lru_page ( page ) ; unlock_page ( page ) ; spin_lock_irq ( & mapping -> tree_lock ) ; } VM_BUG_ON ( nr_none ) ; spin_unlock_irq ( & mapping -> tree_lock ) ; page_ref_unfreeze ( new_page , 1 ) ; mem_cgroup_cancel_charge ( new_page , memcg , true ) ; unlock_page ( new_page ) ; new_page -> mapping = NULL ; } out VM_BUG_ON ( ! list_empty ( & pagelist ) ) ; } 
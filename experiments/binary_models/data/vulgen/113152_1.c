BPF_CALL_3 ( , , , , , , ) { struct bpf_stack_map * smap = container_of ( map , bpf_stack_map , map ) ; struct perf_callchain_entry * trace ; struct stack_map_bucket * bucket , * new_bucket , * old_bucket ; u32 max_depth = map -> value_size / 8 ; u32 init_nr = sysctl_perf_event_max_stack - max_depth ; u32 skip = flags & BPF_F_SKIP_FIELD_MASK ; u32 hash , id , trace_nr , trace_len ; bool user = flags & BPF_F_USER_STACK ; bool kernel = ! user ; u64 * ips ; trace = get_perf_callchain ( regs , init_nr , kernel , user , sysctl_perf_event_max_stack , false , false ) ; if ( unlikely ( ! trace ) ) { return - EFAULT ; } trace_nr = trace -> nr - init_nr ; if ( trace_nr <= skip ) { return - EFAULT ; } trace_nr -= skip ; trace_len = trace_nr * sizeof ( u64 ) ; ips = trace -> ip + skip + init_nr ; hash = jhash2 ( ( u32 * ) ips , trace_len / sizeof ( u32 ) , 0 ) ; id = hash & ( smap -> n_buckets - 1 ) ; bucket = READ_ONCE ( smap -> buckets [ id ] ) ; if ( bucket && bucket -> hash == hash ) { if ( flags & BPF_F_FAST_STACK_CMP ) { return id ; } if ( bucket -> nr == trace_nr && memcmp ( bucket -> ip , ips , trace_len ) == 0 ) { return id ; } } if ( bucket && ! ( flags & BPF_F_REUSE_STACKID ) ) { return - EEXIST ; } new_bucket = ( stack_map_bucket * ) pcpu_freelist_pop ( & smap -> freelist ) ; if ( unlikely ( ! new_bucket ) ) { return - ENOMEM ; } memcpy ( new_bucket -> ip , ips , trace_len ) ; new_bucket -> hash = hash ; new_bucket -> nr = trace_nr ; old_bucket = xchg ( & smap -> buckets [ id ] , new_bucket ) ; if ( old_bucket ) { pcpu_freelist_push ( & smap -> freelist , & old_bucket -> fnode ) ; } return id ; } const struct bpf_func_proto bpf_get_stackid_proto = { . func = bpf_get_stackid . gpl_only = true . ret_type = RET_INTEGER . arg1_type = ARG_PTR_TO_CTX . arg2_type = ARG_CONST_MAP_PTR . arg3_type = ARG_ANYTHING } ; 
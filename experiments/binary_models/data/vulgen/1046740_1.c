static boolean_t ena_attach_device_init ( ena_t * ena ) { ena_adminq_t * aq = & ena -> ena_aq ; uint32_t rval , wval ; uint8_t dma_width ; hrtime_t timeout , cmd_timeout ; hrtime_t expired ; enahw_resp_desc_t resp ; enahw_feat_dev_attr_t * feat = & resp . erd_resp . erd_get_feat . ergf_dev_attr ; uint8_t * maddr ; uint32_t supported_features ; int ret = 0 ; rval = ena_hw_bar_read32 ( ena , ENAHW_REG_DEV_STS ) ; if ( ( rval & ENAHW_DEV_STS_READY_MASK ) == 0 ) { ena_err ( ena , "device is not ready" ) ; return ( B_FALSE ) ; } rval = ena_hw_bar_read32 ( ena , ENAHW_REG_CAPS ) ; if ( timeout == 0 ) { ena_err ( ena , "device gave invalid reset timeout" ) ; return ( B_FALSE ) ; } expired = gethrtime ( ) + timeout ; wval = ENAHW_DEV_CTL_DEV_RESET_MASK ; wval |= ( ENAHW_RESET_NORMAL << ENAHW_DEV_CTL_RESET_REASON_SHIFT ) & ENAHW_DEV_CTL_RESET_REASON_MASK ; ena_hw_bar_write32 ( ena , ENAHW_REG_DEV_CTL , wval ) ; while ( 1 ) { rval = ena_hw_bar_read32 ( ena , ENAHW_REG_DEV_STS ) ; if ( ( rval & ENAHW_DEV_STS_RESET_IN_PROGRESS_MASK ) != 0 ) { break ; } if ( gethrtime ( ) > expired ) { ena_err ( ena , "device reset start timed out" ) ; return ( B_FALSE ) ; } delay ( drv_usectohz ( 100 * 1000 ) ) ; } expired = gethrtime ( ) + timeout ; ena_hw_bar_write32 ( ena , ENAHW_REG_DEV_CTL , 0 ) ; while ( 1 ) { rval = ena_hw_bar_read32 ( ena , ENAHW_REG_DEV_STS ) ; if ( ( rval & ENAHW_DEV_STS_RESET_IN_PROGRESS_MASK ) == 0 ) { break ; } if ( gethrtime ( ) > expired ) { ena_err ( ena , "device reset timed out" ) ; return ( B_FALSE ) ; } delay ( drv_usectohz ( 100 * 1000 ) ) ; } if ( ! ena_check_versions ( ena ) ) { return ( B_FALSE ) ; } rval = ena_hw_bar_read32 ( ena , ENAHW_REG_CAPS ) ; dma_width = ENAHW_CAPS_DMA_ADDR_WIDTH ( rval ) ; ena -> ena_dma_width = dma_width ; mutex_init ( & aq -> ea_sq_lock , NULL , MUTEX_DRIVER , NULL ) ; mutex_init ( & aq -> ea_cq_lock , NULL , MUTEX_DRIVER , NULL ) ; mutex_init ( & aq -> ea_stat_lock , NULL , MUTEX_DRIVER , NULL ) ; aq -> ea_qlen = ENA_ADMINQ_DEPTH ; aq -> ea_pending_cmds = 0 ; aq -> ea_cmd_ctxs = kmem_zalloc ( sizeof ( ena_cmd_ctx_t ) * aq -> ea_qlen , KM_SLEEP ) ; list_create ( & aq -> ea_cmd_ctxs_free , sizeof ( ena_cmd_ctx_t ) , offsetof ( ena_cmd_ctx_t , ectx_node ) ) ; for ( uint_t i = 0 ; i < aq -> ea_qlen ; i ++ ) { ena_cmd_ctx_t * ctx = & aq -> ea_cmd_ctxs [ i ] ; ctx -> ectx_id = i ; ctx -> ectx_pending = B_FALSE ; ctx -> ectx_cmd_opcode = ENAHW_CMD_NONE ; ctx -> ectx_resp = NULL ; list_insert_tail ( & aq -> ea_cmd_ctxs_free , ctx ) ; } cmd_timeout = MSEC2NSEC ( ENAHW_CAPS_ADMIN_CMD_TIMEOUT ( rval ) * 100 ) ; aq -> ea_cmd_timeout_ns = max ( cmd_timeout , ena_admin_cmd_timeout_ns ) ; if ( aq -> ea_cmd_timeout_ns == 0 ) { aq -> ea_cmd_timeout_ns = ENA_ADMIN_CMD_DEF_TIMEOUT ; } if ( ! ena_admin_sq_init ( ena ) ) { return ( B_FALSE ) ; } if ( ! ena_admin_cq_init ( ena ) ) { return ( B_FALSE ) ; } if ( ! ena_aenq_init ( ena ) ) { return ( B_FALSE ) ; } ena_hw_bar_write32 ( ena , ENAHW_REG_INTERRUPT_MASK , 0 ) ; aq -> ea_poll_mode = B_TRUE ; bzero ( & resp , sizeof ( resp ) ) ; ret = ena_get_feature ( ena , & resp , ENAHW_FEAT_DEVICE_ATTRIBUTES , ENAHW_FEAT_DEVICE_ATTRIBUTES_VER ) ; if ( ret != 0 ) { ena_err ( ena , "failed to get device attributes: %d" , ret ) ; return ( B_FALSE ) ; } ena_dbg ( ena , "impl ID: %u" , feat -> efda_impl_id ) ; ena_dbg ( ena , "device version: %u" , feat -> efda_device_version ) ; ena_dbg ( ena , "supported features: 0x%x" , feat -> efda_supported_features ) ; ena_dbg ( ena , "phys addr width: %u" , feat -> efda_phys_addr_width ) ; ena_dbg ( ena , "virt addr width: %u" , feat -> efda_virt_addr_with ) ; maddr = feat -> efda_mac_addr ; ena_dbg ( ena , "mac addr: %x:%x:%x:%x:%x:%x" , maddr [ 0 ] , maddr [ 1 ] , maddr [ 2 ] , maddr [ 3 ] , maddr [ 4 ] , maddr [ 5 ] ) ; ena_dbg ( ena , "max MTU: %u" , feat -> efda_max_mtu ) ; bcopy ( maddr , ena -> ena_mac_addr , ETHERADDRL ) ; ena -> ena_max_mtu = feat -> efda_max_mtu ; supported_features = feat -> efda_supported_features ; ena -> ena_supported_features = supported_features ; feat = NULL ; bzero ( & resp , sizeof ( resp ) ) ; if ( supported_features & BIT ( ENAHW_FEAT_MAX_QUEUES_EXT ) ) { enahw_feat_max_queue_ext_t * feat_mqe = & resp . erd_resp . erd_get_feat . ergf_max_queue_ext ; ret = ena_get_feature ( ena , & resp , ENAHW_FEAT_MAX_QUEUES_EXT , ENAHW_FEAT_MAX_QUEUES_EXT_VER ) ; if ( ret != 0 ) { ena_err ( ena , "failed to query max queues ext: %d" , ret ) ; return ( B_FALSE ) ; } ena -> ena_tx_max_sq_num = feat_mqe -> efmqe_max_tx_sq_num ; ena -> ena_tx_max_sq_num_descs = feat_mqe -> efmqe_max_tx_sq_depth ; ena -> ena_tx_max_cq_num = feat_mqe -> efmqe_max_tx_cq_num ; ena -> ena_tx_max_cq_num_descs = feat_mqe -> efmqe_max_tx_cq_depth ; ena -> ena_tx_max_desc_per_pkt = feat_mqe -> efmqe_max_per_packet_tx_descs ; ena -> ena_tx_max_hdr_len = feat_mqe -> efmqe_max_tx_header_size ; ena -> ena_rx_max_sq_num = feat_mqe -> efmqe_max_rx_sq_num ; ena -> ena_rx_max_sq_num_descs = feat_mqe -> efmqe_max_rx_sq_depth ; ena -> ena_rx_max_cq_num = feat_mqe -> efmqe_max_rx_cq_num ; ena -> ena_rx_max_cq_num_descs = feat_mqe -> efmqe_max_rx_cq_depth ; ena -> ena_rx_max_desc_per_pkt = feat_mqe -> efmqe_max_per_packet_rx_descs ; ena_set_max_io_queues ( ena ) ; } else { enahw_feat_max_queue_t * feat_mq = & resp . erd_resp . erd_get_feat . ergf_max_queue ; ret = ena_get_feature ( ena , & resp , ENAHW_FEAT_MAX_QUEUES_NUM , ENAHW_FEAT_MAX_QUEUES_NUM_VER ) ; if ( ret != 0 ) { ena_err ( ena , "failed to query max queues: %d" , ret ) ; return ( B_FALSE ) ; } ena -> ena_tx_max_sq_num = feat_mq -> efmq_max_sq_num ; ena -> ena_tx_max_sq_num_descs = feat_mq -> efmq_max_sq_depth ; ena -> ena_tx_max_cq_num = feat_mq -> efmq_max_cq_num ; ena -> ena_tx_max_cq_num_descs = feat_mq -> efmq_max_cq_depth ; ena -> ena_tx_max_desc_per_pkt = feat_mq -> efmq_max_per_packet_tx_descs ; ena -> ena_tx_max_hdr_len = feat_mq -> efmq_max_header_size ; ena -> ena_rx_max_sq_num = feat_mq -> efmq_max_sq_num ; ena -> ena_rx_max_sq_num_descs = feat_mq -> efmq_max_sq_depth ; ena -> ena_rx_max_cq_num = feat_mq -> efmq_max_cq_num ; ena -> ena_rx_max_cq_num_descs = feat_mq -> efmq_max_cq_depth ; ena -> ena_rx_max_desc_per_pkt = feat_mq -> efmq_max_per_packet_rx_descs ; ena_set_max_io_queues ( ena ) ; } ena -> ena_mtu = ena -> ena_max_mtu ; ena_update_buf_sizes ( ena ) ; ena -> ena_tx_sgl_max_sz = 1 ; ena -> ena_rx_sgl_max_sz = 1 ; if ( ! ena_init_host_info ( ena ) ) { return ( B_FALSE ) ; } if ( ! ena_setup_aenq ( ena ) ) { return ( B_FALSE ) ; } ena_get_link_config ( ena ) ; if ( ! ena_get_offloads ( ena ) ) { return ( B_FALSE ) ; } if ( ! ena_stat_device_basic_init ( ena ) ) { return ( B_FALSE ) ; } if ( ! ena_stat_device_extended_init ( ena ) ) { return ( B_FALSE ) ; } if ( ! ena_stat_aenq_init ( ena ) ) { return ( B_FALSE ) ; } return ( B_TRUE ) ; } 
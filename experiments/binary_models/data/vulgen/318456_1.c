static netdev_tx_t cxgb4_eth_xmit ( struct sk_buff * skb , struct net_device * dev ) { enum cpl_tx_tnl_lso_type tnl_type = TX_TNL_TYPE_OPAQUE ; bool ptp_enabled = is_ptp_enabled ( skb , dev ) ; unsigned int last_desc , flits , ndesc ; u32 wr_mid , ctrl0 , op , sgl_off = 0 ; const struct skb_shared_info * ssi ; int len , qidx , credits , ret , left ; struct tx_sw_desc * sgl_sdesc ; struct fw_eth_tx_eo_wr * eowr ; struct fw_eth_tx_pkt_wr * wr ; struct cpl_tx_pkt_core * cpl ; const struct port_info * pi ; bool immediate = false ; u64 cntrl , * end , * sgl ; struct sge_eth_txq * q ; unsigned int chip_ver ; struct adapter * adap ; ret = cxgb4_validate_skb ( skb , dev , ETH_HLEN ) ; if ( ret ) { out_free } pi = netdev_priv ( dev ) ; adap = pi -> adapter ; ssi = skb_shinfo ( skb ) ; if ( xfrm_offload ( skb ) && ! ssi -> gso_size ) { return adap -> uld [ CXGB4_ULD_IPSEC ] . tx_handler ( skb , dev ) ; } if ( cxgb4_is_ktls_skb ( skb ) && ( skb -> len - skb_tcp_all_headers ( skb ) ) ) { return adap -> uld [ CXGB4_ULD_KTLS ] . tx_handler ( skb , dev ) ; } qidx = skb_get_queue_mapping ( skb ) ; if ( ptp_enabled ) { if ( ! ( adap -> ptp_tx_skb ) ) { skb_shinfo ( skb ) -> tx_flags |= SKBTX_IN_PROGRESS ; adap -> ptp_tx_skb = skb_get ( skb ) ; } else { out_free } q = & adap -> sge . ptptxq ; } else { q = & adap -> sge . ethtxq [ qidx + pi -> first_qset ] ; } skb_tx_timestamp ( skb ) ; reclaim_completed_tx ( adap , & q -> q , - 1 , true ) ; cntrl = TXPKT_L4CSUM_DIS_F | TXPKT_IPCSUM_DIS_F ; ret = cxgb_fcoe_offload ( skb , adap , pi , & cntrl ) ; if ( unlikely ( ret == - EOPNOTSUPP ) ) { out_free } chip_ver = CHELSIO_CHIP_VERSION ( adap -> params . chip ) ; flits = calc_tx_flits ( skb , chip_ver ) ; ndesc = flits_to_desc ( flits ) ; credits = txq_avail ( & q -> q ) - ndesc ; if ( unlikely ( credits < 0 ) ) { eth_txq_stop ( q ) ; dev_err ( adap -> pdev_dev , "%s: Tx ring %u full while queue awake!\n" , dev -> name , qidx ) ; return NETDEV_TX_BUSY ; } if ( is_eth_imm ( skb , chip_ver ) ) { immediate = true ; } if ( skb -> encapsulation && chip_ver > CHELSIO_T5 ) { tnl_type = cxgb_encap_offload_supported ( skb ) ; } last_desc = q -> q . pidx + ndesc - 1 ; if ( last_desc >= q -> q . size ) { last_desc -= q -> q . size ; } sgl_sdesc = & q -> q . sdesc [ last_desc ] ; if ( ! immediate && unlikely ( cxgb4_map_skb ( adap -> pdev_dev , skb , sgl_sdesc -> addr ) < 0 ) ) { memset ( sgl_sdesc -> addr , 0 , sizeof ( sgl_sdesc -> addr ) ) ; q -> mapping_err ++ ; out_free } wr_mid = FW_WR_LEN16_V ( DIV_ROUND_UP ( flits , 2 ) ) ; if ( unlikely ( credits < ETHTXQ_STOP_THRES ) ) { eth_txq_stop ( q ) ; if ( chip_ver > CHELSIO_T5 ) { wr_mid |= FW_WR_EQUEQ_F | FW_WR_EQUIQ_F ; } } wr = ( void * ) & q -> q . desc [ q -> q . pidx ] ; eowr = ( void * ) & q -> q . desc [ q -> q . pidx ] ; wr -> equiq_to_len16 = htonl ( wr_mid ) ; wr -> r3 = cpu_to_be64 ( 0 ) ; if ( skb_shinfo ( skb ) -> gso_type & SKB_GSO_UDP_L4 ) { end = ( u64 * ) eowr + flits ; } else { end = ( u64 * ) wr + flits ; } len = immediate ?skb -> len : 0 ; len += sizeof ( * cpl ) ; if ( ssi -> gso_size && ! ( ssi -> gso_type & SKB_GSO_UDP_L4 ) ) { struct cpl_tx_pkt_lso_core * lso = ( void * ) ( wr + 1 ) ; struct cpl_tx_tnl_lso * tnl_lso = ( void * ) ( wr + 1 ) ; if ( tnl_type ) { len += sizeof ( * tnl_lso ) ; } else { len += sizeof ( * lso ) ; } wr -> op_immdlen = htonl ( FW_WR_OP_V ( FW_ETH_TX_PKT_WR ) | FW_WR_IMMDLEN_V ( len ) ) ; if ( tnl_type ) { struct iphdr * iph = ip_hdr ( skb ) ; t6_fill_tnl_lso ( skb , tnl_lso , tnl_type ) ; cpl = ( void * ) ( tnl_lso + 1 ) ; if ( iph -> version == 4 ) { iph -> check = 0 ; iph -> tot_len = 0 ; iph -> check = ~ ip_fast_csum ( ( u8 * ) iph , iph -> ihl ) ; } if ( skb -> ip_summed == CHECKSUM_PARTIAL ) { cntrl = hwcsum ( adap -> params . chip , skb ) ; } } else { cpl = write_tso_wr ( adap , skb , lso ) ; cntrl = hwcsum ( adap -> params . chip , skb ) ; } sgl = ( u64 * ) ( cpl + 1 ) ; q -> tso ++ ; q -> tx_cso += ssi -> gso_segs ; } if ( ssi -> gso_size ) { u64 * start ; u32 hdrlen ; hdrlen = eth_get_headlen ( dev , skb -> data , skb_headlen ( skb ) ) ; len += hdrlen ; wr -> op_immdlen = cpu_to_be32 ( FW_WR_OP_V ( FW_ETH_TX_EO_WR ) | FW_ETH_TX_EO_WR_IMMDLEN_V ( len ) ) ; cpl = write_eo_udp_wr ( skb , eowr , hdrlen ) ; cntrl = hwcsum ( adap -> params . chip , skb ) ; start = ( u64 * ) ( cpl + 1 ) ; sgl = ( u64 * ) inline_tx_skb_header ( skb , & q -> q , ( void * ) start , hdrlen ) ; if ( unlikely ( start > sgl ) ) { left = ( u8 * ) end - ( u8 * ) q -> q . stat ; end = ( void * ) q -> q . desc + left ; } sgl_off = hdrlen ; q -> uso ++ ; q -> tx_cso += ssi -> gso_segs ; } else { if ( ptp_enabled ) { op = FW_PTP_TX_PKT_WR ; } else { op = FW_ETH_TX_PKT_WR ; } wr -> op_immdlen = htonl ( FW_WR_OP_V ( op ) | FW_WR_IMMDLEN_V ( len ) ) ; cpl = ( void * ) ( wr + 1 ) ; sgl = ( u64 * ) ( cpl + 1 ) ; if ( skb -> ip_summed == CHECKSUM_PARTIAL ) { cntrl = hwcsum ( adap -> params . chip , skb ) | TXPKT_IPCSUM_DIS_F ; q -> tx_cso ++ ; } } if ( unlikely ( ( u8 * ) sgl >= ( u8 * ) q -> q . stat ) ) { left = ( u8 * ) end - ( u8 * ) q -> q . stat ; end = ( void * ) q -> q . desc + left ; sgl = ( void * ) q -> q . desc ; } if ( skb_vlan_tag_present ( skb ) ) { q -> vlan_ins ++ ; cntrl |= TXPKT_VLAN_VLD_F | TXPKT_VLAN_V ( skb_vlan_tag_get ( skb ) ) ; } ctrl0 = TXPKT_OPCODE_V ( CPL_TX_PKT_XT ) | TXPKT_INTF_V ( pi -> tx_chan ) | TXPKT_PF_V ( adap -> pf ) ; if ( ptp_enabled ) { ctrl0 |= TXPKT_TSTAMP_F ; } if ( is_t4 ( adap -> params . chip ) ) { ctrl0 |= TXPKT_OVLAN_IDX_V ( q -> dcb_prio ) ; } else { ctrl0 |= TXPKT_T5_OVLAN_IDX_V ( q -> dcb_prio ) ; } cpl -> ctrl0 = htonl ( ctrl0 ) ; cpl -> pack = htons ( 0 ) ; cpl -> len = htons ( skb -> len ) ; cpl -> ctrl1 = cpu_to_be64 ( cntrl ) ; if ( immediate ) { cxgb4_inline_tx_skb ( skb , & q -> q , sgl ) ; dev_consume_skb_any ( skb ) ; } else { cxgb4_write_sgl ( skb , & q -> q , ( void * ) sgl , end , sgl_off , sgl_sdesc -> addr ) ; skb_orphan ( skb ) ; sgl_sdesc -> skb = skb ; } txq_advance ( & q -> q , ndesc ) ; cxgb4_ring_tx_db ( adap , & q -> q , ndesc ) ; return NETDEV_TX_OK ; out_free dev_kfree_skb_any ( skb ) ; return NETDEV_TX_OK ; } 
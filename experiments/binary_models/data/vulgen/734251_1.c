LIBXSMM_API_INTERN void libxsmm_generator_gemm_setup_B_in_vnniT_to_stack_aarch64 ( libxsmm_generated_code * io_generated_code , libxsmm_loop_label_tracker * io_loop_label_tracker , const libxsmm_gp_reg_mapping * i_gp_reg_mapping , libxsmm_micro_kernel_config * i_micro_kernel_config , libxsmm_gemm_descriptor * i_xgemm_desc , const libxsmm_gemm_descriptor * i_xgemm_desc_orig , libxsmm_datatype i_in_dtype ) { int is_stride_brgemm = ( ( i_xgemm_desc -> flags & LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE ) > 0 ) ?1 : 0 ; int is_offset_brgemm = ( ( i_xgemm_desc -> flags & LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET ) > 0 ) ?1 : 0 ; int is_address_brgemm = ( ( i_xgemm_desc -> flags & LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS ) > 0 ) ?1 : 0 ; int is_brgemm = ( ( is_stride_brgemm == 1 ) || ( is_offset_brgemm == 1 ) || ( is_address_brgemm == 1 ) ) ?1 : 0 ; unsigned int struct_gp_reg = LIBXSMM_AARCH64_GP_REG_X6 ; unsigned int tmp_reg = LIBXSMM_AARCH64_GP_REG_X10 ; unsigned int loop_reg = LIBXSMM_AARCH64_GP_REG_X28 ; unsigned int bound_reg = LIBXSMM_AARCH64_GP_REG_X27 ; unsigned int tmp_reg2 = LIBXSMM_AARCH64_GP_REG_X11 ; unsigned int tmp_reg3 = LIBXSMM_AARCH64_GP_REG_X26 ; libxsmm_aarch64_instruction_open_stream ( io_generated_code , 0xe0f ) ; libxsmm_generator_gemm_apply_ops_input_tensor_and_store_to_stack_aarch64 ( io_generated_code , io_loop_label_tracker , i_micro_kernel_config , i_xgemm_desc , i_gp_reg_mapping -> gp_reg_b , struct_gp_reg , tmp_reg , loop_reg , bound_reg , tmp_reg2 , tmp_reg3 , LIBXSMM_MELTW_TYPE_UNARY_TRANSFORM_NORM_TO_VNNI4T , i_xgemm_desc_orig -> k , i_xgemm_desc_orig -> n , i_xgemm_desc_orig -> ldb , i_xgemm_desc_orig -> n , LIBXSMM_CAST_BLASINT ( i_xgemm_desc_orig -> c2 ) , LIBXSMM_DATATYPE_BF16 , LIBXSMM_DATATYPE_BF16 , LIBXSMM_DATATYPE_BF16 , LIBXSMM_GEMM_STACK_VAR_B_OFFS_BRGEMM_PTR , LIBXSMM_GEMM_STACK_VAR_A_SCRATCH_PTR , LIBXSMM_GEMM_STACK_VAR_B_EMU_PTR , LIBXSMM_MELTW_TYPE_UNARY_NONE , 0 , 0 , 0 , 0 , ( libxsmm_datatype ) 0 , ( libxsmm_datatype ) 0 , ( libxsmm_datatype ) 0 ) ; libxsmm_generator_gemm_getval_stack_var_aarch64 ( io_generated_code , LIBXSMM_GEMM_STACK_VAR_B_EMU_PTR , tmp_reg ) ; libxsmm_aarch64_instruction_alu_compute_imm12 ( io_generated_code , LIBXSMM_AARCH64_INSTR_GP_ADD_I , tmp_reg , i_gp_reg_mapping -> gp_reg_b , 0 , 0 ) ; if ( ( is_offset_brgemm > 0 ) || ( is_address_brgemm > 0 ) ) { unsigned int a_vnni_factor = 1 ; int l_use_bfdot = libxsmm_cpuid_arm_use_bfdot ( ) ; char l_use_mmla = 0 ; if ( ( LIBXSMM_DATATYPE_BF16 == LIBXSMM_GETENUM_INP ( i_xgemm_desc -> datatype ) ) || ( LIBXSMM_DATATYPE_I8 == LIBXSMM_GETENUM_INP ( i_xgemm_desc -> datatype ) ) ) { if ( l_use_bfdot == 0 ) { l_use_mmla = 1 ; } else { l_use_mmla = 0 ; } if ( LIBXSMM_DATATYPE_BF16 == LIBXSMM_GETENUM_INP ( i_xgemm_desc -> datatype ) ) { a_vnni_factor = ( l_use_mmla == 0 ) ?2 : 4 ; } if ( LIBXSMM_DATATYPE_I8 == LIBXSMM_GETENUM_INP ( i_xgemm_desc -> datatype ) ) { a_vnni_factor = ( l_use_mmla == 0 ) ?4 : 8 ; } } libxsmm_generator_gemm_apply_ops_input_tensor_and_store_to_stack_aarch64 ( io_generated_code , io_loop_label_tracker , i_micro_kernel_config , i_xgemm_desc , i_gp_reg_mapping -> gp_reg_a , struct_gp_reg , tmp_reg , loop_reg , bound_reg , tmp_reg2 , tmp_reg3 , LIBXSMM_MELTW_TYPE_UNARY_IDENTITY , i_xgemm_desc_orig -> m * a_vnni_factor , i_xgemm_desc_orig -> k / a_vnni_factor , i_xgemm_desc_orig -> lda * a_vnni_factor , i_xgemm_desc_orig -> m * a_vnni_factor , LIBXSMM_CAST_BLASINT ( i_xgemm_desc_orig -> c1 ) , i_in_dtype , i_in_dtype , i_in_dtype , LIBXSMM_GEMM_STACK_VAR_A_OFFS_BRGEMM_PTR , LIBXSMM_GEMM_STACK_VAR_A_SCRATCH_PTR , LIBXSMM_GEMM_STACK_VAR_A_EMU_PTR , LIBXSMM_MELTW_TYPE_UNARY_NONE , 0 , 0 , 0 , 0 , ( libxsmm_datatype ) 0 , ( libxsmm_datatype ) 0 , ( libxsmm_datatype ) 0 ) ; libxsmm_generator_gemm_getval_stack_var_aarch64 ( io_generated_code , LIBXSMM_GEMM_STACK_VAR_A_EMU_PTR , tmp_reg ) ; libxsmm_aarch64_instruction_alu_compute_imm12 ( io_generated_code , LIBXSMM_AARCH64_INSTR_GP_ADD_I , tmp_reg , i_gp_reg_mapping -> gp_reg_a , 0 , 0 ) ; } if ( is_brgemm > 0 ) { if ( is_offset_brgemm > 0 ) { i_xgemm_desc -> flags = i_xgemm_desc -> flags ^ LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET ; i_xgemm_desc -> flags = i_xgemm_desc -> flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE ; i_xgemm_desc -> c1 = ( long long ) LIBXSMM_TYPESIZE ( i_in_dtype ) * i_xgemm_desc -> m * i_xgemm_desc -> k ; i_xgemm_desc -> lda = i_xgemm_desc -> m ; } i_xgemm_desc -> c2 = ( long long ) LIBXSMM_TYPESIZE ( i_in_dtype ) * i_xgemm_desc -> n * i_xgemm_desc -> k ; } i_xgemm_desc -> flags = i_xgemm_desc -> flags | LIBXSMM_GEMM_FLAG_VNNI_B | LIBXSMM_GEMM_FLAG_TRANS_B ; i_xgemm_desc -> ldb = i_xgemm_desc -> n ; libxsmm_aarch64_instruction_restore_regs ( io_generated_code , 0xe0f ) ; } 
void efx_fast_push_rx_descriptors ( struct efx_rx_queue * rx_queue , bool atomic ) { struct efx_nic * efx = rx_queue -> efx ; unsigned int fill_level , batch_size ; int space , rc = 0 ; if ( ! rx_queue -> refill_enabled ) { return ; } fill_level = ( rx_queue -> added_count - rx_queue -> removed_count ) ; EFX_WARN_ON_ONCE_PARANOID ( fill_level > rx_queue -> efx -> rxq_entries ) ; if ( fill_level >= rx_queue -> fast_fill_trigger ) { out } if ( unlikely ( fill_level < rx_queue -> min_fill ) ) { if ( fill_level ) { rx_queue -> min_fill = fill_level ; } } batch_size = efx -> rx_pages_per_batch * efx -> rx_bufs_per_page ; space = rx_queue -> max_fill - fill_level ; EFX_WARN_ON_ONCE_PARANOID ( space < batch_size ) ; netif_vdbg ( rx_queue -> efx , rx_status , rx_queue -> efx -> net_dev , "RX queue %d fast-filling descriptor ring from" " level %d to level %d\n" , efx_rx_queue_index ( rx_queue ) , fill_level , rx_queue -> max_fill ) ; { rc = efx_init_rx_buffers ( rx_queue , atomic ) ; if ( unlikely ( rc ) ) { if ( rx_queue -> added_count == rx_queue -> removed_count ) { efx_nic_notify_rx_desc ( 0 ) ; } out } } ( space -= batch_size ) >= batch_size ; netif_vdbg ( rx_queue -> efx , rx_status , rx_queue -> efx -> net_dev , "RX queue %d fast-filled descriptor ring " "to level %d\n" , efx_rx_queue_index ( rx_queue ) , rx_queue -> added_count - rx_queue -> removed_count ) ; out if ( rx_queue -> notified_count != rx_queue -> added_count ) { efx_nic_notify_rx_desc ( rx_queue ) ; } } 
static bool v7m_stack_write ( ARMCPU * cpu , uint32_t addr , uint32_t value , ARMMMUIdx mmu_idx , StackingMode mode ) { CPUState * cs = CPU ( cpu ) ; CPUARMState * env = & cpu -> env ; MemTxAttrs attrs = { } ; MemTxResult txres ; target_ulong page_size ; hwaddr physaddr ; int prot ; ARMMMUFaultInfo fi = { } ; ARMCacheAttrs cacheattrs = { } ; bool secure = mmu_idx & ARM_MMU_IDX_M_S ; int exc ; bool exc_secure ; if ( get_phys_addr ( env , addr , MMU_DATA_STORE , mmu_idx , & physaddr , & attrs , & prot , & page_size , & fi , & cacheattrs ) ) { if ( fi . type == ARMFault_QEMU_SFault ) { if ( mode == STACK_LAZYFP ) { qemu_log_mask ( CPU_LOG_INT , "...SecureFault with SFSR.LSPERR " "during lazy stacking\n" ) ; env -> v7m . sfsr |= R_V7M_SFSR_LSPERR_MASK ; } else { qemu_log_mask ( CPU_LOG_INT , "...SecureFault with SFSR.AUVIOL " "during stacking\n" ) ; env -> v7m . sfsr |= R_V7M_SFSR_AUVIOL_MASK ; } env -> v7m . sfsr |= R_V7M_SFSR_SFARVALID_MASK ; env -> v7m . sfar = addr ; exc = ARMV7M_EXCP_SECURE ; exc_secure = false ; } else { if ( mode == STACK_LAZYFP ) { qemu_log_mask ( CPU_LOG_INT , "...MemManageFault with CFSR.MLSPERR\n" ) ; env -> v7m . cfsr [ secure ] |= R_V7M_CFSR_MLSPERR_MASK ; } else { qemu_log_mask ( CPU_LOG_INT , "...MemManageFault with CFSR.MSTKERR\n" ) ; env -> v7m . cfsr [ secure ] |= R_V7M_CFSR_MSTKERR_MASK ; } exc = ARMV7M_EXCP_MEM ; exc_secure = secure ; } pend_fault } address_space_stl_le ( arm_addressspace ( cs , attrs ) , physaddr , value , attrs , & txres ) ; return true ; pend_fault switch ( mode ) { case STACK_NORMAL : armv7m_nvic_set_pending_derived ( env -> nvic , exc , exc_secure ) ; break ; case STACK_LAZYFP : armv7m_nvic_set_pending_lazyfp ( env -> nvic , exc , exc_secure ) ; break ; case STACK_IGNFAULTS : break ; } return false ; } 
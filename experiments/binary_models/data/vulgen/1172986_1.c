static netdev_tx_t cpsw_ndo_start_xmit ( struct sk_buff * skb , struct net_device * ndev ) { struct cpsw_priv * priv = netdev_priv ( ndev ) ; struct cpsw_common * cpsw = priv -> cpsw ; struct cpts * cpts = cpsw -> cpts ; struct netdev_queue * txq ; struct cpdma_chan * txch ; int ret , q_idx ; if ( skb_put_padto ( skb , CPSW_MIN_PACKET_SIZE ) ) { cpsw_err ( priv , tx_err , "packet pad failed\n" ) ; ndev -> stats . tx_dropped ++ ; return NET_XMIT_DROP ; } q_idx = skb_get_queue_mapping ( skb ) ; if ( q_idx >= cpsw -> tx_ch_num ) { q_idx = q_idx % cpsw -> tx_ch_num ; } txch = cpsw -> txv [ q_idx ] . ch ; txq = netdev_get_tx_queue ( ndev , q_idx ) ; skb_tx_timestamp ( skb ) ; ret = cpdma_chan_submit ( txch , skb , skb -> data , skb -> len , priv -> emac_port + cpsw -> data . dual_emac ) ; if ( unlikely ( ret != 0 ) ) { cpsw_err ( priv , tx_err , "desc submit failed\n" ) ; fail } if ( unlikely ( ! cpdma_check_free_tx_desc ( txch ) ) ) { netif_tx_stop_queue ( txq ) ; smp_mb__after_atomic ( ) ; if ( cpdma_check_free_tx_desc ( txch ) ) { netif_tx_wake_queue ( txq ) ; } } return NETDEV_TX_OK ; fail ndev -> stats . tx_dropped ++ ; netif_tx_stop_queue ( txq ) ; smp_mb__after_atomic ( ) ; if ( cpdma_check_free_tx_desc ( txch ) ) { netif_tx_wake_queue ( txq ) ; } return NETDEV_TX_BUSY ; } 
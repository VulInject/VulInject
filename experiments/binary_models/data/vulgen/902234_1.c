static void hevc_idct_8x32_column_lsx ( int16_t * coeffs , int32_t buf_pitch , uint8_t round ) { uint8_t i ; int32_t buf_pitch_2 = buf_pitch << 1 ; int32_t buf_pitch_4 = buf_pitch << 2 ; int32_t buf_pitch_8 = buf_pitch << 3 ; int32_t buf_pitch_16 = buf_pitch << 4 ; const int16_t * filter_ptr0 = & gt32x32_cnst0 [ 0 ] ; const int16_t * filter_ptr1 = & gt32x32_cnst1 [ 0 ] ; const int16_t * filter_ptr2 = & gt32x32_cnst2 [ 0 ] ; const int16_t * filter_ptr3 = & gt8x8_cnst [ 0 ] ; int16_t * src0 = ( coeffs + buf_pitch ) ; int16_t * src1 = ( coeffs + buf_pitch_2 ) ; int16_t * src2 = ( coeffs + buf_pitch_4 ) ; int16_t * src3 = ( coeffs ) ; int32_t tmp_buf [ 8 * 32 + 15 ] ; int32_t * tmp_buf_ptr = tmp_buf + 15 ; __m128i in0 , in1 , in2 , in3 , in4 , in5 , in6 , in7 ; __m128i src0_r , src1_r , src2_r , src3_r , src4_r , src5_r , src6_r , src7_r ; __m128i src0_l , src1_l , src2_l , src3_l , src4_l , src5_l , src6_l , src7_l ; __m128i filter0 , filter1 , filter2 , filter3 ; __m128i sum0_r , sum0_l , sum1_r , sum1_l , tmp0_r , tmp0_l , tmp1_r , tmp1_l ; tmp_buf_ptr = ( int32_t * ) ( ( ( uintptr_t ) tmp_buf_ptr ) & ~ ( uintptr_t ) 63 ) ; in0 = __lsx_vld ( src2 , 0 ) ; in1 = __lsx_vld ( src2 + buf_pitch_8 , 0 ) ; in2 = __lsx_vld ( src2 + buf_pitch_16 , 0 ) ; in3 = __lsx_vld ( src2 + buf_pitch_16 + buf_pitch_8 , 0 ) ; in4 = __lsx_vld ( src3 , 0 ) ; in5 = __lsx_vld ( src3 + buf_pitch_8 , 0 ) ; in6 = __lsx_vld ( src3 + buf_pitch_16 , 0 ) ; in7 = __lsx_vld ( src3 + buf_pitch_16 + buf_pitch_8 , 0 ) ; DUP4_ARG2 ( __lsx_vilvl_h , in1 , in0 , in3 , in2 , in6 , in4 , in7 , in5 , src0_r , src1_r , src2_r , src3_r ) ; DUP4_ARG2 ( __lsx_vilvh_h , in1 , in0 , in3 , in2 , in6 , in4 , in7 , in5 , src0_l , src1_l , src2_l , src3_l ) ; filter0 = __lsx_vldrepl_w ( filter_ptr2 , 0 ) ; filter1 = __lsx_vldrepl_w ( filter_ptr2 , 4 ) ; sum0_r = __lsx_vdp2_w_h ( src0_r , filter0 ) ; sum0_l = __lsx_vdp2_w_h ( src0_l , filter0 ) ; sum0_r = __lsx_vdp2add_w_h ( sum0_r , src1_r , filter1 ) ; sum0_l = __lsx_vdp2add_w_h ( sum0_l , src1_l , filter1 ) ; __lsx_vst ( sum0_r , tmp_buf_ptr , 0 ) ; __lsx_vst ( sum0_l , tmp_buf_ptr , 16 ) ; filter0 = __lsx_vldrepl_w ( filter_ptr2 , 8 ) ; filter1 = __lsx_vldrepl_w ( filter_ptr2 , 12 ) ; sum0_r = __lsx_vdp2_w_h ( src0_r , filter0 ) ; sum0_l = __lsx_vdp2_w_h ( src0_l , filter0 ) ; sum0_r = __lsx_vdp2add_w_h ( sum0_r , src1_r , filter1 ) ; sum0_l = __lsx_vdp2add_w_h ( sum0_l , src1_l , filter1 ) ; __lsx_vst ( sum0_r , tmp_buf_ptr , 32 ) ; __lsx_vst ( sum0_l , tmp_buf_ptr , 48 ) ; filter0 = __lsx_vldrepl_w ( filter_ptr2 , 16 ) ; filter1 = __lsx_vldrepl_w ( filter_ptr2 , 20 ) ; sum0_r = __lsx_vdp2_w_h ( src0_r , filter0 ) ; sum0_l = __lsx_vdp2_w_h ( src0_l , filter0 ) ; sum0_r = __lsx_vdp2add_w_h ( sum0_r , src1_r , filter1 ) ; sum0_l = __lsx_vdp2add_w_h ( sum0_l , src1_l , filter1 ) ; __lsx_vst ( sum0_r , tmp_buf_ptr , 64 ) ; __lsx_vst ( sum0_l , tmp_buf_ptr , 80 ) ; filter0 = __lsx_vldrepl_w ( filter_ptr2 , 24 ) ; filter1 = __lsx_vldrepl_w ( filter_ptr2 , 28 ) ; sum0_r = __lsx_vdp2_w_h ( src0_r , filter0 ) ; sum0_l = __lsx_vdp2_w_h ( src0_l , filter0 ) ; sum0_r = __lsx_vdp2add_w_h ( sum0_r , src1_r , filter1 ) ; sum0_l = __lsx_vdp2add_w_h ( sum0_l , src1_l , filter1 ) ; __lsx_vst ( sum0_r , tmp_buf_ptr , 96 ) ; __lsx_vst ( sum0_l , tmp_buf_ptr , 112 ) ; filter0 = __lsx_vldrepl_w ( filter_ptr3 , 0 ) ; filter1 = __lsx_vldrepl_w ( filter_ptr3 , 4 ) ; DUP4_ARG2 ( __lsx_vdp2_w_h , src2_r , filter0 , src2_l , filter0 , src3_r , filter1 , src3_l , filter1 , sum0_r , sum0_l , tmp1_r , tmp1_l ) ; sum1_r = __lsx_vsub_w ( sum0_r , tmp1_r ) ; sum1_l = __lsx_vsub_w ( sum0_l , tmp1_l ) ; sum0_r = __lsx_vadd_w ( sum0_r , tmp1_r ) ; sum0_l = __lsx_vadd_w ( sum0_l , tmp1_l ) ; HEVC_EVEN16_CALC ( tmp_buf_ptr , sum0_r , sum0_l , 0 , 7 ) ; HEVC_EVEN16_CALC ( tmp_buf_ptr , sum1_r , sum1_l , 3 , 4 ) ; filter0 = __lsx_vldrepl_w ( filter_ptr3 , 16 ) ; filter1 = __lsx_vldrepl_w ( filter_ptr3 , 20 ) ; DUP4_ARG2 ( __lsx_vdp2_w_h , src2_r , filter0 , src2_l , filter0 , src3_r , filter1 , src3_l , filter1 , sum0_r , sum0_l , tmp1_r , tmp1_l ) ; sum1_r = __lsx_vsub_w ( sum0_r , tmp1_r ) ; sum1_l = __lsx_vsub_w ( sum0_l , tmp1_l ) ; sum0_r = __lsx_vadd_w ( sum0_r , tmp1_r ) ; sum0_l = __lsx_vadd_w ( sum0_l , tmp1_l ) ; HEVC_EVEN16_CALC ( tmp_buf_ptr , sum0_r , sum0_l , 1 , 6 ) ; HEVC_EVEN16_CALC ( tmp_buf_ptr , sum1_r , sum1_l , 2 , 5 ) ; in0 = __lsx_vld ( src1 , 0 ) ; in1 = __lsx_vld ( src1 + buf_pitch_4 , 0 ) ; in2 = __lsx_vld ( src1 + buf_pitch_8 , 0 ) ; in3 = __lsx_vld ( src1 + buf_pitch_8 + buf_pitch_4 , 0 ) ; in4 = __lsx_vld ( src1 + buf_pitch_16 , 0 ) ; in5 = __lsx_vld ( src1 + buf_pitch_16 + buf_pitch_4 , 0 ) ; in6 = __lsx_vld ( src1 + buf_pitch_16 + buf_pitch_8 , 0 ) ; in7 = __lsx_vld ( src1 + buf_pitch_16 + buf_pitch_8 + buf_pitch_4 , 0 ) ; DUP4_ARG2 ( __lsx_vilvl_h , in1 , in0 , in3 , in2 , in5 , in4 , in7 , in6 , src0_r , src1_r , src2_r , src3_r ) ; DUP4_ARG2 ( __lsx_vilvh_h , in1 , in0 , in3 , in2 , in5 , in4 , in7 , in6 , src0_l , src1_l , src2_l , src3_l ) ; for ( i = 0 ; i < 8 ; i ++ ) { filter0 = __lsx_vldrepl_w ( filter_ptr1 , 0 ) ; filter1 = __lsx_vldrepl_w ( filter_ptr1 , 4 ) ; filter2 = __lsx_vldrepl_w ( filter_ptr1 , 8 ) ; filter3 = __lsx_vldrepl_w ( filter_ptr1 , 12 ) ; sum0_r = __lsx_vdp2_w_h ( src0_r , filter0 ) ; sum0_l = __lsx_vdp2_w_h ( src0_l , filter0 ) ; sum0_r = __lsx_vdp2add_w_h ( sum0_r , src1_r , filter1 ) ; sum0_l = __lsx_vdp2add_w_h ( sum0_l , src1_l , filter1 ) ; sum0_r = __lsx_vdp2add_w_h ( sum0_r , src2_r , filter2 ) ; sum0_l = __lsx_vdp2add_w_h ( sum0_l , src2_l , filter2 ) ; sum0_r = __lsx_vdp2add_w_h ( sum0_r , src3_r , filter3 ) ; sum0_l = __lsx_vdp2add_w_h ( sum0_l , src3_l , filter3 ) ; tmp0_r = __lsx_vld ( tmp_buf_ptr + ( i << 3 ) , 0 ) ; tmp0_l = __lsx_vld ( tmp_buf_ptr + ( i << 3 ) , 16 ) ; tmp1_r = tmp0_r ; tmp1_l = tmp0_l ; tmp0_r = __lsx_vadd_w ( tmp0_r , sum0_r ) ; tmp0_l = __lsx_vadd_w ( tmp0_l , sum0_l ) ; tmp1_r = __lsx_vsub_w ( tmp1_r , sum0_r ) ; tmp1_l = __lsx_vsub_w ( tmp1_l , sum0_l ) ; __lsx_vst ( tmp0_r , tmp_buf_ptr + ( i << 3 ) , 0 ) ; __lsx_vst ( tmp0_l , tmp_buf_ptr + ( i << 3 ) , 16 ) ; __lsx_vst ( tmp1_r , tmp_buf_ptr + ( ( 15 - i ) * 8 ) , 0 ) ; __lsx_vst ( tmp1_l , tmp_buf_ptr + ( ( 15 - i ) * 8 ) , 16 ) ; filter_ptr1 += 8 ; } in0 = __lsx_vld ( src0 , 0 ) ; in1 = __lsx_vld ( src0 + buf_pitch_2 , 0 ) ; in2 = __lsx_vld ( src0 + buf_pitch_4 , 0 ) ; in3 = __lsx_vld ( src0 + buf_pitch_4 + buf_pitch_2 , 0 ) ; in4 = __lsx_vld ( src0 + buf_pitch_8 , 0 ) ; in5 = __lsx_vld ( src0 + buf_pitch_8 + buf_pitch_2 , 0 ) ; in6 = __lsx_vld ( src0 + buf_pitch_8 + buf_pitch_4 , 0 ) ; in7 = __lsx_vld ( src0 + buf_pitch_8 + buf_pitch_4 + buf_pitch_2 , 0 ) ; src0 += 16 * buf_pitch ; DUP4_ARG2 ( __lsx_vilvl_h , in1 , in0 , in3 , in2 , in5 , in4 , in7 , in6 , src0_r , src1_r , src2_r , src3_r ) ; DUP4_ARG2 ( __lsx_vilvh_h , in1 , in0 , in3 , in2 , in5 , in4 , in7 , in6 , src0_l , src1_l , src2_l , src3_l ) ; in0 = __lsx_vld ( src0 , 0 ) ; in1 = __lsx_vld ( src0 + buf_pitch_2 , 0 ) ; in2 = __lsx_vld ( src0 + buf_pitch_4 , 0 ) ; in3 = __lsx_vld ( src0 + buf_pitch_4 + buf_pitch_2 , 0 ) ; in4 = __lsx_vld ( src0 + buf_pitch_8 , 0 ) ; in5 = __lsx_vld ( src0 + buf_pitch_8 + buf_pitch_2 , 0 ) ; in6 = __lsx_vld ( src0 + buf_pitch_8 + buf_pitch_4 , 0 ) ; in7 = __lsx_vld ( src0 + buf_pitch_8 + buf_pitch_4 + buf_pitch_2 , 0 ) ; DUP4_ARG2 ( __lsx_vilvl_h , in1 , in0 , in3 , in2 , in5 , in4 , in7 , in6 , src4_r , src5_r , src6_r , src7_r ) ; DUP4_ARG2 ( __lsx_vilvh_h , in1 , in0 , in3 , in2 , in5 , in4 , in7 , in6 , src4_l , src5_l , src6_l , src7_l ) ; for ( i = 0 ; i < 16 ; i ++ ) { filter0 = __lsx_vldrepl_w ( filter_ptr0 , 0 ) ; filter1 = __lsx_vldrepl_w ( filter_ptr0 , 4 ) ; filter2 = __lsx_vldrepl_w ( filter_ptr0 , 8 ) ; filter3 = __lsx_vldrepl_w ( filter_ptr0 , 12 ) ; sum0_r = __lsx_vdp2_w_h ( src0_r , filter0 ) ; sum0_l = __lsx_vdp2_w_h ( src0_l , filter0 ) ; sum0_r = __lsx_vdp2add_w_h ( sum0_r , src1_r , filter1 ) ; sum0_l = __lsx_vdp2add_w_h ( sum0_l , src1_l , filter1 ) ; sum0_r = __lsx_vdp2add_w_h ( sum0_r , src2_r , filter2 ) ; sum0_l = __lsx_vdp2add_w_h ( sum0_l , src2_l , filter2 ) ; sum0_r = __lsx_vdp2add_w_h ( sum0_r , src3_r , filter3 ) ; sum0_l = __lsx_vdp2add_w_h ( sum0_l , src3_l , filter3 ) ; tmp1_r = sum0_r ; tmp1_l = sum0_l ; filter0 = __lsx_vldrepl_w ( filter_ptr0 , 16 ) ; filter1 = __lsx_vldrepl_w ( filter_ptr0 , 20 ) ; filter2 = __lsx_vldrepl_w ( filter_ptr0 , 24 ) ; filter3 = __lsx_vldrepl_w ( filter_ptr0 , 28 ) ; sum0_r = __lsx_vdp2_w_h ( src4_r , filter0 ) ; sum0_l = __lsx_vdp2_w_h ( src4_l , filter0 ) ; sum0_r = __lsx_vdp2add_w_h ( sum0_r , src5_r , filter1 ) ; sum0_l = __lsx_vdp2add_w_h ( sum0_l , src5_l , filter1 ) ; sum0_r = __lsx_vdp2add_w_h ( sum0_r , src6_r , filter2 ) ; sum0_l = __lsx_vdp2add_w_h ( sum0_l , src6_l , filter2 ) ; sum0_r = __lsx_vdp2add_w_h ( sum0_r , src7_r , filter3 ) ; sum0_l = __lsx_vdp2add_w_h ( sum0_l , src7_l , filter3 ) ; sum0_r = __lsx_vadd_w ( sum0_r , tmp1_r ) ; sum0_l = __lsx_vadd_w ( sum0_l , tmp1_l ) ; tmp0_r = __lsx_vld ( tmp_buf_ptr + i * 8 , 0 ) ; tmp0_l = __lsx_vld ( tmp_buf_ptr + i * 8 , 16 ) ; tmp1_r = tmp0_r ; tmp1_l = tmp0_l ; tmp0_r = __lsx_vadd_w ( tmp0_r , sum0_r ) ; tmp0_l = __lsx_vadd_w ( tmp0_l , sum0_l ) ; sum1_r = __lsx_vreplgr2vr_w ( round ) ; tmp0_r = __lsx_vssrarn_h_w ( tmp0_r , sum1_r ) ; tmp0_l = __lsx_vssrarn_h_w ( tmp0_l , sum1_r ) ; in0 = __lsx_vpackev_d ( tmp0_l , tmp0_r ) ; __lsx_vst ( in0 , ( coeffs + i * buf_pitch ) , 0 ) ; tmp1_r = __lsx_vsub_w ( tmp1_r , sum0_r ) ; tmp1_l = __lsx_vsub_w ( tmp1_l , sum0_l ) ; tmp1_r = __lsx_vssrarn_h_w ( tmp1_r , sum1_r ) ; tmp1_l = __lsx_vssrarn_h_w ( tmp1_l , sum1_r ) ; in0 = __lsx_vpackev_d ( tmp1_l , tmp1_r ) ; filter_ptr0 += 16 ; } } 
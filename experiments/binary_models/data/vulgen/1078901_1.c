static void do_unoptimize_kprobes ( void ) { struct optimized_kprobe * op , * tmp ; lockdep_assert_cpus_held ( ) ; if ( ! list_empty ( & unoptimizing_list ) ) { arch_unoptimize_kprobes ( & unoptimizing_list , & freeing_list ) ; } list_for_each_entry_safe ( , , , ) { op -> kp . flags &= ~ KPROBE_FLAG_OPTIMIZED ; if ( kprobe_disabled ( & op -> kp ) && ! kprobe_gone ( & op -> kp ) ) { arch_disarm_kprobe ( & op -> kp ) ; } if ( kprobe_unused ( & op -> kp ) ) { hlist_del_rcu ( & op -> kp . hlist ) ; } else { list_del_init ( & op -> list ) ; } } } 
int zfs_setattr ( struct inode * ip , vattr_t * vap , int flags , cred_t * cr ) { znode_t * zp = ITOZ ( ip ) ; zfsvfs_t * zfsvfs = ITOZSB ( ip ) ; zilog_t * zilog ; dmu_tx_t * tx ; vattr_t oldva ; xvattr_t * tmpxvattr ; uint_t mask = vap -> va_mask ; uint_t saved_mask = 0 ; int trim_mask = 0 ; uint64_t new_mode ; uint64_t new_kuid = 0 , new_kgid = 0 , new_uid , new_gid ; uint64_t xattr_obj ; uint64_t mtime [ 2 ] , ctime [ 2 ] , atime [ 2 ] ; znode_t * attrzp ; int need_policy = FALSE ; int err , err2 ; zfs_fuid_info_t * fuidp = NULL ; xvattr_t * xvap = ( xvattr_t * ) vap ; xoptattr_t * xoap ; zfs_acl_t * aclp ; boolean_t skipaclchk = ( flags & ATTR_NOACLCHECK ) ?B_TRUE : B_FALSE ; boolean_t fuid_dirtied = B_FALSE ; sa_bulk_attr_t * bulk , * xattr_bulk ; int count = 0 , xattr_count = 0 ; if ( mask == 0 ) { return ( 0 ) ; } ZFS_ENTER ( zfsvfs ) ; ZFS_VERIFY_ZP ( zp ) ; zilog = zfsvfs -> z_log ; if ( zfsvfs -> z_use_fuids == B_FALSE && ( ( ( mask & ATTR_UID ) && IS_EPHEMERAL ( vap -> va_uid ) ) || ( ( mask & ATTR_GID ) && IS_EPHEMERAL ( vap -> va_gid ) ) || ( mask & ATTR_XVATTR ) ) ) { ZFS_EXIT ( zfsvfs ) ; return ( SET_ERROR ( EINVAL ) ) ; } if ( mask & ATTR_SIZE && S_ISDIR ( ip -> i_mode ) ) { ZFS_EXIT ( zfsvfs ) ; return ( SET_ERROR ( EISDIR ) ) ; } if ( mask & ATTR_SIZE && ! S_ISREG ( ip -> i_mode ) && ! S_ISFIFO ( ip -> i_mode ) ) { ZFS_EXIT ( zfsvfs ) ; return ( SET_ERROR ( EINVAL ) ) ; } xoap = xva_getxoptattr ( xvap ) ; tmpxvattr = kmem_alloc ( sizeof ( xvattr_t ) , KM_SLEEP ) ; xva_init ( tmpxvattr ) ; bulk = kmem_alloc ( sizeof ( sa_bulk_attr_t ) * 7 , KM_SLEEP ) ; xattr_bulk = kmem_alloc ( sizeof ( sa_bulk_attr_t ) * 7 , KM_SLEEP ) ; if ( ( zp -> z_pflags & ZFS_IMMUTABLE ) && ( ( mask & ( ATTR_SIZE | ATTR_UID | ATTR_GID | ATTR_MTIME | ATTR_MODE ) ) || ( ( mask & ATTR_XVATTR ) && XVA_ISSET_REQ ( xvap , XAT_CREATETIME ) ) ) ) { err = SET_ERROR ( EPERM ) ; out3 } if ( ( mask & ATTR_SIZE ) && ( zp -> z_pflags & ZFS_READONLY ) ) { err = SET_ERROR ( EPERM ) ; out3 } if ( mask & ( ATTR_ATIME | ATTR_MTIME ) ) { if ( ( ( mask & ATTR_ATIME ) && TIMESPEC_OVERFLOW ( & vap -> va_atime ) ) || ( ( mask & ATTR_MTIME ) && TIMESPEC_OVERFLOW ( & vap -> va_mtime ) ) ) { err = SET_ERROR ( EOVERFLOW ) ; out3 } } top attrzp = NULL ; aclp = NULL ; if ( zfs_is_readonly ( zfsvfs ) ) { err = SET_ERROR ( EROFS ) ; out3 } if ( mask & ATTR_SIZE ) { err = zfs_zaccess ( zp , ACE_WRITE_DATA , 0 , skipaclchk , cr ) ; if ( err ) { out3 } err = zfs_freesp ( zp , vap -> va_size , 0 , 0 , FALSE ) ; if ( err ) { out3 } } if ( mask & ( ATTR_ATIME | ATTR_MTIME ) || ( ( mask & ATTR_XVATTR ) && ( XVA_ISSET_REQ ( xvap , XAT_HIDDEN ) || XVA_ISSET_REQ ( xvap , XAT_READONLY ) || XVA_ISSET_REQ ( xvap , XAT_ARCHIVE ) || XVA_ISSET_REQ ( xvap , XAT_OFFLINE ) || XVA_ISSET_REQ ( xvap , XAT_SPARSE ) || XVA_ISSET_REQ ( xvap , XAT_CREATETIME ) || XVA_ISSET_REQ ( xvap , XAT_SYSTEM ) ) ) ) { need_policy = zfs_zaccess ( zp , ACE_WRITE_ATTRIBUTES , 0 , skipaclchk , cr ) ; } if ( mask & ( ATTR_UID | ATTR_GID ) ) { int idmask = ( mask & ( ATTR_UID | ATTR_GID ) ) ; int take_owner ; int take_group ; if ( ! ( mask & ATTR_MODE ) ) { vap -> va_mode = zp -> z_mode ; } take_owner = ( mask & ATTR_UID ) && ( vap -> va_uid == crgetuid ( cr ) ) ; take_group = ( mask & ATTR_GID ) && zfs_groupmember ( zfsvfs , vap -> va_gid , cr ) ; if ( ( ( idmask == ( ATTR_UID | ATTR_GID ) ) && take_owner && take_group ) || ( ( idmask == ATTR_UID ) && take_owner ) || ( ( idmask == ATTR_GID ) && take_group ) ) { if ( zfs_zaccess ( zp , ACE_WRITE_OWNER , 0 , skipaclchk , cr ) == 0 ) { ( void ) secpolicy_setid_clear ( vap , cr ) ; trim_mask = ( mask & ( ATTR_UID | ATTR_GID ) ) ; } else { need_policy = TRUE ; } } else { need_policy = TRUE ; } } mutex_enter ( & zp -> z_lock ) ; oldva . va_mode = zp -> z_mode ; zfs_fuid_map_ids ( zp , cr , & oldva . va_uid , & oldva . va_gid ) ; if ( mask & ATTR_XVATTR ) { if ( XVA_ISSET_REQ ( xvap , XAT_APPENDONLY ) ) { if ( xoap -> xoa_appendonly != ( ( zp -> z_pflags & ZFS_APPENDONLY ) != 0 ) ) { need_policy = TRUE ; } else { XVA_CLR_REQ ( xvap , XAT_APPENDONLY ) ; XVA_SET_REQ ( tmpxvattr , XAT_APPENDONLY ) ; } } if ( XVA_ISSET_REQ ( xvap , XAT_NOUNLINK ) ) { if ( xoap -> xoa_nounlink != ( ( zp -> z_pflags & ZFS_NOUNLINK ) != 0 ) ) { need_policy = TRUE ; } else { XVA_CLR_REQ ( xvap , XAT_NOUNLINK ) ; XVA_SET_REQ ( tmpxvattr , XAT_NOUNLINK ) ; } } if ( XVA_ISSET_REQ ( xvap , XAT_IMMUTABLE ) ) { if ( xoap -> xoa_immutable != ( ( zp -> z_pflags & ZFS_IMMUTABLE ) != 0 ) ) { need_policy = TRUE ; } else { XVA_CLR_REQ ( xvap , XAT_IMMUTABLE ) ; XVA_SET_REQ ( tmpxvattr , XAT_IMMUTABLE ) ; } } if ( XVA_ISSET_REQ ( xvap , XAT_NODUMP ) ) { if ( xoap -> xoa_nodump != ( ( zp -> z_pflags & ZFS_NODUMP ) != 0 ) ) { need_policy = TRUE ; } else { XVA_CLR_REQ ( xvap , XAT_NODUMP ) ; XVA_SET_REQ ( tmpxvattr , XAT_NODUMP ) ; } } if ( XVA_ISSET_REQ ( xvap , XAT_AV_MODIFIED ) ) { if ( xoap -> xoa_av_modified != ( ( zp -> z_pflags & ZFS_AV_MODIFIED ) != 0 ) ) { need_policy = TRUE ; } else { XVA_CLR_REQ ( xvap , XAT_AV_MODIFIED ) ; XVA_SET_REQ ( tmpxvattr , XAT_AV_MODIFIED ) ; } } if ( XVA_ISSET_REQ ( xvap , XAT_AV_QUARANTINED ) ) { if ( ( ! S_ISREG ( ip -> i_mode ) && xoap -> xoa_av_quarantined ) || xoap -> xoa_av_quarantined != ( ( zp -> z_pflags & ZFS_AV_QUARANTINED ) != 0 ) ) { need_policy = TRUE ; } else { XVA_CLR_REQ ( xvap , XAT_AV_QUARANTINED ) ; XVA_SET_REQ ( tmpxvattr , XAT_AV_QUARANTINED ) ; } } if ( XVA_ISSET_REQ ( xvap , XAT_REPARSE ) ) { mutex_exit ( & zp -> z_lock ) ; err = SET_ERROR ( EPERM ) ; out3 } if ( need_policy == FALSE && ( XVA_ISSET_REQ ( xvap , XAT_AV_SCANSTAMP ) || XVA_ISSET_REQ ( xvap , XAT_OPAQUE ) ) ) { need_policy = TRUE ; } } mutex_exit ( & zp -> z_lock ) ; if ( mask & ATTR_MODE ) { if ( zfs_zaccess ( zp , ACE_WRITE_ACL , 0 , skipaclchk , cr ) == 0 ) { err = secpolicy_setid_setsticky_clear ( ip , vap , & oldva , cr ) ; if ( err ) { out3 } trim_mask |= ATTR_MODE ; } else { need_policy = TRUE ; } } if ( need_policy ) { if ( trim_mask ) { saved_mask = vap -> va_mask ; vap -> va_mask &= ~ trim_mask ; } err = secpolicy_vnode_setattr ( cr , ip , vap , & oldva , flags , ( int ( * ) ( void * , int , cred_t * ) ) zfs_zaccess_unix , zp ) ; if ( err ) { out3 } if ( trim_mask ) { vap -> va_mask |= saved_mask ; } } mask = vap -> va_mask ; if ( ( mask & ( ATTR_UID | ATTR_GID ) ) ) { err = sa_lookup ( zp -> z_sa_hdl , SA_ZPL_XATTR ( zfsvfs ) , & xattr_obj , sizeof ( xattr_obj ) ) ; if ( err == 0 && xattr_obj ) { err = zfs_zget ( ZTOZSB ( zp ) , xattr_obj , & attrzp ) ; if ( err ) { out2 } } if ( mask & ATTR_UID ) { new_kuid = zfs_fuid_create ( zfsvfs , ( uint64_t ) vap -> va_uid , cr , ZFS_OWNER , & fuidp ) ; if ( new_kuid != KUID_TO_SUID ( ZTOI ( zp ) -> i_uid ) && zfs_fuid_overquota ( zfsvfs , B_FALSE , new_kuid ) ) { if ( attrzp ) { iput ( ZTOI ( attrzp ) ) ; } err = SET_ERROR ( EDQUOT ) ; out2 } } if ( mask & ATTR_GID ) { new_kgid = zfs_fuid_create ( zfsvfs , ( uint64_t ) vap -> va_gid , cr , ZFS_GROUP , & fuidp ) ; if ( new_kgid != KGID_TO_SGID ( ZTOI ( zp ) -> i_gid ) && zfs_fuid_overquota ( zfsvfs , B_TRUE , new_kgid ) ) { if ( attrzp ) { iput ( ZTOI ( attrzp ) ) ; } err = SET_ERROR ( EDQUOT ) ; out2 } } } tx = dmu_tx_create ( zfsvfs -> z_os ) ; if ( mask & ATTR_MODE ) { uint64_t pmode = zp -> z_mode ; uint64_t acl_obj ; new_mode = ( pmode & S_IFMT ) | ( vap -> va_mode & ~ S_IFMT ) ; zfs_acl_chmod_setattr ( zp , & aclp , new_mode ) ; mutex_enter ( & zp -> z_lock ) ; if ( ! zp -> z_is_sa && ( ( acl_obj = zfs_external_acl ( zp ) ) != 0 ) ) { if ( zfsvfs -> z_version >= ZPL_VERSION_FUID && zfs_znode_acl_version ( zp ) == ZFS_ACL_VERSION_INITIAL ) { dmu_tx_hold_free ( tx , acl_obj , 0 , DMU_OBJECT_END ) ; dmu_tx_hold_write ( tx , DMU_NEW_OBJECT , 0 , aclp -> z_acl_bytes ) ; } else { dmu_tx_hold_write ( tx , acl_obj , 0 , aclp -> z_acl_bytes ) ; } } if ( ! zp -> z_is_sa && aclp -> z_acl_bytes > ZFS_ACE_SPACE ) { dmu_tx_hold_write ( tx , DMU_NEW_OBJECT , 0 , aclp -> z_acl_bytes ) ; } mutex_exit ( & zp -> z_lock ) ; dmu_tx_hold_sa ( tx , zp -> z_sa_hdl , B_TRUE ) ; } else { if ( ( mask & ATTR_XVATTR ) && XVA_ISSET_REQ ( xvap , XAT_AV_SCANSTAMP ) ) { dmu_tx_hold_sa ( tx , zp -> z_sa_hdl , B_TRUE ) ; } else { dmu_tx_hold_sa ( tx , zp -> z_sa_hdl , B_FALSE ) ; } } if ( attrzp ) { dmu_tx_hold_sa ( tx , attrzp -> z_sa_hdl , B_FALSE ) ; } fuid_dirtied = zfsvfs -> z_fuid_dirty ; if ( fuid_dirtied ) { zfs_fuid_txhold ( zfsvfs , tx ) ; } zfs_sa_upgrade_txholds ( tx , zp ) ; err = dmu_tx_assign ( tx , TXG_WAIT ) ; if ( err ) { out } count = 0 ; if ( mask & ( ATTR_UID | ATTR_GID | ATTR_MODE ) ) { mutex_enter ( & zp -> z_acl_lock ) ; } mutex_enter ( & zp -> z_lock ) ; SA_ADD_BULK_ATTR ( bulk , count , SA_ZPL_FLAGS ( zfsvfs ) , NULL , & zp -> z_pflags , sizeof ( zp -> z_pflags ) ) ; if ( attrzp ) { if ( mask & ( ATTR_UID | ATTR_GID | ATTR_MODE ) ) { mutex_enter ( & attrzp -> z_acl_lock ) ; } mutex_enter ( & attrzp -> z_lock ) ; SA_ADD_BULK_ATTR ( xattr_bulk , xattr_count , SA_ZPL_FLAGS ( zfsvfs ) , NULL , & attrzp -> z_pflags , sizeof ( attrzp -> z_pflags ) ) ; } if ( mask & ( ATTR_UID | ATTR_GID ) ) { if ( mask & ATTR_UID ) { ZTOI ( zp ) -> i_uid = SUID_TO_KUID ( new_kuid ) ; new_uid = zfs_uid_read ( ZTOI ( zp ) ) ; SA_ADD_BULK_ATTR ( bulk , count , SA_ZPL_UID ( zfsvfs ) , NULL , & new_uid , sizeof ( new_uid ) ) ; if ( attrzp ) { SA_ADD_BULK_ATTR ( xattr_bulk , xattr_count , SA_ZPL_UID ( zfsvfs ) , NULL , & new_uid , sizeof ( new_uid ) ) ; ZTOI ( attrzp ) -> i_uid = SUID_TO_KUID ( new_uid ) ; } } if ( mask & ATTR_GID ) { ZTOI ( zp ) -> i_gid = SGID_TO_KGID ( new_kgid ) ; new_gid = zfs_gid_read ( ZTOI ( zp ) ) ; SA_ADD_BULK_ATTR ( bulk , count , SA_ZPL_GID ( zfsvfs ) , NULL , & new_gid , sizeof ( new_gid ) ) ; if ( attrzp ) { SA_ADD_BULK_ATTR ( xattr_bulk , xattr_count , SA_ZPL_GID ( zfsvfs ) , NULL , & new_gid , sizeof ( new_gid ) ) ; ZTOI ( attrzp ) -> i_gid = SGID_TO_KGID ( new_kgid ) ; } } if ( ! ( mask & ATTR_MODE ) ) { SA_ADD_BULK_ATTR ( bulk , count , SA_ZPL_MODE ( zfsvfs ) , NULL , & new_mode , sizeof ( new_mode ) ) ; new_mode = zp -> z_mode ; } err = zfs_acl_chown_setattr ( zp ) ; ASSERT ( err == 0 ) ; if ( attrzp ) { err = zfs_acl_chown_setattr ( attrzp ) ; ASSERT ( err == 0 ) ; } } if ( mask & ATTR_MODE ) { SA_ADD_BULK_ATTR ( bulk , count , SA_ZPL_MODE ( zfsvfs ) , NULL , & new_mode , sizeof ( new_mode ) ) ; zp -> z_mode = ZTOI ( zp ) -> i_mode = new_mode ; ASSERT3P ( aclp , != , NULL ) ; err = zfs_aclset_common ( zp , aclp , cr , tx ) ; ASSERT0 ( err ) ; if ( zp -> z_acl_cached ) { zfs_acl_free ( zp -> z_acl_cached ) ; } zp -> z_acl_cached = aclp ; aclp = NULL ; } if ( ( mask & ATTR_ATIME ) || zp -> z_atime_dirty ) { zp -> z_atime_dirty = 0 ; ZFS_TIME_ENCODE ( & ip -> i_atime , atime ) ; SA_ADD_BULK_ATTR ( bulk , count , SA_ZPL_ATIME ( zfsvfs ) , NULL , & atime , sizeof ( atime ) ) ; } if ( mask & ATTR_MTIME ) { ZFS_TIME_ENCODE ( & vap -> va_mtime , mtime ) ; ZTOI ( zp ) -> i_mtime = timespec_trunc ( vap -> va_mtime , ZTOI ( zp ) -> i_sb -> s_time_gran ) ; SA_ADD_BULK_ATTR ( bulk , count , SA_ZPL_MTIME ( zfsvfs ) , NULL , mtime , sizeof ( mtime ) ) ; } if ( mask & ATTR_CTIME ) { ZFS_TIME_ENCODE ( & vap -> va_ctime , ctime ) ; ZTOI ( zp ) -> i_ctime = timespec_trunc ( vap -> va_ctime , ZTOI ( zp ) -> i_sb -> s_time_gran ) ; SA_ADD_BULK_ATTR ( bulk , count , SA_ZPL_CTIME ( zfsvfs ) , NULL , ctime , sizeof ( ctime ) ) ; } if ( attrzp && mask ) { SA_ADD_BULK_ATTR ( xattr_bulk , xattr_count , SA_ZPL_CTIME ( zfsvfs ) , NULL , & ctime , sizeof ( ctime ) ) ; } if ( xoap && ( mask & ATTR_XVATTR ) ) { if ( XVA_ISSET_REQ ( tmpxvattr , XAT_APPENDONLY ) ) { XVA_SET_REQ ( xvap , XAT_APPENDONLY ) ; } if ( XVA_ISSET_REQ ( tmpxvattr , XAT_NOUNLINK ) ) { XVA_SET_REQ ( xvap , XAT_NOUNLINK ) ; } if ( XVA_ISSET_REQ ( tmpxvattr , XAT_IMMUTABLE ) ) { XVA_SET_REQ ( xvap , XAT_IMMUTABLE ) ; } if ( XVA_ISSET_REQ ( tmpxvattr , XAT_NODUMP ) ) { XVA_SET_REQ ( xvap , XAT_NODUMP ) ; } if ( XVA_ISSET_REQ ( tmpxvattr , XAT_AV_MODIFIED ) ) { XVA_SET_REQ ( xvap , XAT_AV_MODIFIED ) ; } if ( XVA_ISSET_REQ ( tmpxvattr , XAT_AV_QUARANTINED ) ) { XVA_SET_REQ ( xvap , XAT_AV_QUARANTINED ) ; } if ( XVA_ISSET_REQ ( xvap , XAT_AV_SCANSTAMP ) ) { ASSERT ( S_ISREG ( ip -> i_mode ) ) ; } zfs_xvattr_set ( zp , xvap , tx ) ; } if ( fuid_dirtied ) { zfs_fuid_sync ( zfsvfs , tx ) ; } if ( mask != 0 ) { zfs_log_setattr ( zilog , tx , TX_SETATTR , zp , vap , mask , fuidp ) ; } mutex_exit ( & zp -> z_lock ) ; if ( mask & ( ATTR_UID | ATTR_GID | ATTR_MODE ) ) { mutex_exit ( & zp -> z_acl_lock ) ; } if ( attrzp ) { if ( mask & ( ATTR_UID | ATTR_GID | ATTR_MODE ) ) { mutex_exit ( & attrzp -> z_acl_lock ) ; } mutex_exit ( & attrzp -> z_lock ) ; } out if ( err == 0 && attrzp ) { err2 = sa_bulk_update ( attrzp -> z_sa_hdl , xattr_bulk , xattr_count , tx ) ; ASSERT ( err2 == 0 ) ; } if ( aclp ) { zfs_acl_free ( aclp ) ; } if ( fuidp ) { zfs_fuid_info_free ( fuidp ) ; fuidp = NULL ; } if ( err ) { dmu_tx_abort ( tx ) ; if ( attrzp ) { iput ( ZTOI ( attrzp ) ) ; } if ( err == ERESTART ) { top } } else { err2 = sa_bulk_update ( zp -> z_sa_hdl , bulk , count , tx ) ; dmu_tx_commit ( tx ) ; if ( attrzp ) { iput ( ZTOI ( attrzp ) ) ; } zfs_inode_update ( zp ) ; } out2 out3 kmem_free ( xattr_bulk , sizeof ( sa_bulk_attr_t ) * 7 ) ; kmem_free ( bulk , sizeof ( sa_bulk_attr_t ) * 7 ) ; kmem_free ( tmpxvattr , sizeof ( xvattr_t ) ) ; ZFS_EXIT ( zfsvfs ) ; return ( err ) ; } 
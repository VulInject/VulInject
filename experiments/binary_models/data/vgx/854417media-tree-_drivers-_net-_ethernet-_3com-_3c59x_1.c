static netdev_tx_t boomerang_start_xmit ( struct sk_buff * skb , struct net_device * dev ) { struct vortex_private * vp = netdev_priv ( dev ) ; void __iomem * ioaddr = vp -> ioaddr ; int entry = vp -> cur_tx % TX_RING_SIZE ; int skblen = skb -> len ; struct boom_tx_desc * prev_entry = & vp -> tx_ring [ ( vp -> cur_tx - 1 ) % TX_RING_SIZE ] ; unsigned long flags ; dma_addr_t dma_addr ; if ( vortex_debug > 6 ) { pr_debug ( "boomerang_start_xmit()\n" ) ; pr_debug ( "%s: Trying to send a packet, Tx index %d.\n" , dev -> name , vp -> cur_tx ) ; } if ( vp -> cur_tx - vp -> dirty_tx >= TX_RING_SIZE ) { if ( vortex_debug > 0 ) { pr_warn ( "%s: BUG! Tx Ring full, refusing to send buffer\n" , dev -> name ) ; } netif_stop_queue ( dev ) ; return NETDEV_TX_BUSY ; } vp -> tx_skbuff [ entry ] = skb ; vp -> tx_ring [ entry ] . next = 0 ; if ( skb -> ip_summed != CHECKSUM_PARTIAL ) { vp -> tx_ring [ entry ] . status = cpu_to_le32 ( skb -> len | TxIntrUploaded ) ; } else { vp -> tx_ring [ entry ] . status = cpu_to_le32 ( skb -> len | TxIntrUploaded | AddTCPChksum | AddUDPChksum ) ; } if ( ! skb_shinfo ( skb ) -> nr_frags ) { dma_addr = pci_map_single ( VORTEX_PCI ( vp ) , skb -> data , skb -> len , PCI_DMA_TODEVICE ) ; if ( dma_mapping_error ( & VORTEX_PCI ( vp ) -> dev , dma_addr ) ) { out_dma_err } vp -> tx_ring [ entry ] . frag [ 0 ] . addr = cpu_to_le32 ( dma_addr ) ; vp -> tx_ring [ entry ] . frag [ 0 ] . length = cpu_to_le32 ( skb -> len | LAST_FRAG ) ; } else { int i ; dma_addr = pci_map_single ( VORTEX_PCI ( vp ) , skb -> data , skb_headlen ( skb ) , PCI_DMA_TODEVICE ) ; if ( dma_mapping_error ( & VORTEX_PCI ( vp ) -> dev , dma_addr ) ) { out_dma_err } vp -> tx_ring [ entry ] . frag [ 0 ] . addr = cpu_to_le32 ( dma_addr ) ; vp -> tx_ring [ entry ] . frag [ 0 ] . length = cpu_to_le32 ( skb_headlen ( skb ) ) ; for ( i = 0 ; i < skb_shinfo ( skb ) -> nr_frags ; i ++ ) { skb_frag_t * frag = & skb_shinfo ( skb ) -> frags [ i ] ; dma_addr = skb_frag_dma_map ( & VORTEX_PCI ( vp ) -> dev , frag , 0 , frag -> size , DMA_TO_DEVICE ) ; if ( dma_mapping_error ( & VORTEX_PCI ( vp ) -> dev , dma_addr ) ) { for ( i = i - 1 ; i >= 0 ; i -- ) { dma_unmap_page ( & VORTEX_PCI ( vp ) -> dev , le32_to_cpu ( vp -> tx_ring [ entry ] . frag [ i + 1 ] . addr ) , le32_to_cpu ( vp -> tx_ring [ entry ] . frag [ i + 1 ] . length ) , DMA_TO_DEVICE ) ; } pci_unmap_single ( VORTEX_PCI ( vp ) , le32_to_cpu ( vp -> tx_ring [ entry ] . frag [ 0 ] . addr ) , le32_to_cpu ( vp -> tx_ring [ entry ] . frag [ 0 ] . length ) , PCI_DMA_TODEVICE ) ; out_dma_err } vp -> tx_ring [ entry ] . frag [ i + 1 ] . addr = cpu_to_le32 ( dma_addr ) ; if ( i == skb_shinfo ( skb ) -> nr_frags - 1 ) { vp -> tx_ring [ entry ] . frag [ i + 1 ] . length = cpu_to_le32 ( skb_frag_size ( frag ) | LAST_FRAG ) ; } else { vp -> tx_ring [ entry ] . frag [ i + 1 ] . length = cpu_to_le32 ( skb_frag_size ( frag ) ) ; } } } dma_addr = pci_map_single ( VORTEX_PCI ( vp ) , skb -> data , skb -> len , PCI_DMA_TODEVICE ) ; if ( dma_mapping_error ( & VORTEX_PCI ( vp ) -> dev , dma_addr ) ) { out_dma_err } vp -> tx_ring [ entry ] . addr = cpu_to_le32 ( dma_addr ) ; vp -> tx_ring [ entry ] . length = cpu_to_le32 ( skb -> len | LAST_FRAG ) ; vp -> tx_ring [ entry ] . status = cpu_to_le32 ( skb -> len | TxIntrUploaded ) ; spin_lock_irqsave ( & vp -> lock , flags ) ; issue_and_wait ( dev , DownStall ) ; prev_entry -> next = cpu_to_le32 ( vp -> tx_ring_dma + entry * sizeof ( boom_tx_desc ) ) ; if ( ioread32 ( ioaddr + DownListPtr ) == 0 ) { iowrite32 ( vp -> tx_ring_dma + entry * sizeof ( boom_tx_desc ) , ioaddr + DownListPtr ) ; vp -> queued_packet ++ ; } vp -> cur_tx ++ ; netdev_sent_queue ( dev , skblen ) ; if ( vp -> cur_tx - vp -> dirty_tx > TX_RING_SIZE - 1 ) { netif_stop_queue ( dev ) ; } else { prev_entry -> status &= cpu_to_le32 ( ~ TxIntrUploaded ) ; } skb_tx_timestamp ( skb ) ; iowrite16 ( DownUnstall , ioaddr + EL3_CMD ) ; spin_unlock_irqrestore ( & vp -> lock , flags ) ; out return NETDEV_TX_OK ; out_dma_err dev_err ( & VORTEX_PCI ( vp ) -> dev , "Error mapping dma buffer\n" ) ; out } 
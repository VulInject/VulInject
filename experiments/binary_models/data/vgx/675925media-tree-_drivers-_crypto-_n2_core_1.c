static int n2_compute_chunks ( struct ablkcipher_request * req ) { struct n2_request_context * rctx = ablkcipher_request_ctx ( req ) ; struct ablkcipher_walk * walk = & rctx -> walk ; struct n2_crypto_chunk * chunk ; unsigned long dest_prev ; unsigned int tot_len ; bool prev_in_place ; int err , nbytes ; ablkcipher_walk_init ( walk , req -> dst , req -> src , req -> nbytes ) ; err = ablkcipher_walk_phys ( req , walk ) ; if ( err ) { return err ; } INIT_LIST_HEAD ( & rctx -> chunk_list ) ; chunk = & rctx -> chunk ; INIT_LIST_HEAD ( & chunk -> entry ) ; chunk -> iv_paddr = 0UL ; chunk -> dest_paddr = 0UL ; prev_in_place = false ; dest_prev = ~ 0UL ; tot_len = 0 ; while ( ( nbytes = walk -> nbytes ) != 0 ) { unsigned long dest_paddr , src_paddr ; bool in_place ; int this_len ; src_paddr = ( page_to_phys ( walk -> src . page ) + walk -> src . offset ) ; dest_paddr = ( page_to_phys ( walk -> dst . page ) + walk -> dst . offset ) ; in_place = ( src_paddr == dest_paddr ) ; this_len = cipher_descriptor_len ( nbytes , walk -> blocksize ) ; if ( chunk -> arr_len != 0 ) { if ( in_place != prev_in_place || ( ! prev_in_place && dest_paddr != dest_prev ) || chunk -> arr_len == N2_CHUNK_ARR_LEN || tot_len + this_len > ( 1 << 16 ) ) { chunk -> dest_final = dest_prev ; list_add_tail ( & chunk -> entry , & rctx -> chunk_list ) ; chunk = kzalloc ( sizeof ( * chunk ) , GFP_ATOMIC ) ; if ( ! chunk ) { err = - ENOMEM ; break ; } INIT_LIST_HEAD ( & chunk -> entry ) ; } } if ( chunk -> arr_len == 0 ) { chunk -> dest_paddr = dest_paddr ; tot_len = 0 ; } chunk -> arr [ chunk -> arr_len ] . src_paddr = src_paddr ; chunk -> arr [ chunk -> arr_len ] . src_len = this_len ; chunk -> arr_len ++ ; dest_prev = dest_paddr + this_len ; prev_in_place = in_place ; tot_len += this_len ; err = ablkcipher_walk_done ( req , walk , nbytes - this_len ) ; if ( err ) { break ; } } if ( ! err && chunk -> arr_len != 0 ) { chunk -> dest_final = dest_prev ; list_add_tail ( & chunk -> entry , & rctx -> chunk_list ) ; } return err ; } 
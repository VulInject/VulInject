static int seqiv_aead_encrypt ( struct aead_request * req ) { struct crypto_aead * geniv = crypto_aead_reqtfm ( req ) ; struct aead_geniv_ctx * ctx = crypto_aead_ctx ( geniv ) ; struct aead_request * subreq = aead_request_ctx ( req ) ; crypto_completion_t compl ; void * data ; u8 * info ; unsigned int ivsize = 8 ; int err ; aead_request_set_tfm ( subreq , ctx -> child ) ; compl = req -> base . complete ; data = req -> base . data ; info = req -> iv ; if ( req -> src != req -> dst ) { SYNC_SKCIPHER_REQUEST_ON_STACK ( nreq , ctx -> sknull ) ; skcipher_request_set_sync_tfm ( nreq , ctx -> sknull ) ; skcipher_request_set_callback ( nreq , req -> base . flags , NULL , NULL ) ; skcipher_request_set_crypt ( nreq , req -> src , req -> dst , req -> assoclen + req -> cryptlen , NULL ) ; err = crypto_skcipher_encrypt ( nreq ) ; if ( err ) { return err ; } } if ( unlikely ( ! IS_ALIGNED ( ( unsigned long ) info , crypto_aead_alignmask ( geniv ) + 1 ) ) ) { info = kmemdup ( req -> iv , ivsize , req -> base . flags & CRYPTO_TFM_REQ_MAY_SLEEP ?GFP_KERNEL : GFP_ATOMIC ) ; if ( ! info ) { return - ENOMEM ; } compl = seqiv_aead_encrypt_complete ; data = req ; } aead_request_set_callback ( subreq , req -> base . flags , compl , data ) ; aead_request_set_crypt ( subreq , req -> dst , req -> dst , req -> cryptlen - ivsize , info ) ; aead_request_set_ad ( subreq , req -> assoclen + ivsize ) ; crypto_xor ( info , ctx -> salt , ivsize ) ; scatterwalk_map_and_copy ( info , req -> dst , req -> assoclen , ivsize , 1 ) ; err = crypto_aead_encrypt ( subreq ) ; if ( unlikely ( info != req -> iv ) ) { seqiv_aead_encrypt_complete2 ( req , err ) ; } return err ; } 
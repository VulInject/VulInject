static struct iio_buffer * iio_dmaengine_buffer_alloc ( struct device * dev , const char * channel ) { struct dmaengine_buffer * dmaengine_buffer ; unsigned int width , src_width , dest_width ; struct dma_slave_caps caps ; struct dma_chan * chan ; int ret ; dmaengine_buffer = kzalloc ( sizeof ( * dmaengine_buffer ) , GFP_KERNEL ) ; if ( ! dmaengine_buffer ) { return ERR_PTR ( - ENOMEM ) ; } chan = dma_request_chan ( dev , channel ) ; if ( IS_ERR ( chan ) ) { ret = PTR_ERR ( chan ) ; err_free } ret = dma_get_slave_caps ( chan , & caps ) ; if ( ret < 0 ) { err_free } if ( caps . src_addr_widths ) { src_width = __ffs ( caps . src_addr_widths ) ; } else { src_width = 1 ; } if ( caps . dst_addr_widths ) { dest_width = __ffs ( caps . dst_addr_widths ) ; } else { dest_width = 1 ; } width = max ( src_width , dest_width ) ; INIT_LIST_HEAD ( & dmaengine_buffer -> active ) ; dmaengine_buffer -> chan = chan ; dmaengine_buffer -> align = width ; dmaengine_buffer -> max_size = dma_get_max_seg_size ( chan -> device -> dev ) ; iio_dma_buffer_init ( & dmaengine_buffer -> queue , chan -> device -> dev , & iio_dmaengine_default_ops ) ; dmaengine_buffer -> queue . buffer . attrs = iio_dmaengine_buffer_attrs ; dmaengine_buffer -> queue . buffer . access = & iio_dmaengine_buffer_ops ; return & dmaengine_buffer -> queue . buffer ; err_free return ERR_PTR ( ret ) ; } 
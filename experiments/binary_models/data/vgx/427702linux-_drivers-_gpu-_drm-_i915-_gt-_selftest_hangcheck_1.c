static struct i915_request * hang_create_request ( struct hang * h , struct intel_engine_cs * engine ) { struct intel_gt * gt = h -> gt ; struct i915_address_space * vm = i915_gem_context_get_eb_vm ( h -> ctx ) ; struct drm_i915_gem_object * obj ; struct i915_request * rq = NULL ; struct i915_vma * hws , * vma ; unsigned int flags ; void * vaddr ; u32 * batch ; int err ; obj = i915_gem_object_create_internal ( gt -> i915 , PAGE_SIZE ) ; if ( IS_ERR ( obj ) ) { i915_vm_put ( vm , NULL ) ; return ERR_CAST ( obj ) ; } vaddr = i915_gem_object_pin_map_unlocked ( obj , i915_coherent_map_type ( gt -> i915 , obj , false ) ) ; if ( IS_ERR ( vaddr ) ) { i915_gem_object_put ( obj ) ; i915_vm_put ( vm ) ; return ERR_CAST ( vaddr ) ; } i915_gem_object_unpin_map ( h -> obj ) ; i915_gem_object_put ( h -> obj ) ; h -> obj = obj ; h -> batch = vaddr ; vma = i915_vma_instance ( h -> obj , vm , NULL ) ; if ( IS_ERR ( vma ) ) { i915_vm_put ( vm ) ; return ERR_CAST ( vma ) ; } hws = i915_vma_instance ( h -> hws , vm , NULL ) ; if ( IS_ERR ( hws ) ) { i915_vm_put ( vm ) ; return ERR_CAST ( hws ) ; } err = i915_vma_pin ( vma , 0 , 0 , PIN_USER ) ; if ( err ) { i915_vm_put ( vm ) ; return ERR_PTR ( err ) ; } err = i915_vma_pin ( hws , 0 , 0 , PIN_USER ) ; if ( err ) { unpin_vma } rq = igt_request_alloc ( h -> ctx , engine ) ; if ( IS_ERR ( rq ) ) { err = PTR_ERR ( rq ) ; unpin_hws } err = igt_vma_move_to_active_unlocked ( vma , rq , 0 ) ; if ( err ) { cancel_rq } err = igt_vma_move_to_active_unlocked ( hws , rq , 0 ) ; if ( err ) { cancel_rq } batch = h -> batch ; if ( GRAPHICS_VER ( gt -> i915 ) >= 8 ) { * batch ++ = MI_STORE_DWORD_IMM_GEN4 ; * batch ++ = lower_32_bits ( hws_address ( hws , rq ) ) ; * batch ++ = upper_32_bits ( hws_address ( hws , rq ) ) ; * batch ++ = rq -> fence . seqno ; * batch ++ = MI_NOOP ; memset ( batch , 0 , 1024 ) ; batch += 1024 / sizeof ( * batch ) ; * batch ++ = MI_NOOP ; * batch ++ = MI_BATCH_BUFFER_START | 1 << 8 | 1 ; * batch ++ = lower_32_bits ( i915_vma_offset ( vma ) ) ; * batch ++ = upper_32_bits ( i915_vma_offset ( vma ) ) ; } if ( GRAPHICS_VER ( gt -> i915 ) >= 6 ) { * batch ++ = MI_STORE_DWORD_IMM_GEN4 ; * batch ++ = 0 ; * batch ++ = lower_32_bits ( hws_address ( hws , rq ) ) ; * batch ++ = rq -> fence . seqno ; * batch ++ = MI_NOOP ; memset ( batch , 0 , 1024 ) ; batch += 1024 / sizeof ( * batch ) ; * batch ++ = MI_NOOP ; * batch ++ = MI_BATCH_BUFFER_START | 1 << 8 ; * batch ++ = lower_32_bits ( i915_vma_offset ( vma ) ) ; } if ( GRAPHICS_VER ( gt -> i915 ) >= 4 ) { * batch ++ = MI_STORE_DWORD_IMM_GEN4 | MI_USE_GGTT ; * batch ++ = 0 ; * batch ++ = lower_32_bits ( hws_address ( hws , rq ) ) ; * batch ++ = rq -> fence . seqno ; * batch ++ = MI_NOOP ; memset ( batch , 0 , 1024 ) ; batch += 1024 / sizeof ( * batch ) ; * batch ++ = MI_NOOP ; * batch ++ = MI_BATCH_BUFFER_START | 2 << 6 ; * batch ++ = lower_32_bits ( i915_vma_offset ( vma ) ) ; } else { * batch ++ = MI_STORE_DWORD_IMM | MI_MEM_VIRTUAL ; * batch ++ = lower_32_bits ( hws_address ( hws , rq ) ) ; * batch ++ = rq -> fence . seqno ; * batch ++ = MI_NOOP ; memset ( batch , 0 , 1024 ) ; batch += 1024 / sizeof ( * batch ) ; * batch ++ = MI_NOOP ; * batch ++ = MI_BATCH_BUFFER_START | 2 << 6 ; * batch ++ = lower_32_bits ( i915_vma_offset ( vma ) ) ; } * batch ++ = MI_BATCH_BUFFER_END ; intel_gt_chipset_flush ( engine -> gt ) ; if ( rq -> engine -> emit_init_breadcrumb ) { err = rq -> engine -> emit_init_breadcrumb ( rq ) ; if ( err ) { cancel_rq } } flags = 0 ; if ( GRAPHICS_VER ( gt -> i915 ) <= 5 ) { flags |= I915_DISPATCH_SECURE ; } err = rq -> engine -> emit_bb_start ( rq , i915_vma_offset ( vma ) , PAGE_SIZE , flags ) ; cancel_rq if ( err ) { i915_request_set_error_once ( rq , err ) ; i915_request_add ( rq ) ; } unpin_hws i915_vma_unpin ( hws ) ; unpin_vma i915_vma_unpin ( vma ) ; i915_vm_put ( vm ) ; return err ?ERR_PTR ( err ) : rq ; } 
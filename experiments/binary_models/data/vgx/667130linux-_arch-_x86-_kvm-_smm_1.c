static int rsm_load_state_64 ( struct x86_emulate_ctxt * ctxt , const struct kvm_smram_state_64 * smstate ) { struct kvm_vcpu * vcpu = ctxt -> vcpu ; struct desc_ptr dt ; int i , r ; for ( i = 0 ; i < 16 ; i ++ ) { * reg_write ( ctxt , i ) = smstate -> gprs [ 15 - i ] ; } ctxt -> _eip = smstate -> rip ; ctxt -> eflags = smstate -> rflags | X86_EFLAGS_FIXED ; if ( kvm_set_dr ( vcpu , 7 , smstate -> dr7 ) ) { return X86EMUL_UNHANDLEABLE ; } vcpu -> arch . smbase = smstate -> smbase ; if ( kvm_set_msr ( vcpu , MSR_EFER , smstate -> efer & ~ EFER_LMA ) ) { return X86EMUL_UNHANDLEABLE ; } rsm_load_seg_64 ( vcpu , & smstate -> tr , VCPU_SREG_TR ) ; dt . size = smstate -> idtr . limit ; dt . address = smstate -> idtr . base ; static_call ( kvm_x86_set_idt ) ( vcpu , & dt ) ; rsm_load_seg_64 ( vcpu , & smstate -> ldtr , VCPU_SREG_LDTR ) ; dt . size = smstate -> gdtr . limit ; dt . address = smstate -> gdtr . base ; static_call ( kvm_x86_set_gdt ) ( vcpu , & dt ) ; r = rsm_enter_protected_mode ( vcpu , smstate -> cr0 , smstate -> cr3 , smstate -> cr4 ) ; if ( r != X86EMUL_CONTINUE ) { return r ; } rsm_load_seg_64 ( vcpu , & smstate -> es , VCPU_SREG_ES ) ; rsm_load_seg_64 ( vcpu , & smstate -> cs , VCPU_SREG_CS ) ; rsm_load_seg_64 ( vcpu , & smstate -> ss , VCPU_SREG_SS ) ; rsm_load_seg_64 ( vcpu , & smstate -> ds , VCPU_SREG_DS ) ; rsm_load_seg_64 ( vcpu , & smstate -> fs , VCPU_SREG_FS ) ; rsm_load_seg_64 ( vcpu , & smstate -> gs , VCPU_SREG_GS ) ; static_call ( kvm_x86_set_interrupt_shadow ) ( vcpu , 0 ) ; ctxt -> interruptibility = ( u8 ) smstate -> int_shadow ; return X86EMUL_CONTINUE ; } 
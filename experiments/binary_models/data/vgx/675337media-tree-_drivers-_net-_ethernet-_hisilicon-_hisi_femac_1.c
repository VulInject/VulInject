static netdev_tx_t hisi_femac_net_xmit ( struct sk_buff * skb , struct net_device * dev ) { struct hisi_femac_priv * priv = netdev_priv ( dev ) ; struct hisi_femac_queue * txq = & priv -> txq ; dma_addr_t addr ; u32 val ; val = readl ( priv -> port_base + ADDRQ_STAT ) ; val &= BIT_TX_READY ; if ( ! val ) { hisi_femac_irq_enable ( priv , IRQ_INT_TX_PER_PACKET ) ; dev -> stats . tx_dropped ++ ; dev -> stats . tx_fifo_errors ++ ; netif_stop_queue ( dev ) ; return NETDEV_TX_BUSY ; } if ( unlikely ( ! CIRC_SPACE ( txq -> head , txq -> tail , txq -> num ) ) ) { hisi_femac_irq_enable ( priv , IRQ_INT_TX_PER_PACKET ) ; dev -> stats . tx_dropped ++ ; dev -> stats . tx_fifo_errors ++ ; netif_stop_queue ( dev ) ; return NETDEV_TX_BUSY ; } addr = dma_map_single ( priv -> dev , skb -> data , skb -> len , DMA_TO_DEVICE ) ; if ( unlikely ( dma_mapping_error ( priv -> dev , addr ) ) ) { dev -> stats . tx_dropped ++ ; return NETDEV_TX_OK ; } txq -> dma_phys [ txq -> head ] = addr ; txq -> skb [ txq -> head ] = skb ; txq -> head = ( txq -> head + 1 ) % txq -> num ; writel ( addr , priv -> port_base + EQ_ADDR ) ; writel ( skb -> len + ETH_FCS_LEN , priv -> port_base + EQFRM_LEN ) ; priv -> tx_fifo_used_cnt ++ ; dev -> stats . tx_packets ++ ; dev -> stats . tx_bytes += skb -> len ; netdev_sent_queue ( dev , skb -> len ) ; return NETDEV_TX_OK ; } 
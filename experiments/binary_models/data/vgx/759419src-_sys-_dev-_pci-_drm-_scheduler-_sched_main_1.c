void drm_sched_start ( struct drm_gpu_scheduler * sched , bool full_recovery ) { struct drm_sched_job * s_job , * tmp ; int r ; list_for_each_entry_safe ( , , , ) { struct dma_fence * fence = s_job -> s_fence -> parent ; atomic_inc ( & sched -> hw_rq_count ) ; if ( ! full_recovery ) { continue ; } if ( fence ) { r = dma_fence_add_callback ( fence , & s_job -> cb , drm_sched_job_done_cb ) ; if ( r == - ENOENT ) { drm_sched_job_done ( s_job ) ; } if ( r ) { DRM_DEV_ERROR ( sched -> dev , "fence add callback failed (%d)\n" , r ) ; } } else { drm_sched_job_done ( s_job ) ; } } if ( full_recovery ) { spin_lock ( & sched -> job_list_lock ) ; drm_sched_start_timeout ( sched , NULL ) ; spin_unlock ( & sched -> job_list_lock ) ; } kthread_unpark ( sched -> thread ) ; } 
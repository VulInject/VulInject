static bool qede_rx_xdp ( struct qede_dev * edev , struct qede_fastpath * fp , struct qede_rx_queue * rxq , struct bpf_prog * prog , struct sw_rx_data * bd , struct eth_fast_path_rx_reg_cqe * cqe , u16 * data_offset , u16 * len ) { struct xdp_buff xdp ; enum xdp_action act ; xdp . data_hard_start = page_address ( bd -> data , NULL ) ; xdp . data = xdp . data_hard_start + * data_offset ; xdp . data_end = xdp . data + * len ; rcu_read_lock ( ) ; act = bpf_prog_run_xdp ( prog , & xdp ) ; rcu_read_unlock ( ) ; * data_offset = xdp . data - xdp . data_hard_start ; * len = xdp . data_end - xdp . data ; if ( act == XDP_PASS ) { return true ; } rxq -> xdp_no_pass ++ ; switch ( act ) { case XDP_TX : if ( qede_alloc_rx_buffer ( rxq , true ) ) { qede_recycle_rx_bd_ring ( rxq , 1 ) ; trace_xdp_exception ( edev -> ndev , prog , act ) ; return false ; } if ( qede_xdp_xmit ( edev , fp , bd , * data_offset , * len ) ) { dma_unmap_page ( rxq -> dev , bd -> mapping , PAGE_SIZE , DMA_BIDIRECTIONAL ) ; __free_page ( bd -> data ) ; trace_xdp_exception ( edev -> ndev , prog , act ) ; } qede_rx_bd_ring_consume ( rxq ) ; return false ; default : bpf_warn_invalid_xdp_action ( act ) ; case XDP_ABORTED : trace_xdp_exception ( edev -> ndev , prog , act ) ; case XDP_DROP : qede_recycle_rx_bd_ring ( rxq , cqe -> bd_num ) ; } return false ; } 
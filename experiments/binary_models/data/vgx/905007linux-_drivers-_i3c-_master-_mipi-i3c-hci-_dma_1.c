static bool hci_dma_irq_handler ( struct i3c_hci * hci , unsigned int mask ) { struct hci_rings_data * rings = hci -> io_data ; unsigned int i ; bool handled = false ; for ( i = 0 ; mask && i < 8 ; i ++ ) { struct hci_rh_data * rh ; u32 status ; mask &= ~ BIT ( i ) ; rh = & rings -> headers [ i ] ; status = rh_reg_read ( INTR_STATUS ) ; DBG ( "rh%d status: %#x" , i , status ) ; if ( ! status ) { continue ; } rh_reg_write ( INTR_STATUS , status ) ; if ( status & INTR_IBI_READY ) { hci_dma_process_ibi ( hci , rh ) ; } if ( status & ( INTR_TRANSFER_COMPLETION | INTR_TRANSFER_ERR ) ) { hci_dma_xfer_done ( hci , rh ) ; } if ( status & INTR_RING_OP ) { complete ( & rh -> op_done ) ; } if ( status & INTR_TRANSFER_ABORT ) { dev_notice_ratelimited ( & hci -> master . dev , "ring %d: Transfer Aborted\n" , i ) ; } if ( status & INTR_WARN_INS_STOP_MODE ) { dev_warn_ratelimited ( & hci -> master . dev , "ring %d: Inserted Stop on Mode Change\n" , i ) ; } if ( status & INTR_IBI_RING_FULL ) { dev_err_ratelimited ( & hci -> master . dev , "ring %d: IBI Ring Full Condition\n" , i ) ; } handled = true ; } return handled ; } const struct hci_io_ops mipi_i3c_hci_dma = { . init = hci_dma_init . cleanup = hci_dma_cleanup . queue_xfer = hci_dma_queue_xfer . dequeue_xfer = hci_dma_dequeue_xfer . irq_handler = hci_dma_irq_handler . request_ibi = hci_dma_request_ibi . free_ibi = hci_dma_free_ibi . recycle_ibi_slot = hci_dma_recycle_ibi_slot } ; 
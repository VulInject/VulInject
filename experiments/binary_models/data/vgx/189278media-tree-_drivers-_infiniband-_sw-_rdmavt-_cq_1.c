void rvt_cq_enter ( struct rvt_cq * cq , struct ib_wc * entry , bool solicited ) { struct rvt_cq_wc * wc ; cfs_time_t flags ; u32 head ; u32 next ; spin_lock_irqsave ( & cq -> lock , flags ) ; wc = cq -> queue ; head = wc -> head ; if ( head >= ( unsigned ) cq -> ibcq . cqe ) { head = cq -> ibcq . cqe ; next = 0 ; } else { next = head + 1 ; } if ( unlikely ( next == wc -> tail ) ) { spin_unlock_irqrestore ( & cq -> lock , flags ) ; if ( cq -> ibcq . event_handler ) { struct ib_event ev ; ev . device = cq -> ibcq . device ; ev . element . cq = & cq -> ibcq ; ev . event = IB_EVENT_CQ_ERR ; cq -> ibcq . event_handler ( & ev , cq -> ibcq . cq_context ) ; } return ; } trace_rvt_cq_enter ( cq , entry , head ) ; if ( cq -> ip ) { wc -> uqueue [ head ] . wr_id = entry -> wr_id ; wc -> uqueue [ head ] . status = entry -> status ; wc -> uqueue [ head ] . opcode = entry -> opcode ; wc -> uqueue [ head ] . vendor_err = entry -> vendor_err ; wc -> uqueue [ head ] . byte_len = entry -> byte_len ; wc -> uqueue [ head ] . ex . imm_data = ( __u32 __force ) entry -> ex . imm_data ; wc -> uqueue [ head ] . qp_num = entry -> qp -> qp_num ; wc -> uqueue [ head ] . src_qp = entry -> src_qp ; wc -> uqueue [ head ] . wc_flags = entry -> wc_flags ; wc -> uqueue [ head ] . pkey_index = entry -> pkey_index ; wc -> uqueue [ head ] . slid = entry -> slid ; wc -> uqueue [ head ] . sl = entry -> sl ; wc -> uqueue [ head ] . dlid_path_bits = entry -> dlid_path_bits ; wc -> uqueue [ head ] . port_num = entry -> port_num ; smp_wmb ( ) ; } else { wc -> kqueue [ head ] = * entry ; } wc -> head = next ; if ( cq -> notify == IB_CQ_NEXT_COMP || ( cq -> notify == IB_CQ_SOLICITED && ( solicited || entry -> status != IB_WC_SUCCESS ) ) ) { spin_lock ( & cq -> rdi -> n_cqs_lock ) ; if ( likely ( cq -> rdi -> worker ) ) { cq -> notify = RVT_CQ_NONE ; cq -> triggered ++ ; kthread_queue_work ( cq -> rdi -> worker , & cq -> comptask ) ; } spin_unlock ( & cq -> rdi -> n_cqs_lock ) ; } spin_unlock_irqrestore ( & cq -> lock , flags ) ; } 
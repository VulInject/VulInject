static int ibmvnic_complete_tx ( struct ibmvnic_adapter * adapter , struct ibmvnic_sub_crq_queue * scrq ) { struct device * dev = & adapter -> vdev -> dev ; struct ibmvnic_tx_buff * txbuff ; union sub_crq * next ; int index ; int i , j ; u8 first ; restart_loop while ( pending_scrq ( adapter , scrq ) ) { int pool = scrq -> pool_index ; next = ibmvnic_next_scrq ( adapter , scrq ) ; for ( i = 0 ; i < next -> tx_comp . num_comps ; i ++ ) { if ( next -> tx_comp . rcs [ i ] ) { dev_err ( dev , "tx error %x\n" , next -> tx_comp . rcs [ i ] ) ; continue ; } index = be32_to_cpu ( next -> tx_comp . correlators [ i ] ) ; txbuff = & adapter -> tx_pool [ pool ] . tx_buff [ index ] ; for ( j = 0 ; j < IBMVNIC_MAX_FRAGS_PER_CRQ ; j ++ ) { if ( ! txbuff -> data_dma [ j ] ) { continue ; } txbuff -> data_dma [ j ] = 0 ; } first = txbuff -> indir_arr [ 0 ] . generic . first ; if ( first == IBMVNIC_CRQ_CMD ) { dma_unmap_single ( dev , txbuff -> indir_dma , sizeof ( txbuff -> indir_arr ) , DMA_TO_DEVICE ) ; } if ( txbuff -> last_frag ) { dev_kfree_skb_any ( txbuff -> skb ) ; txbuff -> skb = NULL ; } adapter -> tx_pool [ pool ] . free_map [ adapter -> tx_pool [ pool ] . producer_index ] = index ; adapter -> tx_pool [ pool ] . producer_index = ( adapter -> tx_pool [ pool ] . producer_index + 1 ) % adapter -> req_tx_entries_per_subcrq ; } next -> tx_comp . first = 0 ; if ( atomic_sub_return ( next -> tx_comp . num_comps , & scrq -> used ) <= ( adapter -> req_tx_entries_per_subcrq / 2 ) && __netif_subqueue_stopped ( adapter -> netdev , scrq -> pool_index ) ) { netif_wake_subqueue ( adapter -> netdev , scrq -> pool_index ) ; netdev_info ( adapter -> netdev , "Started queue %d\n" , scrq -> pool_index ) ; } } enable_scrq_irq ( adapter , scrq ) ; if ( pending_scrq ( adapter , scrq ) ) { disable_scrq_irq ( adapter , scrq ) ; restart_loop } return 0 ; } 
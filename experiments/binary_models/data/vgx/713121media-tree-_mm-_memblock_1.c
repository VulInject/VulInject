void __init_memblock __next_mem_range ( u64 * idx , int nid , ulong flags , struct memblock_type * type_a , struct memblock_type * type_b , phys_addr_t * out_start , phys_addr_t * out_end , int * out_nid ) { int idx_a = * idx & 0xffffffff ; int idx_b = * idx >> 32 ; if ( WARN_ONCE ( nid == MAX_NUMNODES , "Usage of MAX_NUMNODES is deprecated. Use NUMA_NO_NODE instead\n" ) ) { nid = NUMA_NO_NODE ; } for ( ; idx_a < type_a -> cnt ; idx_a ++ ) { struct memblock_region * m = & type_a -> regions [ idx_a ] ; phys_addr_t m_start = m -> base ; phys_addr_t m_end = m -> base + m -> size ; int m_nid = memblock_get_region_node ( m ) ; if ( nid != NUMA_NO_NODE && nid != m_nid ) { continue ; } if ( movable_node_is_enabled ( ) && memblock_is_hotpluggable ( m ) ) { continue ; } if ( ( flags & MEMBLOCK_MIRROR ) && ! memblock_is_mirror ( m ) ) { continue ; } if ( ! type_b ) { if ( out_start ) { * out_start = m_start ; } if ( out_end ) { * out_end = m_end ; } if ( out_nid ) { * out_nid = m_nid ; } idx_a ++ ; * idx = ( u32 ) idx_a | ( u64 ) idx_b << 32 ; return ; } for ( ; idx_b < type_b -> cnt + 1 ; idx_b ++ ) { struct memblock_region * r ; phys_addr_t r_start ; phys_addr_t r_end ; r = & type_b -> regions [ idx_b ] ; r_start = idx_b ?r [ - 1 ] . base + r [ - 1 ] . size : 0 ; r_end = idx_b < type_b -> cnt ?r -> base : ULLONG_MAX ; if ( r_start >= m_end ) { break ; } if ( m_start < r_end ) { if ( out_start ) { * out_start = max ( m_start , r_start ) ; } if ( out_end ) { * out_end = min ( m_end , r_end ) ; } if ( out_nid ) { * out_nid = m_nid ; } if ( m_end <= r_end ) { idx_a ++ ; } else { idx_b ++ ; } * idx = ( u32 ) idx_a | ( u64 ) idx_b << 32 ; return ; } } } * idx = ULLONG_MAX ; } 
static int txq_reclaim ( struct tx_queue * txq , int budget , int force ) { struct mv643xx_eth_private * mp = txq_to_mp ( txq ) ; struct netdev_queue * nq = netdev_get_tx_queue ( mp -> dev , txq -> index ) ; int reclaimed ; __netif_tx_lock_bh ( nq ) ; while ( reclaimed ( budget && txq -> tx_desc_count ) 0 ) { int tx_index ; struct tx_desc * desc ; u32 cmd_sts ; char desc_dma_map ; tx_index = txq -> tx_used_desc ; desc = & txq -> tx_desc_area [ tx_index ] ; desc_dma_map = txq -> tx_desc_mapping [ tx_index ] ; cmd_sts = desc -> cmd_sts ; if ( cmd_sts & BUFFER_OWNED_BY_DMA ) { if ( ! force ) { break ; } desc -> cmd_sts = cmd_sts & ~ BUFFER_OWNED_BY_DMA ; } txq -> tx_used_desc = tx_index + 1 ; if ( txq -> tx_used_desc == txq -> tx_ring_size ) { txq -> tx_used_desc = 0 ; } reclaimed ++ ; txq -> tx_desc_count -- ; if ( ! IS_TSO_HEADER ( txq , desc -> buf_ptr ) ) { if ( desc_dma_map == DESC_DMA_MAP_PAGE ) { dma_unmap_page ( mp -> dev -> dev . parent , desc -> buf_ptr , desc -> byte_cnt , DMA_TO_DEVICE ) ; } else { dma_unmap_single ( mp -> dev -> dev . parent , desc -> buf_ptr , desc -> byte_cnt , DMA_TO_DEVICE ) ; } } if ( cmd_sts & TX_ENABLE_INTERRUPT ) { struct sk_buff * skb = __skb_dequeue ( & txq -> tx_skb ) ; if ( ! WARN_ON ( ! skb ) ) { dev_kfree_skb ( skb ) ; } } if ( cmd_sts & ERROR_SUMMARY ) { netdev_info ( mp -> dev , "tx error\n" ) ; mp -> dev -> stats . tx_errors ++ ; } } __netif_tx_unlock_bh ( nq ) ; if ( reclaimed < budget ) { mp -> work_tx &= ~ ( 1 << txq -> index ) ; } return reclaimed ; } 
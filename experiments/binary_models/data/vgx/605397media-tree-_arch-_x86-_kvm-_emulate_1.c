static __always_inline int __linearize ( struct x86_emulate_ctxt * ctxt , struct segmented_address addr , unsigned * max_size , unsigned size , bool write , bool fetch , enum x86emul_mode mode , ulong * linear ) { struct desc_struct desc ; bool usable ; ulong la ; u32 lim ; u16 sel ; la = seg_base ( ctxt , addr . seg ) + addr . ea ; * max_size = 0 ; switch ( mode ) { case X86EMUL_MODE_PROT64 : * linear = la ; if ( is_noncanonical_address ( la ) ) { bad } * max_size = min_t ( u64 , ~ 0u , ( 1ull << 48 ) - la ) ; if ( size > * max_size ) { bad } break ; default : * linear = la = ( u32 ) la ; usable = ctxt -> ops -> get_segment ( ctxt , & sel , & desc , NULL , addr . seg ) ; if ( ( ( ( ctxt -> mode != X86EMUL_MODE_REAL ) && ( desc . type & 8 ) ) || ! ( desc . type & 2 ) ) && write ) { bad } if ( ! fetch && ( desc . type & 8 ) && ! ( desc . type & 2 ) ) { bad } lim = desc_limit_scaled ( & desc ) ; if ( ! ( desc . type & 8 ) && ( desc . type & 4 ) ) { if ( addr . ea <= lim ) { bad } lim = desc . d ?0xffffffff : 0xffff ; } if ( addr . ea > lim ) { bad } if ( lim == 0xffffffff ) { * max_size = ~ 0u ; } else { * max_size = ( u64 ) lim + 1 - addr . ea ; if ( size > * max_size ) { bad } } break ; } if ( la & ( insn_alignment ( ctxt , size ) - 1 ) ) { return emulate_gp ( ctxt , 0 ) ; } return X86EMUL_CONTINUE ; bad if ( addr . seg == VCPU_SREG_SS ) { return emulate_ss ( ctxt , 0 ) ; } else { return emulate_gp ( ctxt , 0 ) ; } } 
static int __virtio_crypto_ablkcipher_do_req ( struct virtio_crypto_request * vc_req , struct ablkcipher_request * req , struct data_queue * data_vq ) { struct crypto_ablkcipher * tfm = crypto_ablkcipher_reqtfm ( req ) ; unsigned int ivsize = crypto_ablkcipher_ivsize ( tfm ) ; struct virtio_crypto_ablkcipher_ctx * ctx = vc_req -> ablkcipher_ctx ; struct virtio_crypto * vcrypto = ctx -> vcrypto ; struct virtio_crypto_op_data_req * req_data ; int src_nents , dst_nents ; int err ; unsigned long flags ; struct scatterlist outhdr , iv_sg , status_sg , * * sgs ; int i ; u64 dst_len ; unsigned int num_out = 0 , num_in = 0 ; int sg_total ; uint8_t * iv ; src_nents = sg_nents_for_len ( req -> src , req -> nbytes ) ; dst_nents = sg_nents ( req -> dst ) ; pr_debug ( "virtio_crypto: Number of sgs (src_nents: %d, dst_nents: %d)\n" , src_nents , dst_nents ) ; sg_total = src_nents + dst_nents + 3 ; sgs = kzalloc_node ( sg_total * sizeof ( * sgs ) , GFP_ATOMIC , dev_to_node ( & vcrypto -> vdev -> dev ) ) ; if ( ! sgs ) { return - ENOMEM ; } req_data = kzalloc_node ( sizeof ( * req_data ) , GFP_ATOMIC , dev_to_node ( & vcrypto -> vdev -> dev ) ) ; if ( ! req_data ) { return - ENOMEM ; } vc_req -> req_data = req_data ; vc_req -> type = VIRTIO_CRYPTO_SYM_OP_CIPHER ; if ( vc_req -> encrypt ) { req_data -> header . session_id = cpu_to_le64 ( ctx -> enc_sess_info . session_id ) ; req_data -> header . opcode = cpu_to_le32 ( VIRTIO_CRYPTO_CIPHER_ENCRYPT ) ; } else { req_data -> header . session_id = cpu_to_le64 ( ctx -> dec_sess_info . session_id ) ; req_data -> header . opcode = cpu_to_le32 ( VIRTIO_CRYPTO_CIPHER_DECRYPT ) ; } req_data -> u . sym_req . op_type = cpu_to_le32 ( VIRTIO_CRYPTO_SYM_OP_CIPHER ) ; req_data -> u . sym_req . u . cipher . para . iv_len = cpu_to_le32 ( ivsize ) ; req_data -> u . sym_req . u . cipher . para . src_data_len = cpu_to_le32 ( req -> nbytes ) ; dst_len = virtio_crypto_alg_sg_nents_length ( req -> dst ) ; if ( unlikely ( dst_len > U32_MAX ) ) { pr_err ( "virtio_crypto: The dst_len is beyond U32_MAX\n" ) ; err = - EINVAL ; free } pr_debug ( "virtio_crypto: src_len: %u, dst_len: %llu\n" , req -> nbytes , dst_len ) ; if ( unlikely ( req -> nbytes + dst_len + ivsize + sizeof ( vc_req -> status ) > vcrypto -> max_size ) ) { pr_err ( "virtio_crypto: The length is too big\n" ) ; err = - EINVAL ; free } req_data -> u . sym_req . u . cipher . para . dst_data_len = cpu_to_le32 ( ( uint32_t ) dst_len ) ; sg_init_one ( & outhdr , req_data , sizeof ( * req_data ) ) ; sgs [ num_out ++ ] = & outhdr ; iv = kzalloc_node ( ivsize , GFP_ATOMIC , dev_to_node ( & vcrypto -> vdev -> dev ) ) ; if ( ! iv ) { err = - ENOMEM ; free } memcpy ( iv , req -> info , ivsize ) ; sg_init_one ( & iv_sg , iv , ivsize ) ; sgs [ num_out ++ ] = & iv_sg ; vc_req -> iv = iv ; for ( i = 0 ; i < src_nents ; i ++ ) { sgs [ num_out ++ ] = & req -> src [ i ] ; } for ( i = 0 ; i < dst_nents ; i ++ ) { sgs [ num_out + num_in ++ ] = & req -> dst [ i ] ; } sg_init_one ( & status_sg , & vc_req -> status , sizeof ( vc_req -> status ) ) ; sgs [ num_out + num_in ++ ] = & status_sg ; vc_req -> sgs = sgs ; spin_lock_irqsave ( & data_vq -> lock , flags ) ; err = virtqueue_add_sgs ( data_vq -> vq , sgs , num_out , num_in , vc_req , GFP_ATOMIC ) ; virtqueue_kick ( data_vq -> vq ) ; spin_unlock_irqrestore ( & data_vq -> lock , flags ) ; if ( unlikely ( err < 0 ) ) { free_iv } return 0 ; free_iv kzfree ( iv ) ; free kzfree ( req_data ) ; kfree ( sgs ) ; return err ; } 
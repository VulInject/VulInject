static int mthca_alloc_wqe_buf ( struct mthca_dev * dev , struct mthca_pd * pd , struct mthca_qp * qp , struct ib_udata * udata ) { int size ; int err = - ENOMEM ; size = sizeof ( mthca_next_seg ) + qp -> rq . max_gs * sizeof ( mthca_data_seg ) ; for ( qp -> rq . wqe_shift = 6 ; 1 << qp -> rq . wqe_shift < size ; qp -> rq . wqe_shift ++ ) { } size = qp -> sq . max_gs * sizeof ( mthca_data_seg ) ; switch ( qp -> transport ) { case MLX : size += 2 * sizeof ( mthca_data_seg ) ; break ; case UD : size += mthca_is_memfree ( dev ) ?sizeof ( mthca_arbel_ud_seg ) : sizeof ( mthca_tavor_ud_seg ) ; break ; case UC : size += sizeof ( mthca_raddr_seg ) ; break ; case RC : size += sizeof ( mthca_raddr_seg ) ; size = max_t ( int , size , sizeof ( mthca_atomic_seg ) + sizeof ( mthca_raddr_seg ) + sizeof ( mthca_data_seg ) ) ; break ; default : break ; } size = max_t ( int , size , sizeof ( mthca_bind_seg ) ) ; size += sizeof ( mthca_next_seg ) ; if ( size > dev -> limits . max_desc_sz ) { return - EINVAL ; } for ( qp -> sq . wqe_shift = 6 ; 1 << qp -> sq . wqe_shift < size ; qp -> sq . wqe_shift ++ ) { } qp -> send_wqe_offset = ALIGN ( qp -> rq . max << qp -> rq . wqe_shift , 1 << qp -> sq . wqe_shift ) ; if ( udata ) { return 0 ; } size = PAGE_ALIGN ( qp -> send_wqe_offset + ( qp -> sq . max << qp -> sq . wqe_shift ) ) ; qp -> wrid = kmalloc_array ( qp -> rq . max + qp -> sq . max , sizeof ( u64 ) , GFP_KERNEL ) ; if ( ! qp -> wrid ) { err_out } err = mthca_buf_alloc ( dev , size , MTHCA_MAX_DIRECT_QP_SIZE , & qp -> queue , & qp -> is_direct , pd , 0 , & qp -> mr ) ; if ( err ) { err_out } return 0 ; err_out kfree ( qp -> wrid ) ; return err ; } 
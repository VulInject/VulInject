void * __dma_alloc_coherent ( struct device * dev , size_t size , dma_addr_t * handle , gfp_t gfp ) { struct page * page ; struct ppc_vm_region * c ; unsigned long order ; u64 mask = ISA_DMA_THRESHOLD , limit ; if ( dev ) { mask = dev -> coherent_dma_mask ; if ( mask == 0 ) { dev_warn ( dev , "coherent DMA mask is unset\n" ) ; no_page } if ( ( ~ mask ) & ISA_DMA_THRESHOLD ) { dev_warn ( dev , "coherent DMA mask %#llx is smaller " "than system GFP_DMA mask %#llx\n" , mask , ( unsigned long long ) ISA_DMA_THRESHOLD ) ; no_page } } size = PAGE_ALIGN ( size ) ; limit = ( mask + 1 ) & ~ mask ; if ( ( limit && size >= limit ) || size >= ( CONSISTENT_END - CONSISTENT_BASE ) ) { printk ( KERN_WARNING "coherent allocation too big (requested %#x mask %#Lx)\n" , size , mask ) ; return NULL ; } order = get_order ( size ) ; if ( mask != 0xffffffff ) { gfp |= GFP_DMA ; } page = alloc_pages ( gfp , order ) ; if ( ! page ) { no_page } { unsigned long kaddr = ( unsigned long ) page_address ( page ) ; flush_dcache_range ( kaddr , kaddr + size ) ; } c = ppc_vm_region_alloc ( & consistent_head , size , gfp & ~ ( __GFP_DMA | __GFP_HIGHMEM ) ) ; if ( c ) { unsigned long vaddr = c -> vm_start ; struct page * end = page + ( 1 << order ) ; split_page ( page , order ) ; * handle = page_to_phys ( page ) ; { SetPageReserved ( page ) ; map_page ( vaddr , page_to_phys ( page ) , pgprot_val ( pgprot_noncached ( PAGE_KERNEL ) ) ) ; page ++ ; vaddr += PAGE_SIZE ; } size -= PAGE_SIZE ; while ( page < end ) { __free_page ( page ) ; page ++ ; } return ( void * ) c -> vm_start ; } if ( page ) { __free_pages ( page , order ) ; } no_page return NULL ; } 
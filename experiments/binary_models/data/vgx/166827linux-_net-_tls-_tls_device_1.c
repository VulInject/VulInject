void tls_device_rx_resync_new_rec ( struct sock * sk , u32 rcd_len , u32 seq ) { struct tls_context * tls_ctx = tls_get_ctx ( sk ) ; struct tls_offload_context_rx * rx_ctx ; int rcd_sn [ TLS_MAX_REC_SEQ_SIZE ] ; u32 sock_data , is_req_pending ; struct tls_prot_info * prot ; s64 resync_req ; u16 rcd_delta ; u32 req_seq ; if ( tls_ctx -> rx_conf != TLS_HW ) { return ; } if ( unlikely ( test_bit ( TLS_RX_DEV_DEGRADED , & tls_ctx -> flags ) ) ) { return ; } prot = & tls_ctx -> prot_info ; rx_ctx = tls_offload_ctx_rx ( tls_ctx ) ; memcpy ( rcd_sn , tls_ctx -> rx . rec_seq , prot -> rec_seq_size ) ; switch ( rx_ctx -> resync_type ) { case TLS_OFFLOAD_SYNC_TYPE_DRIVER_REQ : resync_req = atomic64_read ( & rx_ctx -> resync_req ) ; req_seq = resync_req >> 32 ; seq += TLS_HEADER_SIZE - 1 ; is_req_pending = resync_req ; if ( likely ( ! is_req_pending ) || req_seq != seq || ! atomic64_try_cmpxchg ( & rx_ctx -> resync_req , & resync_req , 0 ) ) { return ; } break ; case TLS_OFFLOAD_SYNC_TYPE_CORE_NEXT_HINT : if ( likely ( ! rx_ctx -> resync_nh_do_now ) ) { return ; } sock_data = tcp_inq ( sk ) ; if ( sock_data > rcd_len ) { trace_tls_device_rx_resync_nh_delay ( sk , sock_data , rcd_len ) ; return ; } rx_ctx -> resync_nh_do_now = 0 ; seq += rcd_len ; tls_bigint_increment ( rcd_sn , prot -> rec_seq_size ) ; break ; case TLS_OFFLOAD_SYNC_TYPE_DRIVER_REQ_ASYNC : resync_req = atomic64_read ( & rx_ctx -> resync_async -> req ) ; is_req_pending = resync_req ; if ( likely ( ! is_req_pending ) ) { return ; } if ( ! tls_device_rx_resync_async ( rx_ctx -> resync_async , resync_req , & seq , & rcd_delta ) ) { return ; } tls_bigint_subtract ( rcd_sn , rcd_delta ) ; break ; } tls_device_resync_rx ( tls_ctx , sk , seq , rcd_sn ) ; } 
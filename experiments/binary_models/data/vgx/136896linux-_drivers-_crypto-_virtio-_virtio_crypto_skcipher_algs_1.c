static int __virtio_crypto_skcipher_do_req ( struct virtio_crypto_sym_request * vc_sym_req , struct skcipher_request * req , struct data_queue * data_vq ) { struct crypto_skcipher * tfm = crypto_skcipher_reqtfm ( req ) ; struct virtio_crypto_skcipher_ctx * ctx = vc_sym_req -> skcipher_ctx ; struct virtio_crypto_request * vc_req = & vc_sym_req -> base ; unsigned int ivsize = crypto_skcipher_ivsize ( tfm ) ; struct virtio_crypto * vcrypto = ctx -> vcrypto ; struct virtio_crypto_op_data_req * req_data ; int src_nents , dst_nents ; int err ; unsigned long flags ; struct scatterlist outhdr , iv_sg , status_sg , * * sgs ; u64 dst_len ; unsigned int num_out = 0 , num_in = 0 ; int sg_total ; uint8_t * iv ; struct scatterlist * sg ; src_nents = sg_nents_for_len ( req -> src , req -> cryptlen ) ; if ( src_nents < 0 ) { pr_err ( "Invalid number of src SG.\n" ) ; return src_nents ; } dst_nents = sg_nents ( req -> dst ) ; pr_debug ( "virtio_crypto: Number of sgs (src_nents: %d, dst_nents: %d)\n" , src_nents , dst_nents ) ; sg_total = src_nents + dst_nents + 3 ; sgs = kcalloc_node ( sg_total , sizeof ( * sgs ) , GFP_KERNEL , dev_to_node ( & vcrypto -> vdev -> dev ) ) ; if ( ! sgs ) { return - ENOMEM ; } req_data = kzalloc_node ( sizeof ( * req_data ) , GFP_KERNEL , dev_to_node ( & vcrypto -> vdev -> dev ) ) ; if ( ! req_data ) { return - ENOMEM ; } vc_req -> req_data = req_data ; vc_sym_req -> type = VIRTIO_CRYPTO_SYM_OP_CIPHER ; if ( vc_sym_req -> encrypt ) { req_data -> header . session_id = cpu_to_le64 ( ctx -> enc_sess_info . session_id ) ; req_data -> header . opcode = cpu_to_le32 ( VIRTIO_CRYPTO_CIPHER_ENCRYPT ) ; } else { req_data -> header . session_id = cpu_to_le64 ( ctx -> dec_sess_info . session_id ) ; req_data -> header . opcode = cpu_to_le32 ( VIRTIO_CRYPTO_CIPHER_DECRYPT ) ; } req_data -> u . sym_req . op_type = cpu_to_le32 ( VIRTIO_CRYPTO_SYM_OP_CIPHER ) ; req_data -> u . sym_req . u . cipher . para . iv_len = cpu_to_le32 ( ivsize ) ; req_data -> u . sym_req . u . cipher . para . src_data_len = cpu_to_le32 ( req -> cryptlen ) ; dst_len = virtio_crypto_alg_sg_nents_length ( req -> dst ) ; if ( unlikely ( dst_len > U32_MAX ) ) { pr_err ( "virtio_crypto: The dst_len is beyond U32_MAX\n" ) ; err = - EINVAL ; free } dst_len = min_t ( , , ) ; pr_debug ( "virtio_crypto: src_len: %u, dst_len: %llu\n" , req -> cryptlen , dst_len ) ; if ( unlikely ( req -> cryptlen + dst_len + ivsize + sizeof ( vc_req -> status ) > vcrypto -> max_size ) ) { pr_err ( "virtio_crypto: The length is too big\n" ) ; err = - EINVAL ; free } req_data -> u . sym_req . u . cipher . para . dst_data_len = cpu_to_le32 ( ( uint32_t ) dst_len ) ; sg_init_one ( & outhdr , req_data , sizeof ( * req_data ) ) ; sgs [ num_out ++ ] = & outhdr ; iv = kzalloc_node ( ivsize , GFP_ATOMIC , dev_to_node ( & vcrypto -> vdev -> dev ) ) ; if ( ! iv ) { err = - ENOMEM ; free } memcpy ( iv , req -> iv , ivsize ) ; if ( ! vc_sym_req -> encrypt ) { scatterwalk_map_and_copy ( req -> iv , req -> src , req -> cryptlen - AES_BLOCK_SIZE , AES_BLOCK_SIZE , 0 ) ; } sg_init_one ( & iv_sg , iv , ivsize ) ; sgs [ num_out ++ ] = & iv_sg ; vc_sym_req -> iv = iv ; for ( sg = req -> src ; src_nents ; sg = sg_next ( sg ) , src_nents -- ) { sgs [ num_out ++ ] = sg ; } for ( sg = req -> dst ; sg ; sg = sg_next ( sg ) ) { sgs [ num_out + num_in ++ ] = sg ; } sg_init_one ( & status_sg , & vc_req -> status , sizeof ( vc_req -> status ) ) ; sgs [ num_out + num_in ++ ] = & status_sg ; vc_req -> sgs = sgs ; spin_lock_irqsave ( & data_vq -> lock , flags ) ; err = virtqueue_add_sgs ( data_vq -> vq , sgs , num_out , num_in , vc_req , GFP_ATOMIC ) ; virtqueue_kick ( data_vq -> vq ) ; spin_unlock_irqrestore ( & data_vq -> lock , flags ) ; if ( unlikely ( err < 0 ) ) { free_iv } return 0 ; free_iv kfree_sensitive ( iv ) ; free kfree_sensitive ( req_data ) ; kfree ( sgs ) ; return err ; } 
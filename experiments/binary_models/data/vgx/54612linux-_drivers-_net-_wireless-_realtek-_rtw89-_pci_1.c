static int rtw89_pci_alloc_tx_wd_ring ( struct rtw89_dev * rtwdev , struct pci_dev * pdev , struct rtw89_pci_tx_ring * tx_ring , enum rtw89_tx_channel txch ) { struct rtw89_pci_tx_wd_ring * wd_ring = & tx_ring -> wd_ring ; struct rtw89_pci_tx_wd * txwd ; dma_addr_t dma ; int cur_paddr ; u8 * head ; u8 * cur_vaddr ; u32 page_size = RTW89_PCI_TXWD_PAGE_SIZE ; u32 page_num = RTW89_PCI_TXWD_NUM_MAX ; u32 ring_sz = page_size * page_num ; u32 page_offset ; int i ; if ( txch == RTW89_TXCH_CH12 ) { return 0 ; } head = dma_alloc_coherent ( & pdev -> dev , ring_sz , & dma , GFP_KERNEL ) ; if ( ! head ) { return - ENOMEM ; } INIT_LIST_HEAD ( & wd_ring -> free_pages ) ; wd_ring -> head = head ; wd_ring -> dma = dma ; wd_ring -> page_size = page_size ; wd_ring -> page_num = page_num ; page_offset = 0 ; for ( i = 0 ; i < page_num ; i ++ ) { txwd = & wd_ring -> pages [ i ] ; cur_paddr = dma + page_offset ; cur_vaddr = head + page_offset ; skb_queue_head_init ( & txwd -> queue ) ; INIT_LIST_HEAD ( & txwd -> list ) ; txwd -> paddr = cur_paddr ; txwd -> vaddr = cur_vaddr ; txwd -> len = page_size ; txwd -> seq = i ; rtw89_pci_enqueue_txwd ( tx_ring , txwd ) ; page_offset += page_size ; } return 0 ; } 
int mlx4_en_process_rx_cq ( struct net_device * dev , struct mlx4_en_cq * cq , int budget ) { struct mlx4_en_priv * priv = netdev_priv ( dev ) ; struct mlx4_en_xdp_buff mxbuf = { } ; int factor = priv -> cqe_factor ; struct mlx4_en_rx_ring * ring ; struct bpf_prog * xdp_prog ; int cq_ring = cq -> ring ; bool doorbell_pending ; bool xdp_redir_flush ; struct mlx4_cqe * cqe ; int polled = 0 ; int index ; ring = priv -> rx_ring [ cq_ring ] ; xdp_prog = rcu_dereference_bh ( ring -> xdp_prog ) ; xdp_init_buff ( & mxbuf . xdp , priv -> frag_info [ 0 ] . frag_stride , & ring -> xdp_rxq ) ; doorbell_pending = false ; xdp_redir_flush = false ; index = cq -> mcq . cons_index & ring -> size_mask ; cqe = mlx4_en_get_cqe ( cq -> buf , index , priv -> cqe_size ) + factor ; while ( XNOR ( cqe -> owner_sr_opcode & MLX4_CQE_OWNER_MASK , cq -> mcq . cons_index & cq -> size ) ) { struct mlx4_en_rx_alloc * frags ; enum pkt_hash_types hash_type ; struct sk_buff * skb ; unsigned int length ; int ip_summed ; void * va ; int nr ; frags = ring -> rx_info + ( index << priv -> log_rx_info ) ; va = page_address ( frags [ 0 ] . page ) + frags [ 0 ] . page_offset ; net_prefetchw ( va ) ; dma_rmb ( ) ; if ( unlikely ( ( cqe -> owner_sr_opcode & MLX4_CQE_OPCODE_MASK ) == MLX4_CQE_OPCODE_ERROR ) ) { en_err ( priv , "CQE completed in error - vendor syndrom:%d syndrom:%d\n" , ( ( mlx4_err_cqe * ) cqe ) -> vendor_err_syndrome , ( ( mlx4_err_cqe * ) cqe ) -> syndrome ) ; next } if ( unlikely ( cqe -> badfcs_enc & MLX4_CQE_BAD_FCS ) ) { en_dbg ( RX_ERR , priv , "Accepted frame with bad FCS\n" ) ; next } if ( priv -> flags & MLX4_EN_FLAG_RX_FILTER_NEEDED ) { const struct ethhdr * ethh = va ; dma_addr_t dma ; dma = frags [ 0 ] . dma + frags [ 0 ] . page_offset ; dma_sync_single_for_cpu ( priv -> ddev , dma , sizeof ( * ethh ) , DMA_FROM_DEVICE ) ; if ( is_multicast_ether_addr ( ethh -> h_dest ) ) { struct mlx4_mac_entry * entry ; struct hlist_head * bucket ; unsigned int mac_hash ; mac_hash = ethh -> h_source [ MLX4_EN_MAC_HASH_IDX ] ; bucket = & priv -> mac_hash [ mac_hash ] ; hlist_for_each_entry_rcu_bh ( , , ) { if ( ether_addr_equal_64bits ( entry -> mac , ethh -> h_source ) ) { next } } } } if ( unlikely ( priv -> validate_loopback ) ) { validate_loopback ( priv , va ) ; next } length = be32_to_cpu ( cqe -> byte_cnt ) ; length -= ring -> fcs_del ; if ( xdp_prog ) { dma_addr_t dma ; void * orig_data ; u32 act ; dma = frags [ 0 ] . dma + frags [ 0 ] . page_offset ; dma_sync_single_for_cpu ( priv -> ddev , dma , priv -> frag_info [ 0 ] . frag_size , DMA_FROM_DEVICE ) ; xdp_prepare_buff ( & mxbuf . xdp , va - frags [ 0 ] . page_offset , frags [ 0 ] . page_offset , length , true ) ; orig_data = mxbuf . xdp . data ; mxbuf . cqe = cqe ; mxbuf . mdev = priv -> mdev ; mxbuf . ring = ring ; mxbuf . dev = dev ; act = bpf_prog_run_xdp ( xdp_prog , & mxbuf . xdp ) ; length = mxbuf . xdp . data_end - mxbuf . xdp . data ; if ( mxbuf . xdp . data != orig_data ) { frags [ 0 ] . page_offset = mxbuf . xdp . data - mxbuf . xdp . data_hard_start ; va = mxbuf . xdp . data ; } switch ( act ) { case XDP_PASS : break ; case XDP_REDIRECT : if ( likely ( ! xdp_do_redirect ( dev , & mxbuf . xdp , xdp_prog ) ) ) { ring -> xdp_redirect ++ ; xdp_redir_flush = true ; frags [ 0 ] . page = NULL ; next } ring -> xdp_redirect_fail ++ ; trace_xdp_exception ( dev , xdp_prog , act ) ; xdp_drop_no_cnt case XDP_TX : if ( likely ( ! mlx4_en_xmit_frame ( ring , frags , priv , length , cq_ring , & doorbell_pending ) ) ) { frags [ 0 ] . page = NULL ; next } trace_xdp_exception ( dev , xdp_prog , act ) ; xdp_drop_no_cnt default : bpf_warn_invalid_xdp_action ( dev , xdp_prog , act ) ; fallthrough ; case XDP_ABORTED : trace_xdp_exception ( dev , xdp_prog , act ) ; fallthrough ; case XDP_DROP : ring -> xdp_drop ++ ; xdp_drop_no_cnt next } } ring -> bytes += length ; ring -> packets ++ ; skb = napi_get_frags ( & cq -> napi ) ; if ( unlikely ( ! skb ) ) { next } if ( unlikely ( ring -> hwtstamp_rx_filter == HWTSTAMP_FILTER_ALL ) ) { u64 timestamp = mlx4_en_get_cqe_ts ( cqe ) ; mlx4_en_fill_hwtstamps ( priv -> mdev , skb_hwtstamps ( skb ) , timestamp ) ; } skb_record_rx_queue ( skb , cq_ring ) ; if ( likely ( dev -> features & NETIF_F_RXCSUM ) ) { if ( ( cqe -> status & cpu_to_be16 ( MLX4_CQE_STATUS_TCP | MLX4_CQE_STATUS_UDP ) ) && ( cqe -> status & cpu_to_be16 ( MLX4_CQE_STATUS_IPOK ) ) && cqe -> checksum == cpu_to_be16 ( 0xffff ) ) { bool l2_tunnel ; l2_tunnel = ( dev -> hw_enc_features & NETIF_F_RXCSUM ) && ( cqe -> vlan_my_qpn & cpu_to_be32 ( MLX4_CQE_L2_TUNNEL ) ) ; ip_summed = CHECKSUM_UNNECESSARY ; hash_type = PKT_HASH_TYPE_L4 ; if ( l2_tunnel ) { skb -> csum_level = 1 ; } ring -> csum_ok ++ ; } else { if ( ! ( priv -> flags & MLX4_EN_FLAG_RX_CSUM_NON_TCP_UDP && ( cqe -> status & cpu_to_be16 ( MLX4_CQE_STATUS_IP_ANY ) ) ) ) { csum_none } if ( check_csum ( cqe , skb , va , dev -> features ) ) { csum_none } ip_summed = CHECKSUM_COMPLETE ; hash_type = PKT_HASH_TYPE_L3 ; ring -> csum_complete ++ ; } } else { csum_none ip_summed = CHECKSUM_NONE ; hash_type = PKT_HASH_TYPE_L3 ; ring -> csum_none ++ ; } skb -> ip_summed = ip_summed ; if ( dev -> features & NETIF_F_RXHASH ) { skb_set_hash ( skb , be32_to_cpu ( cqe -> immed_rss_invalid ) , hash_type ) ; } if ( ( cqe -> vlan_my_qpn & cpu_to_be32 ( MLX4_CQE_CVLAN_PRESENT_MASK ) ) && ( dev -> features & NETIF_F_HW_VLAN_CTAG_RX ) ) { __vlan_hwaccel_put_tag ( skb , htons ( ETH_P_8021Q ) , be16_to_cpu ( cqe -> sl_vid ) ) ; } if ( ( cqe -> vlan_my_qpn & cpu_to_be32 ( MLX4_CQE_SVLAN_PRESENT_MASK ) ) && ( dev -> features & NETIF_F_HW_VLAN_STAG_RX ) ) { __vlan_hwaccel_put_tag ( skb , htons ( ETH_P_8021AD ) , be16_to_cpu ( cqe -> sl_vid ) ) ; } nr = mlx4_en_complete_rx_desc ( priv , frags , skb , length ) ; if ( likely ( nr ) ) { skb_shinfo ( skb ) -> nr_frags = nr ; skb -> len = length ; skb -> data_len = length ; napi_gro_frags ( & cq -> napi ) ; } else { __vlan_hwaccel_clear_tag ( skb ) ; skb_clear_hash ( skb ) ; } next ++ cq -> mcq . cons_index ; index = ( cq -> mcq . cons_index ) & ring -> size_mask ; cqe = mlx4_en_get_cqe ( cq -> buf , index , priv -> cqe_size ) + factor ; if ( unlikely ( ++ polled == budget ) ) { break ; } } if ( xdp_redir_flush ) { xdp_do_flush ( ) ; } if ( likely ( polled ) ) { if ( doorbell_pending ) { priv -> tx_cq [ TX_XDP ] [ cq_ring ] -> xdp_busy = true ; mlx4_en_xmit_doorbell ( priv -> tx_ring [ TX_XDP ] [ cq_ring ] ) ; } mlx4_cq_set_ci ( & cq -> mcq ) ; wmb ( ) ; ring -> cons = cq -> mcq . cons_index ; } mlx4_en_refill_rx_buffers ( priv , ring ) ; return polled ; } 
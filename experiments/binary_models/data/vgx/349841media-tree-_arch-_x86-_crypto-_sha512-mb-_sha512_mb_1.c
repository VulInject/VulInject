static int __init sha512_mb_mod_init ( void ) { int cpu ; int err ; struct mcryptd_alg_cstate * cpu_state ; sha512_mb_alg_state . alg_cstate = alloc_percpu ( mcryptd_alg_cstate ) ; sha512_job_mgr_init = sha512_mb_mgr_init_avx2 ; sha512_job_mgr_submit = sha512_mb_mgr_submit_avx2 ; sha512_job_mgr_flush = sha512_mb_mgr_flush_avx2 ; sha512_job_mgr_get_comp_job = sha512_mb_mgr_get_comp_job_avx2 ; if ( ! sha512_mb_alg_state . alg_cstate ) { return - ENOMEM ; } for_each_possible_cpu ( ) { cpu_state = per_cpu_ptr ( sha512_mb_alg_state . alg_cstate , cpu ) ; cpu_state -> next_flush = 0 ; cpu_state -> next_seq_num = 0 ; cpu_state -> flusher_engaged = false ; INIT_DELAYED_WORK ( & cpu_state -> flush , mcryptd_flusher ) ; cpu_state -> cpu = cpu ; cpu_state -> alg_state = & sha512_mb_alg_state ; cpu_state -> mgr = kzalloc ( sizeof ( sha512_ctx_mgr ) , GFP_KERNEL ) ; if ( ! cpu_state -> mgr ) { err2 } sha512_ctx_mgr_init ( cpu_state -> mgr ) ; INIT_LIST_HEAD ( & cpu_state -> work_list ) ; spin_lock_init ( & cpu_state -> work_lock ) ; } sha512_mb_alg_state . flusher = & sha512_mb_flusher ; err = crypto_register_ahash ( & sha512_mb_areq_alg ) ; if ( err ) { err2 } err = crypto_register_ahash ( & sha512_mb_async_alg ) ; if ( err ) { err1 } return 0 ; err1 crypto_unregister_ahash ( & sha512_mb_areq_alg ) ; err2 for_each_possible_cpu ( ) { cpu_state = per_cpu_ptr ( sha512_mb_alg_state . alg_cstate , cpu ) ; kfree ( cpu_state -> mgr ) ; } free_percpu ( sha512_mb_alg_state . alg_cstate ) ; return - ENODEV ; } 
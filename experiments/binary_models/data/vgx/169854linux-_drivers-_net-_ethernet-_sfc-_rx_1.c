static struct sk_buff * efx_rx_mk_skb ( struct efx_channel * channel , struct efx_rx_buffer * rx_buf , unsigned int n_frags , u8 * eh , int hdr_len ) { struct efx_nic * efx = channel -> efx ; struct sk_buff * skb ; skb = netdev_alloc_skb ( efx -> net_dev , efx -> rx_ip_align + efx -> rx_prefix_size + hdr_len ) ; if ( unlikely ( skb == NULL ) ) { return NULL ; } EFX_WARN_ON_ONCE_PARANOID ( rx_buf -> len < hdr_len ) ; memcpy ( skb -> data + efx -> rx_ip_align , eh - efx -> rx_prefix_size , efx -> rx_prefix_size + hdr_len ) ; skb_reserve ( skb , efx -> rx_ip_align + efx -> rx_prefix_size ) ; __skb_put ( skb , hdr_len ) ; if ( rx_buf -> len > hdr_len ) { rx_buf -> page_offset += hdr_len ; rx_buf -> len -= hdr_len ; for ( ; ; ) { skb_add_rx_frag ( skb , skb_shinfo ( skb ) -> nr_frags , rx_buf -> page , rx_buf -> page_offset , rx_buf -> len , efx -> rx_buffer_truesize ) ; rx_buf -> page = NULL ; if ( skb_shinfo ( skb ) -> nr_frags == n_frags ) { break ; } rx_buf = efx_rx_buf_next ( & channel -> rx_queue , rx_buf ) ; } } else { __free_pages ( rx_buf -> page , efx -> rx_buffer_order ) ; rx_buf -> page = NULL ; n_frags = 0 ; } skb -> protocol = eth_type_trans ( skb , efx -> net_dev ) ; skb_mark_napi_id ( skb , & channel -> napi_str ) ; return skb ; } 
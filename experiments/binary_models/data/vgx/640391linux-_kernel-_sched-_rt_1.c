static void __disable_runtime ( struct rq * rq ) { struct root_domain * rd = rq -> rd ; rt_rq_iter_t iter ; struct rt_rq * rt_rq ; if ( unlikely ( ! scheduler_running ) ) { return ; } for_each_rt_rq ( , , ) { struct rt_bandwidth * rt_b = sched_rt_bandwidth ( rt_rq ) ; s64 want ; int i ; raw_spin_lock ( & rt_b -> rt_runtime_lock ) ; raw_spin_lock ( & rt_rq -> rt_runtime_lock ) ; if ( rt_rq -> rt_runtime == RUNTIME_INF || rt_rq -> rt_runtime == rt_b -> rt_runtime ) { balanced } raw_spin_unlock ( & rt_rq -> rt_runtime_lock ) ; want = rt_b -> rt_runtime - rt_rq -> rt_runtime ; for_each_cpu ( , ) { struct rt_rq * iter = sched_rt_period_rt_rq ( rt_b , i ) ; s64 diff ; if ( iter == rt_rq || iter -> rt_runtime == RUNTIME_INF ) { continue ; } raw_spin_lock ( & iter -> rt_runtime_lock ) ; if ( want > 0 ) { diff = min_t ( s64 , iter -> rt_runtime , want ) ; iter -> rt_runtime -= diff ; want -= diff ; } else { iter -> rt_runtime -= want ; want -= want ; } raw_spin_unlock ( & iter -> rt_runtime_lock ) ; if ( ! want ) { break ; } } raw_spin_lock ( & rt_rq -> rt_runtime_lock ) ; WARN_ON_ONCE ( want , NULL ) ; balanced rt_rq -> rt_runtime = RUNTIME_INF ; rt_rq -> rt_throttled = 0 ; raw_spin_unlock ( & rt_rq -> rt_runtime_lock ) ; raw_spin_unlock ( & rt_b -> rt_runtime_lock ) ; sched_rt_rq_enqueue ( rt_rq ) ; } } 
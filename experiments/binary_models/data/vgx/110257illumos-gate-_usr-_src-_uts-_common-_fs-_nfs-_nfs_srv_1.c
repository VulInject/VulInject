void rfs_write_sync ( struct nfswriteargs * wa , struct nfsattrstat * ns , struct exportinfo * exi , struct svc_req * req , cred_t * cr , bool_t ro ) { int error ; vnode_t * vp ; rlim64_t rlimit ; struct vattr va ; struct uio uio ; struct iovec iov [ MAX_IOVECS ] ; mblk_t * m ; struct iovec * iovp ; int iovcnt ; cred_t * savecred ; int in_crit = 0 ; caller_context_t ct ; vp = nfs_fhtovp ( & wa -> wa_fhandle , exi ) ; if ( vp == NULL ) { return ; } if ( rdonly ( ro , vp ) ) { VN_RELE ( vp ) ; ns -> ns_status = NFSERR_ROFS ; return ; } if ( vp -> v_type != VREG ) { VN_RELE ( vp ) ; ns -> ns_status = NFSERR_ISDIR ; return ; } ct . cc_sysid = 0 ; ct . cc_pid = 0 ; ct . cc_caller_id = nfs2_srv_caller_id ; ct . cc_flags = CC_DONTBLOCK ; va . va_mask = AT_UID | AT_MODE ; error = VOP_GETATTR ( vp , & va , 0 , cr , & ct ) ; if ( error ) { VN_RELE ( vp ) ; ns -> ns_status = puterrno ( error ) ; return ; } if ( crgetuid ( cr ) != va . va_uid ) { error = VOP_ACCESS ( vp , VWRITE , 0 , cr , & ct ) ; if ( error ) { VN_RELE ( vp ) ; ns -> ns_status = puterrno ( error ) ; return ; } } if ( MANDLOCK ( vp , va . va_mode ) ) { VN_RELE ( vp ) ; ns -> ns_status = NFSERR_ACCES ; return ; } if ( nbl_need_check ( vp ) ) { nbl_start_crit ( vp , RW_READER ) ; in_crit = 1 ; if ( nbl_conflict ( vp , NBL_WRITE , wa -> wa_offset , wa -> wa_count , 0 , NULL ) ) { error = EACCES ; out } } error = VOP_RWLOCK ( vp , V_WRITELOCK_TRUE , & ct ) ; if ( error == EAGAIN && ( ct . cc_flags & CC_WOULDBLOCK ) ) { out } if ( wa -> wa_data || wa -> wa_rlist ) { if ( wa -> wa_rlist ) { iov [ 0 ] . iov_base = ( char * ) ( ( wa -> wa_rlist ) -> u . c_daddr3 ) ; iov [ 0 ] . iov_len = wa -> wa_count ; } else { iov [ 0 ] . iov_base = wa -> wa_data ; iov [ 0 ] . iov_len = wa -> wa_count ; } uio . uio_iov = iov ; uio . uio_iovcnt = 1 ; uio . uio_segflg = UIO_SYSSPACE ; uio . uio_extflg = UIO_COPY_DEFAULT ; uio . uio_loffset = ( offset_t ) wa -> wa_offset ; uio . uio_resid = wa -> wa_count ; uio . uio_llimit = curproc -> p_fsz_ctl ; rlimit = uio . uio_llimit - wa -> wa_offset ; if ( rlimit < ( rlim64_t ) uio . uio_resid ) { uio . uio_resid = ( uint_t ) rlimit ; } savecred = curthread -> t_cred ; curthread -> t_cred = cr ; error = VOP_WRITE ( vp , & uio , FSYNC , cr , & ct ) ; curthread -> t_cred = savecred ; } else { iovcnt = 0 ; for ( m = wa -> wa_mblk ; m != NULL ; m = m -> b_cont ) { iovcnt ++ ; } if ( iovcnt <= MAX_IOVECS ) { rfs_write_sync_hits ++ ; iovp = iov ; } else { rfs_write_sync_misses ++ ; iovp = kmem_alloc ( sizeof ( * iovp ) * iovcnt , KM_SLEEP ) ; } mblk_to_iov ( wa -> wa_mblk , iovcnt , iovp ) ; uio . uio_iov = iovp ; uio . uio_iovcnt = iovcnt ; uio . uio_segflg = UIO_SYSSPACE ; uio . uio_extflg = UIO_COPY_DEFAULT ; uio . uio_loffset = ( offset_t ) wa -> wa_offset ; uio . uio_resid = wa -> wa_count ; uio . uio_llimit = curproc -> p_fsz_ctl ; rlimit = uio . uio_llimit - wa -> wa_offset ; if ( rlimit < ( rlim64_t ) uio . uio_resid ) { uio . uio_resid = ( uint_t ) rlimit ; } savecred = curthread -> t_cred ; curthread -> t_cred = cr ; error = VOP_WRITE ( vp , & uio , FSYNC , cr , & ct ) ; curthread -> t_cred = savecred ; if ( iovp != iov ) { kmem_free ( iovp , sizeof ( * iovp ) * iovcnt ) ; } } VOP_RWUNLOCK ( vp , V_WRITELOCK_TRUE , & ct ) ; if ( ! error ) { va . va_mask = AT_ALL ; error = VOP_GETATTR ( vp , & va , 0 , cr , & ct ) ; if ( ! error ) { acl_perm ( vp , exi , & va , cr ) ; error = vattr_to_nattr ( & va , & ns -> ns_attr ) ; } } out if ( in_crit ) { nbl_end_crit ( vp ) ; } VN_RELE ( vp ) ; if ( error == EAGAIN && ( ct . cc_flags & CC_WOULDBLOCK ) ) { curthread -> t_flag |= T_WOULDBLOCK ; } else { ns -> ns_status = puterrno ( error ) ; } } 
static int bnad_per_q_stats_fill ( struct bnad * bnad , u64 * buf , int bi ) { int i , j ; struct bna_rcb * rcb = NULL ; struct bna_tcb * tcb = NULL ; for ( i = 0 ; i < bnad -> num_rx ; i ++ ) { for ( j = 0 ; j < bnad -> num_rxp_per_rx ; j ++ ) { if ( bnad -> rx_info [ i ] . rx_ctrl [ j ] . ccb && bnad -> rx_info [ i ] . rx_ctrl [ j ] . ccb -> rcb [ 0 ] && bnad -> rx_info [ i ] . rx_ctrl [ j ] . ccb -> rcb [ 0 ] -> rxq ) { buf [ bi ++ ] = bnad -> rx_info [ i ] . rx_ctrl [ j ] . ccb -> producer_index ; buf [ bi ++ ] = 0 ; buf [ bi ++ ] = * ( bnad -> rx_info [ i ] . rx_ctrl [ j ] . ccb -> hw_producer_index ) ; buf [ bi ++ ] = bnad -> rx_info [ i ] . rx_ctrl [ j ] . rx_intr_ctr ; buf [ bi ++ ] = bnad -> rx_info [ i ] . rx_ctrl [ j ] . rx_poll_ctr ; buf [ bi ++ ] = bnad -> rx_info [ i ] . rx_ctrl [ j ] . rx_schedule ; buf [ bi ++ ] = bnad -> rx_info [ i ] . rx_ctrl [ j ] . rx_keep_poll ; buf [ bi ++ ] = bnad -> rx_info [ i ] . rx_ctrl [ j ] . rx_complete ; } } } for ( i = 0 ; i < bnad -> num_rx ; i ++ ) { if ( ! bnad -> rx_info [ i ] . rx ) { continue ; } for ( j = 0 ; j < bnad -> num_rxp_per_rx ; j ++ ) { if ( bnad -> rx_info [ i ] . rx_ctrl [ j ] . ccb ) { if ( bnad -> rx_info [ i ] . rx_ctrl [ j ] . ccb -> rcb [ 0 ] && bnad -> rx_info [ i ] . rx_ctrl [ j ] . ccb -> rcb [ 0 ] -> rxq ) { rcb = bnad -> rx_info [ i ] . rx_ctrl [ j ] . ccb -> rcb [ 0 ] ; buf [ bi ++ ] = rcb -> rxq -> rx_packets ; buf [ bi ++ ] = rcb -> rxq -> rx_bytes ; buf [ bi ++ ] = rcb -> rxq -> rx_packets_with_error ; buf [ bi ++ ] = rcb -> rxq -> rxbuf_alloc_failed ; buf [ bi ++ ] = rcb -> rxq -> rxbuf_map_failed ; buf [ bi ++ ] = rcb -> producer_index ; buf [ bi ++ ] = rcb -> consumer_index ; } if ( bnad -> rx_info [ i ] . rx_ctrl [ j ] . ccb -> rcb [ 1 ] && bnad -> rx_info [ i ] . rx_ctrl [ j ] . ccb -> rcb [ 1 ] -> rxq ) { rcb = bnad -> rx_info [ i ] . rx_ctrl [ j ] . ccb -> rcb [ 1 ] ; buf [ bi ++ ] = rcb -> rxq -> rx_packets ; buf [ bi ++ ] = rcb -> rxq -> rx_bytes ; buf [ bi ++ ] = rcb -> rxq -> rx_packets_with_error ; buf [ bi ++ ] = rcb -> rxq -> rxbuf_alloc_failed ; buf [ bi ++ ] = rcb -> rxq -> rxbuf_map_failed ; buf [ bi ++ ] = rcb -> producer_index ; buf [ bi ++ ] = rcb -> consumer_index ; } } } } for ( i = 0 ; i < bnad -> num_tx ; i ++ ) { if ( ! bnad -> tx_info [ i ] . tx ) { continue ; } for ( j = 0 ; j < bnad -> num_txq_per_tx ; j ++ ) { if ( bnad -> tx_info [ i ] . tcb [ j ] && bnad -> tx_info [ i ] . tcb [ j ] -> txq ) { tcb = bnad -> tx_info [ i ] . tcb [ j ] ; buf [ bi ++ ] = tcb -> txq -> tx_packets ; buf [ bi ++ ] = tcb -> txq -> tx_bytes ; buf [ bi ++ ] = tcb -> producer_index ; buf [ bi ++ ] = tcb -> consumer_index ; buf [ bi ++ ] = * ( tcb -> hw_consumer_index ) ; } } } return bi ; } 
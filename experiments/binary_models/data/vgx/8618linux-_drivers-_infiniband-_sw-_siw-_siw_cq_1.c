int siw_reap_cqe ( struct siw_cq * cq , struct ib_wc * wc ) { struct siw_cqe * cqe ; unsigned long flags ; spin_lock_irqsave ( & cq -> lock , flags ) ; cqe = & cq -> queue [ cq -> cq_get % cq -> num_cqe ] ; if ( READ_ONCE ( cqe -> flags ) & SIW_WQE_VALID ) { wc -> wr_id = cqe -> id ; wc -> byte_len = cqe -> bytes ; if ( likely ( rdma_is_kernel_res ( & cq -> base_cq . res ) ) ) { if ( cqe -> flags & SIW_WQE_REM_INVAL ) { wc -> ex . invalidate_rkey = cqe -> inval_stag ; wc -> wc_flags = IB_WC_WITH_INVALIDATE ; } wc -> qp = cqe -> base_qp ; wc -> opcode = map_wc_opcode [ cqe -> opcode ] ; wc -> status = map_cqe_status [ cqe -> status ] . ib ; siw_dbg_cq ( cq , "idx %u, type %d, flags %2x, id 0x%pK\n" , cq -> cq_get % cq -> num_cqe , cqe -> opcode , cqe -> flags , ( void * ) ( uintptr_t ) cqe -> id ) ; } else { u8 opcode = cqe -> opcode ; u16 status = cqe -> status ; if ( opcode >= SIW_NUM_OPCODES ) { opcode = 0 ; status = SIW_WC_GENERAL_ERR ; } if ( status >= SIW_NUM_WC_STATUS ) { status = SIW_WC_GENERAL_ERR ; } wc -> opcode = map_wc_opcode [ opcode ] ; wc -> status = map_cqe_status [ status ] . ib ; } WRITE_ONCE ( cqe -> flags , 0 ) ; cq -> cq_get ++ ; spin_unlock_irqrestore ( & cq -> lock , flags ) ; return 1 ; } spin_unlock_irqrestore ( & cq -> lock , flags ) ; return 0 ; } 
static int __dev_queue_xmit ( struct sk_buff * skb , void * accel_priv ) { struct net_device * dev = skb -> dev ; struct netdev_queue * txq ; struct Qdisc * q ; int rc = - ENOMEM ; skb_reset_mac_header ( skb ) ; if ( unlikely ( skb_shinfo ( skb ) -> tx_flags & SKBTX_SCHED_TSTAMP ) ) { __skb_tstamp_tx ( skb , NULL , skb -> sk , SCM_TSTAMP_SCHED ) ; } rcu_read_lock_bh ( ) ; skb_update_prio ( skb ) ; qdisc_pkt_len_init ( skb ) ; skb -> tc_at_ingress = 0 ; if ( static_key_false ( & egress_needed ) ) { skb = sch_handle_egress ( skb , & rc , dev ) ; if ( ! skb ) { out } } if ( dev -> priv_flags & IFF_XMIT_DST_RELEASE ) { skb_dst_drop ( skb ) ; } else { skb_dst_force ( skb ) ; } txq = netdev_pick_tx ( dev , skb , accel_priv ) ; q = rcu_dereference_bh ( txq -> qdisc ) ; trace_net_dev_queue ( skb ) ; if ( q -> enqueue ) { rc = __dev_xmit_skb ( skb , q , dev , txq ) ; out } if ( dev -> flags & IFF_UP ) { int cpu = smp_processor_id ( ) ; if ( txq -> xmit_lock_owner != cpu ) { if ( unlikely ( __this_cpu_read ( xmit_recursion ) > XMIT_RECURSION_LIMIT ) ) { recursion_alert } skb = validate_xmit_skb ( skb , dev ) ; if ( ! skb ) { out } HARD_TX_LOCK ( dev , txq , cpu ) ; if ( ! netif_xmit_stopped ( txq ) ) { __this_cpu_inc ( xmit_recursion ) ; skb = dev_hard_start_xmit ( skb , dev , txq , & rc ) ; __this_cpu_dec ( xmit_recursion ) ; if ( dev_xmit_complete ( rc ) ) { HARD_TX_UNLOCK ( dev , txq ) ; out } } HARD_TX_UNLOCK ( dev , txq ) ; net_crit_ratelimited ( "Virtual device %s asks to queue packet!\n" , dev -> name ) ; } else { recursion_alert net_crit_ratelimited ( "Dead loop on virtual device %s, fix it urgently!\n" , dev -> name ) ; } } rc = - ENETDOWN ; rcu_read_unlock_bh ( ) ; atomic_long_inc ( & dev -> tx_dropped ) ; return rc ; out rcu_read_unlock_bh ( ) ; return rc ; } 
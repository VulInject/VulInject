static void cgroup_bpf_release ( struct work_struct * work ) { struct cgroup * p , * cgrp = container_of ( work , cgroup , bpf . release_work ) ; struct bpf_prog_array * old_array ; struct list_head * storages = & cgrp -> bpf . storages ; struct bpf_cgroup_storage * storage , * stmp ; unsigned int atype ; mutex_lock ( & cgroup_mutex ) ; for ( atype = 0 ; atype < ARRAY_SIZE ( cgrp -> bpf . progs ) ; atype ++ ) { struct hlist_head * progs = & cgrp -> bpf . progs [ atype ] ; struct bpf_prog_list * pl ; struct hlist_node * pltmp ; hlist_for_each_entry_safe ( , , , ) { hlist_del ( & pl -> node ) ; if ( pl -> prog ) { if ( pl -> prog -> expected_attach_type == BPF_LSM_CGROUP ) { bpf_trampoline_unlink_cgroup_shim ( pl -> prog ) ; } bpf_prog_put ( pl -> prog ) ; } if ( pl -> link ) { if ( pl -> link -> link . prog -> expected_attach_type == BPF_LSM_CGROUP ) { bpf_trampoline_unlink_cgroup_shim ( pl -> link -> link . prog ) ; } bpf_cgroup_link_auto_detach ( pl -> link ) ; } kfree ( pl ) ; static_branch_dec ( & cgroup_bpf_enabled_key [ atype ] ) ; } old_array = rcu_dereference_protected ( cgrp -> bpf . effective [ atype ] , lockdep_is_held ( & cgroup_mutex ) ) ; bpf_prog_array_free ( old_array ) ; } list_for_each_entry_safe ( , , , ) { bpf_cgroup_storage_unlink ( storage ) ; bpf_cgroup_storage_free ( storage ) ; } mutex_unlock ( & cgroup_mutex ) ; for ( p = cgroup_parent ( cgrp ) ; p ; p = cgroup_parent ( p ) ) { cgroup_bpf_put ( p ) ; } percpu_ref_exit ( & cgrp -> bpf . refcnt ) ; cgroup_put ( cgrp , NULL ) ; } 
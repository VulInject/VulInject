int flow_cache_init ( struct net * net ) { int i ; struct flow_cache * fc = & net -> xfrm . flow_cache_global ; if ( ! flow_cachep ) { flow_cachep = kmem_cache_create ( "flow_cache" , sizeof ( flow_cache_entry ) , 0 , SLAB_PANIC , NULL ) ; } spin_lock_init ( & net -> xfrm . flow_cache_gc_lock ) ; INIT_LIST_HEAD ( & net -> xfrm . flow_cache_gc_list ) ; INIT_WORK ( & net -> xfrm . flow_cache_gc_work , flow_cache_gc_task ) ; INIT_WORK ( & net -> xfrm . flow_cache_flush_work , flow_cache_flush_task ) ; mutex_init ( & net -> xfrm . flow_flush_sem ) ; atomic_set ( & net -> xfrm . flow_cache_gc_count , 0 ) ; fc -> hash_shift = 10 ; fc -> low_watermark = 2 * flow_cache_hash_size ( fc ) ; fc -> high_watermark = 4 * flow_cache_hash_size ( fc ) ; fc -> percpu = alloc_percpu ( flow_cache_percpu ) ; if ( ! fc -> percpu ) { return - ENOMEM ; } if ( cpuhp_state_add_instance ( CPUHP_NET_FLOW_PREPARE , & fc -> node ) ) { err } setup_timer ( & fc -> rnd_timer , flow_cache_new_hashrnd , ( unsigned long ) fc ) ; fc -> rnd_timer . expires = jiffies + FLOW_HASH_RND_PERIOD ; add_timer ( & fc -> rnd_timer ) ; return 0 ; err for_each_possible_cpu ( ) { struct flow_cache_percpu * fcp = per_cpu_ptr ( fc -> percpu , i ) ; fcp -> hash_table = NULL ; } free_percpu ( fc -> percpu ) ; fc -> percpu = NULL ; return - ENOMEM ; } 
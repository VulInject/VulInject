static int hns_nic_tx_poll_one ( struct hns_nic_ring_data * ring_data , int budget , void * v ) { struct hnae_ring * ring = ring_data -> ring ; struct net_device * ndev = ring_data -> napi . dev ; struct netdev_queue * dev_queue ; struct hns_nic_priv * priv = netdev_priv ( ndev ) ; int head ; int bytes , pkts ; NETIF_TX_LOCK ( ring , NULL ) ; head = readl_relaxed ( ring -> io_base + RCB_REG_HEAD ) ; rmb ( ) ; if ( is_ring_empty ( ring ) || head == ring -> next_to_clean ) { NETIF_TX_UNLOCK ( ring ) ; return 0 ; } if ( ! is_valid_clean_head ( ring , head ) ) { netdev_err ( ndev , "wrong head (%d, %d-%d)\n" , head , ring -> next_to_use , ring -> next_to_clean ) ; ring -> stats . io_err_cnt ++ ; NETIF_TX_UNLOCK ( ring ) ; return - EIO ; } bytes = 0 ; pkts = 0 ; while ( head != ring -> next_to_clean ) { hns_nic_reclaim_one_desc ( ring , & bytes , & pkts ) ; prefetch ( & ring -> desc_cb [ ring -> next_to_clean ] ) ; } NETIF_TX_UNLOCK ( ring ) ; dev_queue = netdev_get_tx_queue ( ndev , ring_data -> queue_index ) ; netdev_tx_completed_queue ( dev_queue , pkts , bytes ) ; if ( unlikely ( priv -> link && ! netif_carrier_ok ( ndev ) ) ) { netif_carrier_on ( ndev ) ; } if ( unlikely ( pkts && netif_carrier_ok ( ndev ) && ( ring_space ( ring ) >= ring -> max_desc_num_per_pkt * 2 ) ) ) { smp_mb ( ) ; if ( netif_tx_queue_stopped ( dev_queue ) && ! test_bit ( NIC_STATE_DOWN , & priv -> state ) ) { netif_tx_wake_queue ( dev_queue ) ; ring -> stats . restart_queue ++ ; } } return 0 ; } 
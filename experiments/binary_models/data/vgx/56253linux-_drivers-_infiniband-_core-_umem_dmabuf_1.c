int ib_umem_dmabuf_map_pages ( struct ib_umem_dmabuf * umem_dmabuf ) { struct sg_table * sgt ; struct scatterlist * sg ; unsigned long start , end , cur = 0 ; unsigned int nmap = 0 ; long ret ; int i ; dma_resv_assert_held ( umem_dmabuf -> attach -> dmabuf -> resv ) ; if ( umem_dmabuf -> sgt ) { wait_fence } sgt = dma_buf_map_attachment ( umem_dmabuf -> attach , DMA_BIDIRECTIONAL ) ; start = ALIGN_DOWN ( umem_dmabuf -> umem . address , PAGE_SIZE ) ; end = ALIGN ( umem_dmabuf -> umem . address + umem_dmabuf -> umem . length , PAGE_SIZE ) ; for_each_sgtable_dma_sg ( , , ) { if ( start < cur + sg_dma_len ( sg ) && cur < end ) { nmap ++ ; } if ( cur <= start && start < cur + sg_dma_len ( sg ) ) { unsigned long offset = start - cur ; umem_dmabuf -> first_sg = sg ; umem_dmabuf -> first_sg_offset = offset ; sg_dma_address ( sg ) += offset ; sg_dma_len ( sg ) -= offset ; cur += offset ; } if ( cur < end && end <= cur + sg_dma_len ( sg ) ) { unsigned long trim = cur + sg_dma_len ( sg ) - end ; umem_dmabuf -> last_sg = sg ; umem_dmabuf -> last_sg_trim = trim ; sg_dma_len ( sg ) -= trim ; break ; } cur += sg_dma_len ( sg ) ; } umem_dmabuf -> umem . sgt_append . sgt . sgl = umem_dmabuf -> first_sg ; umem_dmabuf -> umem . sgt_append . sgt . nents = nmap ; umem_dmabuf -> sgt = sgt ; wait_fence ret = dma_resv_wait_timeout ( umem_dmabuf -> attach -> dmabuf -> resv , DMA_RESV_USAGE_KERNEL , false , MAX_SCHEDULE_TIMEOUT ) ; if ( ret < 0 ) { return ret ; } if ( ret == 0 ) { return - ETIMEDOUT ; } return 0 ; } 
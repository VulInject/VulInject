int amdgpu_vm_grab_id ( struct amdgpu_vm * vm , struct amdgpu_ring * ring , struct amdgpu_sync * sync , struct dma_fence * fence , struct amdgpu_job * job ) { struct amdgpu_device * adev = ring -> adev ; unsigned vmhub = ring -> funcs -> vmhub ; struct amdgpu_vm_id_manager * id_mgr = & adev -> vm_manager . id_mgr [ vmhub ] ; uint64_t fence_context = adev -> fence_context + ring -> idx ; struct dma_fence * updates = sync -> last_vm_update ; struct amdgpu_vm_id * id , * idle ; struct dma_fence * * fences ; unsigned i ; int r = 0 ; fences = kmalloc_array ( sizeof ( void * ) , id_mgr -> num_ids , GFP_KERNEL ) ; if ( ! fences ) { return - ENOMEM ; } mutex_lock ( & id_mgr -> lock ) ; i = 0 ; list_for_each_entry ( , , ) { fences [ i ] = amdgpu_sync_peek_fence ( & idle -> active , ring ) ; if ( ! fences [ i ] ) { break ; } ++ i ; } if ( & idle -> list == & id_mgr -> ids_lru ) { u64 fence_context = adev -> vm_manager . fence_context + ring -> idx ; unsigned seqno = ++ adev -> vm_manager . seqno [ ring -> idx ] ; struct dma_fence_array * array ; unsigned j ; for ( j = 0 ; j < i ; ++ j ) { dma_fence_get ( fences [ j ] ) ; } array = dma_fence_array_create ( i , fences , fence_context , seqno , true ) ; if ( ! array ) { for ( j = 0 ; j < i ; ++ j ) { dma_fence_put ( fences [ j ] ) ; } r = - ENOMEM ; error } r = amdgpu_sync_fence ( ring -> adev , sync , & array -> base ) ; dma_fence_put ( & array -> base ) ; if ( r ) { error } mutex_unlock ( & id_mgr -> lock ) ; return 0 ; } kfree ( fences ) ; job -> vm_needs_flush = false ; list_for_each_entry_reverse ( , , ) { struct dma_fence * flushed ; bool needs_flush = false ; if ( amdgpu_vm_had_gpu_reset ( adev , id ) ) { continue ; } if ( atomic64_read ( & id -> owner ) != vm -> client_id ) { continue ; } if ( job -> vm_pd_addr != id -> pd_gpu_addr ) { continue ; } if ( ! id -> last_flush || ( id -> last_flush -> context != fence_context && ! dma_fence_is_signaled ( id -> last_flush ) ) ) { needs_flush = true ; } flushed = id -> flushed_updates ; if ( updates && ( ! flushed || dma_fence_is_later ( updates , flushed ) ) ) { needs_flush = true ; } if ( adev -> asic_type < CHIP_VEGA10 && needs_flush ) { continue ; } r = amdgpu_sync_fence ( ring -> adev , & id -> active , fence ) ; if ( r ) { error } if ( updates && ( ! flushed || dma_fence_is_later ( updates , flushed ) ) ) { dma_fence_put ( id -> flushed_updates ) ; id -> flushed_updates = dma_fence_get ( updates ) ; } if ( needs_flush ) { needs_flush } else { no_flush_needed } } id = idle ; r = amdgpu_sync_fence ( ring -> adev , & id -> active , fence ) ; if ( r ) { error } id -> pd_gpu_addr = job -> vm_pd_addr ; dma_fence_put ( id -> flushed_updates ) ; id -> flushed_updates = dma_fence_get ( updates ) ; id -> current_gpu_reset_count = atomic_read ( & adev -> gpu_reset_counter ) ; atomic64_set ( & id -> owner , vm -> client_id ) ; needs_flush job -> vm_needs_flush = true ; dma_fence_put ( id -> last_flush ) ; id -> last_flush = NULL ; no_flush_needed list_move_tail ( & id -> list , & id_mgr -> ids_lru ) ; job -> vm_id = id - id_mgr -> ids ; trace_amdgpu_vm_grab_id ( vm , ring , job ) ; error mutex_unlock ( & id_mgr -> lock ) ; return r ; } 
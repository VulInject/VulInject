static void __init build_mem_type_table ( void ) { struct cachepolicy * cp ; int cr = get_cr ( ) ; pteval_t user_pgprot , kern_pgprot , vecs_pgprot ; pteval_t hyp_device_pgprot , s2_pgprot , s2_device_pgprot ; int cpu_arch = cpu_architecture ( ) ; int i ; if ( cpu_arch < CPU_ARCH_ARMv6 ) { if ( cachepolicy > CPOLICY_BUFFERED ) { cachepolicy = CPOLICY_BUFFERED ; } if ( cachepolicy > CPOLICY_WRITETHROUGH ) { cachepolicy = CPOLICY_WRITETHROUGH ; } } if ( cpu_arch < CPU_ARCH_ARMv5 ) { if ( cachepolicy >= CPOLICY_WRITEALLOC ) { cachepolicy = CPOLICY_WRITEBACK ; } ecc_mask = 0 ; } if ( is_smp ( ) ) { if ( cachepolicy != CPOLICY_WRITEALLOC ) { pr_warn ( "Forcing write-allocate cache policy for SMP\n" ) ; cachepolicy = CPOLICY_WRITEALLOC ; } if ( ! ( initial_pmd_value & PMD_SECT_S ) ) { pr_warn ( "Forcing shared mappings for SMP\n" ) ; initial_pmd_value |= PMD_SECT_S ; } } if ( cpu_arch < CPU_ARCH_ARMv5 ) { for ( i = 0 ; i < ARRAY_SIZE ( mem_types ) ; i ++ ) { mem_types [ i ] . prot_sect &= ~ PMD_SECT_TEX ( 7 ) ; } } if ( ( cpu_arch < CPU_ARCH_ARMv6 || ! ( cr & CR_XP ) ) && ! cpu_is_xsc3 ( ) ) { for ( i = 0 ; i < ARRAY_SIZE ( mem_types ) ; i ++ ) { mem_types [ i ] . prot_sect &= ~ PMD_SECT_S ; } } if ( cpu_is_xscale_family ( ) ) { for ( i = 0 ; i < ARRAY_SIZE ( mem_types ) ; i ++ ) { mem_types [ i ] . prot_sect &= ~ PMD_BIT4 ; mem_types [ i ] . prot_l1 &= ~ PMD_BIT4 ; } } if ( cpu_arch < CPU_ARCH_ARMv6 ) { for ( i = 0 ; i < ARRAY_SIZE ( mem_types ) ; i ++ ) { if ( mem_types [ i ] . prot_l1 ) { mem_types [ i ] . prot_l1 |= PMD_BIT4 ; } if ( mem_types [ i ] . prot_sect ) { mem_types [ i ] . prot_sect |= PMD_BIT4 ; } } } if ( cpu_is_xsc3 ( ) || ( cpu_arch >= CPU_ARCH_ARMv6 && ( cr & CR_XP ) ) ) { if ( ! cpu_is_xsc3 ( ) ) { mem_types [ MT_DEVICE ] . prot_sect |= PMD_SECT_XN ; mem_types [ MT_DEVICE_NONSHARED ] . prot_sect |= PMD_SECT_XN ; mem_types [ MT_DEVICE_CACHED ] . prot_sect |= PMD_SECT_XN ; mem_types [ MT_DEVICE_WC ] . prot_sect |= PMD_SECT_XN ; mem_types [ MT_MEMORY_RW ] . prot_sect |= PMD_SECT_XN ; } if ( cpu_arch >= CPU_ARCH_ARMv7 && ( cr & CR_TRE ) ) { mem_types [ MT_DEVICE ] . prot_sect |= PMD_SECT_TEX ( 1 ) ; mem_types [ MT_DEVICE_NONSHARED ] . prot_sect |= PMD_SECT_TEX ( 1 ) ; mem_types [ MT_DEVICE_WC ] . prot_sect |= PMD_SECT_BUFFERABLE ; } if ( cpu_is_xsc3 ( ) ) { mem_types [ MT_DEVICE ] . prot_sect |= PMD_SECT_TEX ( 1 ) | PMD_SECT_BUFFERED ; mem_types [ MT_DEVICE_NONSHARED ] . prot_sect |= PMD_SECT_TEX ( 2 ) ; mem_types [ MT_DEVICE_WC ] . prot_sect |= PMD_SECT_TEX ( 1 ) ; } else { mem_types [ MT_DEVICE ] . prot_sect |= PMD_SECT_BUFFERED ; mem_types [ MT_DEVICE_NONSHARED ] . prot_sect |= PMD_SECT_TEX ( 2 ) ; mem_types [ MT_DEVICE_WC ] . prot_sect |= PMD_SECT_TEX ( 1 ) ; } } else { mem_types [ MT_DEVICE_WC ] . prot_sect |= PMD_SECT_BUFFERABLE ; } cp = & cache_policies [ cachepolicy ] ; vecs_pgprot = kern_pgprot = user_pgprot = cp -> pte ; s2_pgprot = cp -> pte_s2 ; hyp_device_pgprot = mem_types [ MT_DEVICE ] . prot_pte ; s2_device_pgprot = mem_types [ MT_DEVICE ] . prot_pte_s2 ; if ( cpu_arch == CPU_ARCH_ARMv6 ) { vecs_pgprot |= L_PTE_MT_VECTORS ; } if ( cpu_arch == CPU_ARCH_ARMv7 && ( read_cpuid_ext ( CPUID_EXT_MMFR0 ) & 0xF ) >= 4 ) { user_pmd_table |= PMD_PXNTABLE ; } if ( cpu_arch >= CPU_ARCH_ARMv6 && ( cr & CR_XP ) ) { mem_types [ MT_ROM ] . prot_sect |= PMD_SECT_APX | PMD_SECT_AP_WRITE ; mem_types [ MT_MINICLEAN ] . prot_sect |= PMD_SECT_APX | PMD_SECT_AP_WRITE ; mem_types [ MT_CACHECLEAN ] . prot_sect |= PMD_SECT_APX | PMD_SECT_AP_WRITE ; if ( initial_pmd_value & PMD_SECT_S ) { user_pgprot |= L_PTE_SHARED ; kern_pgprot |= L_PTE_SHARED ; vecs_pgprot |= L_PTE_SHARED ; s2_pgprot |= L_PTE_SHARED ; mem_types [ MT_DEVICE_WC ] . prot_sect |= PMD_SECT_S ; mem_types [ MT_DEVICE_WC ] . prot_pte |= L_PTE_SHARED ; mem_types [ MT_DEVICE_CACHED ] . prot_sect |= PMD_SECT_S ; mem_types [ MT_DEVICE_CACHED ] . prot_pte |= L_PTE_SHARED ; mem_types [ MT_MEMORY_RWX ] . prot_sect |= PMD_SECT_S ; mem_types [ MT_MEMORY_RWX ] . prot_pte |= L_PTE_SHARED ; mem_types [ MT_MEMORY_RW ] . prot_sect |= PMD_SECT_S ; mem_types [ MT_MEMORY_RW ] . prot_pte |= L_PTE_SHARED ; mem_types [ MT_MEMORY_DMA_READY ] . prot_pte |= L_PTE_SHARED ; mem_types [ MT_MEMORY_RWX_NONCACHED ] . prot_sect |= PMD_SECT_S ; mem_types [ MT_MEMORY_RWX_NONCACHED ] . prot_pte |= L_PTE_SHARED ; } } if ( cpu_arch >= CPU_ARCH_ARMv6 ) { if ( cpu_arch >= CPU_ARCH_ARMv7 && ( cr & CR_TRE ) ) { mem_types [ MT_MEMORY_RWX_NONCACHED ] . prot_sect |= PMD_SECT_BUFFERED ; } else { mem_types [ MT_MEMORY_RWX_NONCACHED ] . prot_sect |= PMD_SECT_TEX ( 1 ) ; } } else { mem_types [ MT_MEMORY_RWX_NONCACHED ] . prot_sect |= PMD_SECT_BUFFERABLE ; } for ( i = 0 ; i < ARRAY_SIZE ( mem_types ) ; i ++ ) { mem_types [ i ] . prot_pte |= PTE_EXT_AF ; if ( mem_types [ i ] . prot_sect ) { mem_types [ i ] . prot_sect |= PMD_SECT_AF ; } } kern_pgprot |= PTE_EXT_AF ; vecs_pgprot |= PTE_EXT_AF ; user_pgprot |= PTE_EXT_PXN ; for ( i = 0 ; i < 16 ; i ++ ) { pteval_t v = pgprot_val ( protection_map [ i ] ) ; protection_map [ i ] = __pgprot ( v | user_pgprot ) ; } mem_types [ MT_LOW_VECTORS ] . prot_pte |= vecs_pgprot ; mem_types [ MT_HIGH_VECTORS ] . prot_pte |= vecs_pgprot ; pgprot_user = __pgprot ( L_PTE_PRESENT | L_PTE_YOUNG | user_pgprot ) ; pgprot_kernel = __pgprot ( L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY | kern_pgprot ) ; pgprot_s2 = __pgprot ( L_PTE_PRESENT | L_PTE_YOUNG | s2_pgprot ) ; pgprot_s2_device = __pgprot ( s2_device_pgprot ) ; pgprot_hyp_device = __pgprot ( hyp_device_pgprot ) ; mem_types [ MT_LOW_VECTORS ] . prot_l1 |= ecc_mask ; mem_types [ MT_HIGH_VECTORS ] . prot_l1 |= ecc_mask ; mem_types [ MT_MEMORY_RWX ] . prot_sect |= ecc_mask | cp -> pmd ; mem_types [ MT_MEMORY_RWX ] . prot_pte |= kern_pgprot ; mem_types [ MT_MEMORY_RW ] . prot_sect |= ecc_mask | cp -> pmd ; mem_types [ MT_MEMORY_RW ] . prot_pte |= kern_pgprot ; mem_types [ MT_MEMORY_DMA_READY ] . prot_pte |= kern_pgprot ; mem_types [ MT_MEMORY_RWX_NONCACHED ] . prot_sect |= ecc_mask ; mem_types [ MT_ROM ] . prot_sect |= cp -> pmd ; switch ( cp -> pmd ) { case PMD_SECT_WT : mem_types [ MT_CACHECLEAN ] . prot_sect |= PMD_SECT_WT ; break ; case PMD_SECT_WB : case PMD_SECT_WBWA : mem_types [ MT_CACHECLEAN ] . prot_sect |= PMD_SECT_WB ; break ; } pr_info ( "Memory policy: %sData cache %s\n" , ecc_mask ?"ECC enabled, " : "" , cp -> policy ) ; for ( i = 0 ; i < ARRAY_SIZE ( mem_types ) ; i ++ ) { struct mem_type * t = & mem_types [ i ] ; if ( t -> prot_l1 ) { t -> prot_l1 |= PMD_DOMAIN ( t -> domain ) ; } if ( t -> prot_sect ) { t -> prot_sect |= PMD_DOMAIN ( t -> domain ) ; } } } 
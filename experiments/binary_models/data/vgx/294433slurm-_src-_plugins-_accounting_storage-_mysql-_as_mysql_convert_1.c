static int _insert_into_hash_table ( mysql_conn_t * mysql_conn , char * cluster_name , move_large_type_t type ) { char * query , * hash_inx_col ; char * hash_col = NULL , * type_col = NULL , * type_table = NULL ; int rc ; switch ( type ) { case MOVE_ENV : hash_col = "env_hash" ; hash_inx_col = "env_hash_inx" ; type_col = "env_vars" ; type_table = job_env_table ; break ; case MOVE_BATCH : hash_col = "script_hash" ; hash_inx_col = "script_hash_inx" ; type_col = "batch_script" ; type_table = job_script_table ; break ; default : return SLURM_ERROR ; break ; } info ( "Starting insert from job_table into %s" , type_table ) ; query = xstrdup_printf ( "insert into \"%s_%s\" (%s, %s) " "select distinct %s, %s from \"%s_%s\" " "where %s is not NULL&&" "(id_array_job=id_job || !id_array_job) " "on duplicate key update last_used=UNIX_TIMESTAMP();" , cluster_name , type_table , hash_col , type_col , hash_col , type_col , cluster_name , job_table , type_col ) ; DB_DEBUG ( DB_QUERY , mysql_conn -> conn , "query\n%s" , query ) ; rc = mysql_db_query ( mysql_conn , query ) ; if ( rc != SLURM_SUCCESS ) { return rc ; } query = xstrdup_printf ( "update \"%s_%s\" as jobs inner join \"%s_%s\" as hash " "on jobs.%s = hash.%s set jobs.%s = hash.hash_inx;" , cluster_name , job_table , cluster_name , type_table , hash_col , hash_col , hash_inx_col ) ; DB_DEBUG ( DB_QUERY , mysql_conn -> conn , "query\n%s" , query ) ; rc = mysql_db_query ( mysql_conn , query ) ; xfree ( query ) ; info ( "Done" ) ; return rc ; } 
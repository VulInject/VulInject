unsigned long i915_gem_shrink ( struct drm_i915_private * dev_priv , unsigned long target , unsigned flags ) { const { struct list_head * list ; unsigned int bit ; } , phases [ ] { { & dev_priv -> mm . unbound_list I915_SHRINK_UNBOUND } { & dev_priv -> mm . bound_list I915_SHRINK_BOUND } { NULL 0 } } ; , * phase unsigned long count = 0 ; bool unlock ; if ( ! i915_gem_shrinker_lock ( & dev_priv -> drm , & unlock ) ) { return 0 ; } trace_i915_gem_shrink ( dev_priv , target , flags ) ; i915_gem_retire_requests ( dev_priv ) ; if ( ( flags & I915_SHRINK_BOUND ) && ! intel_runtime_pm_get_if_in_use ( dev_priv ) ) { flags &= ~ I915_SHRINK_BOUND ; } for ( phase = phases ; phase -> list ; phase ++ ) { struct list_head still_in_list ; struct drm_i915_gem_object * obj ; if ( ( flags & phase -> bit ) == 0 ) { continue ; } INIT_LIST_HEAD ( & still_in_list ) ; while ( count < target && ( obj = list_first_entry_or_null ( phase -> list , typeof ( * obj ) , global_link ) ) ) { list_move_tail ( & obj -> global_link , & still_in_list ) ; if ( ! obj -> mm . pages ) { list_del ( & obj -> global_link ) ; continue ; } if ( flags & I915_SHRINK_PURGEABLE && obj -> mm . madv != I915_MADV_DONTNEED ) { continue ; } if ( flags & I915_SHRINK_VMAPS && ! is_vmalloc_addr ( obj -> mm . mapping ) ) { continue ; } if ( ! ( flags & I915_SHRINK_ACTIVE ) && ( i915_gem_object_is_active ( obj ) || i915_gem_object_is_framebuffer ( obj ) ) ) { continue ; } if ( ! can_release_pages ( obj ) ) { continue ; } if ( unsafe_drop_pages ( obj ) ) { mutex_lock_nested ( & obj -> mm . lock , I915_MM_SHRINKER ) ; if ( ! obj -> mm . pages ) { __i915_gem_object_invalidate ( obj ) ; list_del_init ( & obj -> global_link ) ; count += obj -> base . size >> PAGE_SHIFT ; } mutex_unlock ( & obj -> mm . lock ) ; } } list_splice_tail ( & still_in_list , phase -> list ) ; } if ( flags & I915_SHRINK_BOUND ) { intel_runtime_pm_put ( dev_priv ) ; } i915_gem_retire_requests ( dev_priv ) ; i915_gem_shrinker_unlock ( & dev_priv -> drm , unlock ) ; return count ; } 
int amdgpu_hmm_range_get_pages ( struct mmu_interval_notifier * notifier , uint64_t start , uint64_t npages , bool readonly , void * owner , struct page * * pages , struct hmm_range * * phmm_range ) { struct hmm_range * hmm_range ; unsigned long end ; unsigned long timeout ; unsigned long i ; unsigned long * pfns ; int r = 0 ; hmm_range = kzalloc ( sizeof ( * hmm_range ) , GFP_KERNEL ) ; if ( unlikely ( ! hmm_range ) ) { return - ENOMEM ; } pfns = kvmalloc_array ( npages , sizeof ( * pfns ) , GFP_KERNEL ) ; if ( unlikely ( ! pfns ) ) { r = - ENOMEM ; out_free_range } hmm_range -> notifier = notifier ; hmm_range -> default_flags = HMM_PFN_REQ_FAULT ; if ( ! readonly ) { hmm_range -> default_flags |= HMM_PFN_REQ_WRITE ; } hmm_range -> hmm_pfns = pfns ; hmm_range -> start = start ; end = start + npages * PAGE_SIZE ; hmm_range -> dev_private_owner = owner ; { hmm_range -> end = min ( hmm_range -> start + MAX_WALK_BYTE , end ) ; pr_debug ( "hmm range: start = 0x%lx, end = 0x%lx" , hmm_range -> start , hmm_range -> end ) ; timeout = max ( ( hmm_range -> end - hmm_range -> start ) >> 29 , 1UL ) ; timeout *= HMM_RANGE_DEFAULT_TIMEOUT ; timeout = jiffies + msecs_to_jiffies ( timeout ) ; retry hmm_range -> notifier_seq = mmu_interval_read_begin ( notifier ) ; r = hmm_range_fault ( hmm_range ) ; if ( unlikely ( r ) ) { if ( r == - EBUSY && ! time_after ( jiffies , timeout ) ) { retry } out_free_pfns } if ( hmm_range -> end == end ) { break ; } hmm_range -> hmm_pfns += MAX_WALK_BYTE >> PAGE_SHIFT ; hmm_range -> start = hmm_range -> end ; schedule ( ) ; } hmm_range -> end < end ; hmm_range -> start = start ; hmm_range -> hmm_pfns = pfns ; for ( i = 0 ; pages && i < npages ; i ++ ) { pages [ i ] = hmm_pfn_to_page ( pfns [ i ] ) ; } * phmm_range = hmm_range ; return 0 ; out_free_pfns kvfree ( pfns ) ; out_free_range return r ; } 